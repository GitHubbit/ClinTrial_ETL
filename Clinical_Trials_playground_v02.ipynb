{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2bf1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import pathlib\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re\n",
    "from collections import namedtuple\n",
    "# For pausing\n",
    "from time import sleep\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home\"\n",
    "import pickle\n",
    "from itertools import repeat\n",
    "import psycopg2\n",
    "import pandas.io.sql as sqlio\n",
    "# adding Folder_2/subfolder to the system path\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "from pymetamap import MetaMap\n",
    "import random\n",
    "\n",
    "import json\n",
    "# import argparse\n",
    "# import lxml\n",
    "\n",
    "# from Authentication import *\n",
    "import requests\n",
    "# from requests.adapters import HTTPAdapter\n",
    "# from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup UMLS Server global vars\n",
    "# # metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "# # metamap_base_dir = \"/users/knarsinh/projects/clinical_trials/metamap/public_mm/\"\n",
    "# # metamap_bin_dir = 'bin/metamap18' # uncomment for running on local\n",
    "# metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "# metamap_bin_dir = 'bin/metamap20'\n",
    "# metamap_pos_server_dir = 'bin/skrmedpostctl'\n",
    "# metamap_wsd_server_dir = 'bin/wsdserverctl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00464b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup UMLS Server global vars\n",
    "\n",
    "global metamap_pos_server_dir\n",
    "global metamap_wsd_server_dir\n",
    "metamap_pos_server_dir = 'bin/skrmedpostctl'\n",
    "metamap_wsd_server_dir = 'bin/wsdserverctl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24965ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metamap_base_dir': '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/',\n",
       " 'metamap_bin_dir': 'bin/metamap18'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir}\n",
    "        \n",
    "metamap_dirs = check_os()\n",
    "metamap_dirs\n",
    "\n",
    "# /Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db01284d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Clinical Trial data as of 2022-08-04\n",
      "Finished download\n",
      "Unzipping data\n",
      "Finished extracting zip\n"
     ]
    }
   ],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    # get all the links and associated dates of upload into a dict called date_link\n",
    "    url_all = \"https://aact.ctti-clinicaltrials.org/pipe_files\"\n",
    "    response = requests.get(url_all)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    body = soup.find_all('option') #Find all\n",
    "    date_link = {}\n",
    "    for el in body:\n",
    "        tags = el.find('a')\n",
    "        try:\n",
    "\n",
    "            zip_name = tags.contents[0].split()[0]\n",
    "            date = zip_name.split(\"_\")[0]\n",
    "            date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "            date_link[date] = tags.get('href')\n",
    "        except:\n",
    "            pass\n",
    "    # get the date of the latest upload\n",
    "    latest_file_date = max(date_link.keys())\n",
    "    \n",
    "     # get the corresponding download link of the latest upload so we can download the raw data\n",
    "    url = date_link[latest_file_date]\n",
    "    data_dir = \"{}/data\".format(pathlib.Path.cwd().parents[0])\n",
    "    data_extracted = data_dir + \"/{}_extracted\".format(latest_file_date)\n",
    "    data_path = pathlib.Path.cwd().parents[0] / \"{}/{}_pipe-delimited-export.zip\".format(data_dir, latest_file_date)\n",
    "\n",
    "    # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "    if not os.path.exists(data_path):\n",
    "        # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        term_program_flag = False  \n",
    "        print(\"Downloading Clinical Trial data as of {}\".format(latest_file_date))\n",
    "        req = requests.get(url)\n",
    "        with open(data_path, 'wb') as download:\n",
    "            download.write(req.content)\n",
    "        print(\"Finished download\")\n",
    "        # extract the downloaded zip file\n",
    "        with zipfile.ZipFile(data_path, 'r') as download:\n",
    "            print(\"Unzipping data\")\n",
    "            download.extractall(data_extracted) \n",
    "        print(\"Finished extracting zip\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "    \n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted}\n",
    "    \n",
    "    \n",
    "\n",
    "flag_and_path = get_raw_ct_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    \n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    # get all the links and associated dates of upload into a dict called date_link\n",
    "    url_all = \"https://aact.ctti-clinicaltrials.org/pipe_files\"\n",
    "    response = requests.get(url_all)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    body = soup.find_all('td', attrs={'class': 'file-archive'}) #Find all\n",
    "    date_link = {}\n",
    "    for el in body:\n",
    "        tags = el.find('a')\n",
    "        try:\n",
    "            zip_name = tags.contents[0]\n",
    "            date = zip_name.split(\"_\")[0]\n",
    "            date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "            date_link[date] = tags.get('href')\n",
    "        except:\n",
    "            pass\n",
    "    # get the date of the latest upload\n",
    "    latest_file_date = max(date_link.keys())\n",
    "    \n",
    "    # get the corresponding download link of the latest upload so we can download the raw data\n",
    "    url = date_link[latest_file_date]\n",
    "    data_dir = \"{}/data\".format(pathlib.Path.cwd().parents[0])\n",
    "    data_extracted = data_dir + \"/{}_extracted\".format(latest_file_date)\n",
    "    data_path = pathlib.Path.cwd().parents[0] / \"{}/{}_pipe-delimited-export.zip\".format(data_dir, latest_file_date)\n",
    "\n",
    "    # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "    if not os.path.exists(data_path):\n",
    "        # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        term_program_flag = False  \n",
    "        print(\"Downloading Clinical Trial data as of {}\".format(latest_file_date))\n",
    "        req = requests.get(url)\n",
    "        with open(data_path, 'wb') as download:\n",
    "            download.write(req.content)\n",
    "        print(\"Finished download\")\n",
    "        # extract the downloaded zip file\n",
    "        with zipfile.ZipFile(data_path, 'r') as download:\n",
    "            print(\"Unzipping data\")\n",
    "            download.extractall(data_extracted) \n",
    "        print(\"Finished extracting zip\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "    \n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted}\n",
    "    \n",
    "    \n",
    "flag_and_path = get_raw_ct_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b31b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_raw_ct_data(data_extracted):\n",
    "    \n",
    "    # read in pipe-delimited files\n",
    "    conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "    interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "    browse_conditions_df = pd.read_csv(data_extracted + '/browse_conditions.txt', sep='|', index_col=False, header=0)\n",
    "    browse_interventions_df = pd.read_csv(data_extracted + '/browse_interventions.txt', sep='|', index_col=False, header=0)\n",
    "\n",
    "    # rename and drop df relevant columns to prepare for merging\n",
    "    interventions_df = interventions_df.rename(columns={'id': 'int_id',\n",
    "                                                        'nct_id': 'int_nctid',\n",
    "                                                        'intervention_type': 'int_type',\n",
    "                                                        'name': 'int_name',\n",
    "                                                        'description': 'int_description'})\n",
    "    interventions_df = interventions_df.drop(columns=['int_id', 'int_description'])\n",
    "    conditions_df = conditions_df.rename(columns={'id': 'con_id',\n",
    "                                                  'nct_id': 'con_nctid',\n",
    "                                                  'name': 'con_name',\n",
    "                                                  'downcase_name': 'con_downcase_name'})\n",
    "    conditions_df = conditions_df.drop(columns=['con_id', 'con_name'])\n",
    "    browse_interventions_df = browse_interventions_df.rename(columns={'id': 'browseint_id',\n",
    "                                                                      'nct_id': 'browseint_nctid',\n",
    "                                                                      'mesh_term': 'browseint_meshterm',\n",
    "                                                                      'downcase_mesh_term': 'browseint_meshterm_downcase',\n",
    "                                                                      'mesh_type': 'browseint_meshtype'})\n",
    "\n",
    "    browse_interventions_df = browse_interventions_df.drop(columns=['browseint_id', 'browseint_meshterm'])\n",
    "    browse_conditions_df = browse_conditions_df.rename(columns={'id': 'browsecon_id',\n",
    "                                                                'nct_id': 'browsecon_nctid',\n",
    "                                                                'mesh_term': 'browsecon_meshterm',\n",
    "                                                                'downcase_mesh_term': 'browsecon_meshterm_downcase',\n",
    "                                                                'mesh_type': 'browsecon_meshtype'})\n",
    "    browse_conditions_df = browse_conditions_df.drop(columns=['browsecon_id', 'browsecon_meshterm'])                                                                                                                          \n",
    "\n",
    "    # merge conditions_df and interventions_df since they have relevant terms \n",
    "    df = pd.merge(conditions_df, interventions_df, left_on='con_nctid', right_on = 'int_nctid')\n",
    "    df_dedup = df.drop_duplicates(subset = ['con_downcase_name', 'int_name', 'con_nctid'],\n",
    "                                          keep = 'first').reset_index(drop = True)\n",
    "    return df_dedup\n",
    "\n",
    "ct_data = merge_raw_ct_data(flag_and_path.get(\"data_extracted_path\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886405d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ct_data(ct_data):    \n",
    "    # obtain relevant columns\n",
    "    ct_extract = pd.DataFrame(ct_data[['con_nctid', 'con_downcase_name', 'int_type', 'int_name']])\n",
    "    ct_extract = ct_extract.rename(columns={'con_nctid': 'nctid'})\n",
    "    # get CURIE column for nct_id column (https://bioregistry.io/registry/clinicaltrials)\n",
    "\n",
    "    ct_extract['nctid_curie'] = ct_extract['nctid']\n",
    "    ct_extract['nctid_curie'] = 'clinicaltrials:' + ct_extract['nctid'].astype(str)  # generate CURIEs for each clinical trials study\n",
    "\n",
    "    ct_final = pd.DataFrame(columns=['subject','predicate','object', 'subject_name','object_name','category'])\n",
    "\n",
    "    ct_final['subject'] = 'condition:' + ct_extract['con_downcase_name'].astype(str)\n",
    "    ct_final['predicate'] = 'biolink:related_to'\n",
    "    ct_final['object'] = 'intervention:' + ct_extract['int_name'].astype(str)   # this will not all be RxNorm CURIEs since some interventions are not drugs\n",
    "    ct_final.subject_name = ct_extract.con_downcase_name\n",
    "    ct_final.object_name = ct_extract.int_name\n",
    "    ct_final.category = 'biolink:Association'\n",
    "    ct_final['nctid'] = ct_extract['nctid']\n",
    "    ct_final['nctid_curie'] = ct_extract['nctid_curie']\n",
    "\n",
    "    return(ct_final)\n",
    "\n",
    "\n",
    "ct_final = preprocess_ct_data(ct_data)\n",
    "ct_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3da527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metamap(input_term, source_restriction, metamap_dirs):\n",
    "        \n",
    "    def start_metamap_servers():\n",
    "        # Start servers\n",
    "        os.system(metamap_dirs['metamap_base_dir'] + metamap_pos_server_dir + ' start') # Part of speech tagger\n",
    "        os.system(metamap_dirs['metamap_base_dir'] + metamap_wsd_server_dir + ' start') # Word sense disambiguation \n",
    "        # # Sleep a bit to give time for these servers to start up\n",
    "        sleep(60)\n",
    "\n",
    "    def stop_metamap_servers():\n",
    "        # Stop servers\n",
    "        os.system(metamap_dirs['metamap_base_dir'] + metamap_pos_server_dir + ' stop') # Part of speech tagger\n",
    "        os.system(metamap_dirs['metamap_base_dir'] + metamap_wsd_server_dir + ' stop') # Word sense disambiguation \n",
    "        \n",
    "    start_metamap_servers()    \n",
    "    mm = MetaMap.get_instance(metamap_dirs['metamap_base_dir'] + metamap_dirs['metamap_bin_dir'])\n",
    "    concepts_dict = dict()\n",
    "    if all(x is None for x in source_restriction):\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 word_sense_disambiguation = True,\n",
    "                                                 prune = 10,\n",
    "                                                 composite_phrase = 1)\n",
    "            concepts_dict[input_term] = concepts\n",
    "        except:\n",
    "            concepts_dict[input_term] = None \n",
    "    else:\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 word_sense_disambiguation = True,\n",
    "                                                 restrict_to_sources=source_restriction,\n",
    "                                                 prune = 10,\n",
    "                                                 composite_phrase = 1)\n",
    "            concepts_dict[input_term] = concepts\n",
    "        except:\n",
    "            concepts_dict[input_term] = None \n",
    "    stop_metamap_servers()\n",
    "    return(concepts_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08644d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_transformations(ct_final, metamap_dirs):\n",
    "    # prep a list to hold dicts of de-asciied cols from the ClinTrials df\n",
    "    transformations = {}\n",
    "\n",
    "    # get unique lists of the concepts to MetaMap from the ClinTrials df\n",
    "    conditions = list(ct_final['subject_name'].unique())\n",
    "    conditions = [x for x in conditions if pd.isnull(x) == False]\n",
    "    interventions = list(ct_final['object_name'].unique())\n",
    "    interventions = [x for x in interventions if pd.isnull(x) == False]\n",
    "    \n",
    "    # report the number of unique conditions and interventions to get idea of KG at early stage\n",
    "    print(\"# of unique CONDITIONS present in Clinical Trials dataset: {}\".format(len(conditions)))\n",
    "    print(\"# of unique INTERVENTIONS present in Clinical Trials dataset: {}\".format(len(interventions)))\n",
    "    \n",
    "    # only remove non-ASCII for MetaMap versions prior to 2020 release\n",
    "    # if stmnt is checking if 20 for 2020 ---> bin/metamap20 is MetaMap release 2020\n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))]\n",
    "    if metamap_version[0] >= 20:\n",
    "        print(\"MetaMap version >= 2020, no processing necessary\")\n",
    "        conditions_unchanged = dict((i, i) for i in conditions)\n",
    "        interventions_unchanged = dict((i, i) for i in interventions) \n",
    "        transformations[\"conditions\"] = conditions_unchanged\n",
    "        transformations[\"interventions\"] = interventions_unchanged      \n",
    "    else:\n",
    "        print(\"Removing ASCII char from cols.\")\n",
    "        print(\"This step is unnecessary for MetaMap 2020+\")\n",
    "        conditions_translated = dict((i, de_ascii_er(i)) for i in conditions)\n",
    "        interventions_translated = dict((i, de_ascii_er(i)) for i in interventions)   \n",
    "        \n",
    "        transformations[\"conditions\"] = conditions_translated\n",
    "        transformations[\"interventions\"] = interventions_translated  \n",
    "        \n",
    "    con_test = random.sample(transformations.get(\"conditions\"), 10)\n",
    "#     int_test = random.sample(transformations[\"interventions\"].values(), 10)\n",
    "    \n",
    "    # MAP CONDITIONS AND INTERVENTIONS TO METAMAP\n",
    "    vocab_restriction = ['SNOMEDCT_US', 'SNOMEDCT_VET', 'ICD10CM', 'ICD10CM', 'ICD10PCS', 'ICD9CM', 'ICD9CM']\n",
    "#     metamapped_conditions = multiprocessing.Pool(multiprocessing.cpu_count() - 1).starmap(run_metamap,\n",
    "#         zip(list(transformations['conditions'].values()),\n",
    "#             [vocab_restriction]*len(list(transformations['conditions'].values())), metamap_dirs))\n",
    "#     metamapped_interventions = multiprocessing.Pool(multiprocessing.cpu_count() - 1).starmap(run_metamap, \n",
    "#                                                                                              zip(list(transformations['interventions'].values()),\n",
    "#                                                                                                  [[None]]*len(list(transformations['conditions'].values()))))\n",
    "    \n",
    "    metamapped_conditions = multiprocessing.Pool(multiprocessing.cpu_count() - 1).starmap(run_metamap,\n",
    "        zip(list(con_test),\n",
    "            [vocab_restriction]*len(list(transformations['conditions'].values())), metamap_dirs))\n",
    "#     metamapped_interventions = multiprocessing.Pool(multiprocessing.cpu_count() - 1).starmap(run_metamap, \n",
    "#                                                                                              zip(list(int_test),\n",
    "#                                                                                                  [[None]]*len(list(transformations['conditions'].values()))))\n",
    "\n",
    "    \n",
    "    # only remove non-ASCII for MetaMap versions prior to 2020 release\n",
    "#     metamap_dir = \"{}/metamap\".format(pathlib.Path.cwd().parents[0]) # uncomment for  server edition\n",
    "#     metamap_dir = \"/Volumes/TOSHIBA_EXT/ISB/clinical_trials\"\n",
    "#     print(list(pathlib.Path(metamap_base_dir).glob('*.tar.bz2'))) # uncomment for use on server\n",
    "#     print(list(pathlib.Path(metamap_dir).glob('*.tar.bz2')))   # uncomment for use on local\n",
    "#     print(metamap_base_dir)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     if not all(condition.isascii() for condition in conditions):\n",
    "#         print(\"Non-ASCII chars are detected in Conditions col from ClinTrial (not checking other cols), proceed with text processing\")\n",
    "#         print(\"This step is unnecessary for MetaMap 2020+\")\n",
    "#         # process lists to remove non-ascii chars (this is not required for MetaMap 2020!!!!)\n",
    "#         conditions_translated = dict((i, preprocess_cols_for_metamap(i)) for i in conditions)\n",
    "#         interventions_translated = dict((i, preprocess_cols_for_metamap(i)) for i in interventions)\n",
    "        \n",
    "#         metamap_compatible[\"conditions\"] = conditions_translated\n",
    "#         metamap_compatible[\"interventions\"] = interventions_translated\n",
    "\n",
    "#     else:\n",
    "#         print(\"No non-ASCII chars detected or they are present, but we're not using MetaMap versions prior to 2020, no text processing required\")\n",
    "#         metamap_compatible[\"conditions\"] = conditions\n",
    "#         metamap_compatible[\"interventions\"] = interventions\n",
    "\n",
    "#     return(metamap_compatible)\n",
    "\n",
    "          \n",
    "\n",
    "transformations = cols_transformations(ct_final, metamap_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations['conditions'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0027a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9780b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6a155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222a5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e4377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76331e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474dbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver():\n",
    "    metamap_dirs = check_os()\n",
    "    flag_and_path = get_raw_ct_data()\n",
    "    ct_data = merge_raw_ct_data(flag_and_path.get(\"data_extracted_path\"))\n",
    "    ct_final = preprocess_ct_data(ct_data)\n",
    "    metamap_compatible_cols = cols_transformations(ct_final, metamap_dirs)\n",
    "\n",
    "driver()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver():\n",
    "    metamap_dirs = check_os()\n",
    "    flag_and_path = get_raw_ct_data()\n",
    "    \n",
    "    if flag_and_path.get(\"term_program_flag\"):\n",
    "        print(\"Exiting\")\n",
    "        quit()\n",
    "    else:\n",
    "        ct_data = merge_raw_ct_data(flag_and_path.get(\"data_extracted_path\"))\n",
    "        ct_final = preprocess_ct_data(ct_data)\n",
    "#         metamap_compatible_cols = select_cols(ct_final)\n",
    "\n",
    "driver()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775af7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297e60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4431a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463dd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51162951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede116f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://aact.ctti-clinicaltrials.org/static/exported_files/daily/{}_pipe-delimited-export.zip\".format(latest_file_date)\n",
    "data_dir = \"{}/data\".format(pathlib.Path.cwd().parents[0])\n",
    "data_extracted = data_dir + \"/{}_extracted\".format(latest_file_date)\n",
    "data_path = pathlib.Path.cwd().parents[0] / \"{}/{}_pipe-delimited-export.zip\".format(data_dir, latest_file_date)\n",
    "# print(data_path)\n",
    "if not os.path.exists(data_path):\n",
    "    req = requests.get(url)\n",
    "    with open(data_path, 'wb') as download:\n",
    "        download.write(req.content)      \n",
    "    \n",
    "#     driver()\n",
    "else:\n",
    "    print(\"KG is already up to date.\")\n",
    "    \n",
    "print(\"Finished download\")\n",
    "\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e958c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "browse_conditions_df = pd.read_csv(data_extracted + '/browse_conditions.txt', sep='|', index_col=False, header=0)\n",
    "browse_interventions_df = pd.read_csv(data_extracted + '/browse_interventions.txt', sep='|', index_col=False, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb17c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = [folds for folds in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,folds))]\n",
    "# dir_list = [i.split(\"_\")[0] for i in dir_list if \"_extracted\" in i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667dce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir_list = ['20220712', '20210812']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dates = [dt.datetime.strptime(date, '%Y%m%d').date() for date in test_dir_list] # convert all strings in list into datetime objects\n",
    "file_dates\n",
    "latest_file_date = max(file_dates)\n",
    "latest_file_date = latest_file_date.strftime('%m/%d/%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdb291",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_file_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f)) and  f.endswith(\".zip\")]\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [i.split(\"_\")[0] for i in dir_list]\n",
    "file_list\n",
    "# file_dates = [dt.datetime.strptime(date, '%Y%m%d').date() for date in file_list if dt.datetime.strptime(date, '%Y%m%d').date() ] # convert all strings in list into datetime objects\n",
    "\n",
    "def get_dates(date):\n",
    "    try:\n",
    "        return dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "    except ValueError:\n",
    "        pass\n",
    "  \n",
    "  \n",
    "# # list comprehension\n",
    "file_list = [get_dates(x) for x in file_list]\n",
    "file_list = list(filter(None, file_list))\n",
    "\n",
    "latest_file_date = max(file_list)\n",
    "latest_file_date = latest_file_date.strftime('%m/%d/%Y')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_file_date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct_extract_env",
   "language": "python",
   "name": "ct_extract_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
