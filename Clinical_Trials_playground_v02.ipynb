{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93827900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import pathlib\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re\n",
    "from collections import namedtuple\n",
    "# For pausing\n",
    "from time import sleep\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home\"\n",
    "import pickle\n",
    "from itertools import repeat\n",
    "import psycopg2\n",
    "import pandas.io.sql as sqlio\n",
    "# adding Folder_2/subfolder to the system path\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "from pymetamap import MetaMap\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import lxml\n",
    "\n",
    "# from Authentication import *\n",
    "import requests\n",
    "import json\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cf5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup UMLS Server global vars\n",
    "# # metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "# # metamap_base_dir = \"/users/knarsinh/projects/clinical_trials/metamap/public_mm/\"\n",
    "# # metamap_bin_dir = 'bin/metamap18' # uncomment for running on local\n",
    "# metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "# metamap_bin_dir = 'bin/metamap20'\n",
    "# metamap_pos_server_dir = 'bin/skrmedpostctl'\n",
    "# metamap_wsd_server_dir = 'bin/wsdserverctl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d155b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup UMLS Server global vars\n",
    "\n",
    "global metamap_pos_server_dir\n",
    "global metamap_wsd_server_dir\n",
    "metamap_pos_server_dir = 'bin/skrmedpostctl'\n",
    "metamap_wsd_server_dir = 'bin/wsdserverctl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf538d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "    return [metamap_base_dir, metamap_bin_dir]\n",
    "        \n",
    "metamap_dirs = check_os()\n",
    "\n",
    "# /Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6db270fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/data/2022-07-19_pipe-delimited-export.zip\n",
      "Downloading Clinical Trial data as of 2022-07-19\n",
      "Finished download\n",
      "reached zip file extracting point\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_raw_ct_data():\n",
    "    \n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    url_all = \"https://aact.ctti-clinicaltrials.org/pipe_files\"\n",
    "    response = requests.get(url_all)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    body = soup.find_all('td', attrs={'class': 'file-archive'}) #Find all\n",
    "    date_link = {}\n",
    "    for el in body:\n",
    "        tags = el.find('a')\n",
    "        try:\n",
    "            zip_name = tags.contents[0]\n",
    "            date = zip_name.split(\"_\")[0]\n",
    "            date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "            date_link[date] = tags.get('href')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "#     print(date_link)\n",
    "    latest_file_date = max(date_link.keys())\n",
    "    \n",
    "    url = date_link[latest_file_date]\n",
    "    data_dir = \"{}/data\".format(pathlib.Path.cwd().parents[0])\n",
    "    data_extracted = data_dir + \"/{}_extracted\".format(latest_file_date)\n",
    "    data_path = pathlib.Path.cwd().parents[0] / \"{}/{}_pipe-delimited-export.zip\".format(data_dir, latest_file_date)\n",
    "    print(data_path)\n",
    "#     print(data_extracted)\n",
    "#     print(data_dir)\n",
    "    if not os.path.exists(data_path):\n",
    "#     if os.path.exists(data_path):\n",
    "        term_program_flag = False  \n",
    "        print(\"Downloading Clinical Trial data as of {}\".format(latest_file_date))\n",
    "        req = requests.get(url)\n",
    "        \n",
    "        with open(data_path, 'wb') as download:\n",
    "            download.write(req.content)\n",
    "        print(\"Finished download\")\n",
    "#     extract the downloaded zip file\n",
    "        with zipfile.ZipFile(data_path, 'r') as download:\n",
    "            print(\"reached zip file extracting point\")\n",
    "            download.extractall(data_extracted) \n",
    "#         print(\"Finished extracting zip\")\n",
    "#     else:\n",
    "#         print(\"KG is already up to date.\")\n",
    "    \n",
    "    return term_program_flag\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tags = el.find('a')\n",
    "#     try:\n",
    "#         if 'href' in tags.attrs:   # looking for href inside anchor tag    \n",
    "\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             if 'href' in tags.attrs:   # looking for href inside anchor tag    \n",
    "#                 link = tags.get('href')\n",
    "#                 print(link)\n",
    "                \n",
    "#                 print(link)\n",
    "#                 link = tags.get('href')\n",
    "#                 print(link)\n",
    "#                 links.append(link)\n",
    "#                 last_upload = link.split(\"/\")[-1]\n",
    "#                 zip_files.append(last_upload)\n",
    "#                 date_upload = last_upload.split(\"_\")[0]\n",
    "#                 upload_dates.append(date_upload)    # appending link to list of links\n",
    "#         except:    # pass if list missing anchor tag or anchor tag does not has a href params \n",
    "#             pass\n",
    "#     print(upload_dates)\n",
    "#     upload_dates = [dt.datetime.strptime(date, '%Y%m%d').date() for date in upload_dates] # convert all strings in list into datetime objects\n",
    "#     latest_file_date = max(upload_dates)\n",
    "#     latest_file_date = latest_file_date.strftime('%Y%m%d') \n",
    "    \n",
    "#     url = \"https://aact.ctti-clinicaltrials.org/static/exported_files/daily/{}_pipe-delimited-export.zip\".format(latest_file_date)\n",
    "#     data_dir = \"{}/data\".format(pathlib.Path.cwd().parents[0])\n",
    "#     data_extracted = data_dir + \"/{}_extracted\".format(latest_file_date)\n",
    "#     data_path = pathlib.Path.cwd().parents[0] / \"{}/{}_pipe-delimited-export.zip\".format(data_dir, latest_file_date)\n",
    "#     # print(data_path)\n",
    "\n",
    "#     if not os.path.exists(data_path):\n",
    "#         term_program_flag = False   \n",
    "#         req = requests.get(url)\n",
    "#         print(\"Downloading Clinical Trial data as of {}\".format(latest_file_date))\n",
    "#         with open(data_path, 'wb') as download:\n",
    "#             download.write(req.content)\n",
    "#         print(\"Finished download\")\n",
    "#     # extract the downloaded zip file\n",
    "#         with zipfile.ZipFile(data_path, 'r') as download:\n",
    "#             download.extractall(data_extracted) \n",
    "#         print(\"Finished extracting zip\")\n",
    "# #         driver()\n",
    "#     else:\n",
    "#         print(\"KG is already up to date.\")\n",
    "    \n",
    "#     return term_program_flag\n",
    "    \n",
    "get_raw_ct_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b45b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_raw_ct_data():\n",
    "    \n",
    "    # read in pipe-delimited files\n",
    "    conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "    interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "    browse_conditions_df = pd.read_csv(data_extracted + '/browse_conditions.txt', sep='|', index_col=False, header=0)\n",
    "    browse_interventions_df = pd.read_csv(data_extracted + '/browse_interventions.txt', sep='|', index_col=False, header=0)\n",
    "\n",
    "    # rename and drop df relevant columns to prepare for merging\n",
    "    interventions_df = interventions_df.rename(columns={'id': 'int_id',\n",
    "                                                        'nct_id': 'int_nctid',\n",
    "                                                        'intervention_type': 'int_type',\n",
    "                                                        'name': 'int_name',\n",
    "                                                        'description': 'int_description'})\n",
    "    interventions_df = interventions_df.drop(columns=['int_id', 'int_description'])\n",
    "    conditions_df = conditions_df.rename(columns={'id': 'con_id',\n",
    "                                                  'nct_id': 'con_nctid',\n",
    "                                                  'name': 'con_name',\n",
    "                                                  'downcase_name': 'con_downcase_name'})\n",
    "    conditions_df = conditions_df.drop(columns=['con_id', 'con_name'])\n",
    "    browse_interventions_df = browse_interventions_df.rename(columns={'id': 'browseint_id',\n",
    "                                                                      'nct_id': 'browseint_nctid',\n",
    "                                                                      'mesh_term': 'browseint_meshterm',\n",
    "                                                                      'downcase_mesh_term': 'browseint_meshterm_downcase',\n",
    "                                                                      'mesh_type': 'browseint_meshtype'})\n",
    "\n",
    "    browse_interventions_df = browse_interventions_df.drop(columns=['browseint_id', 'browseint_meshterm'])\n",
    "    browse_conditions_df = browse_conditions_df.rename(columns={'id': 'browsecon_id',\n",
    "                                                                'nct_id': 'browsecon_nctid',\n",
    "                                                                'mesh_term': 'browsecon_meshterm',\n",
    "                                                                'downcase_mesh_term': 'browsecon_meshterm_downcase',\n",
    "                                                                'mesh_type': 'browsecon_meshtype'})\n",
    "    browse_conditions_df = browse_conditions_df.drop(columns=['browsecon_id', 'browsecon_meshterm'])                                                                                                                          \n",
    "\n",
    "    # merge conditions_df and interventions_df since they have relevant terms \n",
    "    df = pd.merge(conditions_df, interventions_df, left_on='con_nctid', right_on = 'int_nctid')\n",
    "    df_dedup = df.drop_duplicates(subset = ['con_downcase_name', 'int_name'],\n",
    "                                          keep = 'first').reset_index(drop = True)\n",
    "    return df_dedup\n",
    "\n",
    "ct_data = merge_raw_ct_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb404973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ct_data(ct_data):    \n",
    "    # obtain relevant columns\n",
    "    ct_extract = pd.DataFrame(ct_data[['con_nctid', 'con_downcase_name', 'int_type', 'int_name']])\n",
    "    ct_extract = ct_extract.rename(columns={'con_nctid': 'nctid'})\n",
    "    # get CURIE column for nct_id column (https://bioregistry.io/registry/clinicaltrials)\n",
    "\n",
    "    ct_extract['nctid_curie'] = ct_extract['nctid']\n",
    "    ct_extract['nctid_curie'] = 'clinicaltrials:' + ct_extract['nctid'].astype(str)  # generate CURIEs for each clinical trials study\n",
    "\n",
    "    ct_final = pd.DataFrame(columns=['subject','predicate','object', 'subject_name','object_name','category'])\n",
    "\n",
    "    ct_final['subject'] = 'condition:' + ct_extract['con_downcase_name'].astype(str)\n",
    "    ct_final['predicate'] = 'biolink:related_to'\n",
    "    ct_final['object'] = 'intervention:' + ct_extract['int_name'].astype(str)   # this will not all be RxNorm CURIEs since some interventions are not drugs\n",
    "    ct_final.subject_name = ct_extract.con_downcase_name\n",
    "    ct_final.object_name = ct_extract.int_name\n",
    "    ct_final.category = 'biolink:Association'\n",
    "    ct_final['nctid'] = ct_extract['nctid']\n",
    "    ct_final['nctid_curie'] = ct_extract['nctid_curie']\n",
    "\n",
    "    return(ct_final)\n",
    "\n",
    "\n",
    "ct_final = preprocess_ct_data(ct_data)\n",
    "ct_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a38a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_cols(ct_final, metamap_dirs):\n",
    "    # prep a list to hold dicts of de-asciied cols from the ClinTrials df\n",
    "    processed_ct_cols = []\n",
    "    metamap_compatible = {}\n",
    "    col_transformations = {}\n",
    "\n",
    "    # get unique lists of the concepts to MetaMap from the ClinTrials df\n",
    "    conditions = list(set(list(ct_final['subject_name'])))\n",
    "    interventions = list(filter(None, ct_final['object_name'].unique()))\n",
    "    \n",
    "    # only remove non-ASCII for MetaMap versions prior to 2020 release\n",
    "#     metamap_dir = \"{}/metamap\".format(pathlib.Path.cwd().parents[0]) # uncomment for  server edition\n",
    "#     metamap_dir = \"/Volumes/TOSHIBA_EXT/ISB/clinical_trials\"\n",
    "#     print(list(pathlib.Path(metamap_base_dir).glob('*.tar.bz2'))) # uncomment for use on server\n",
    "#     print(list(pathlib.Path(metamap_dir).glob('*.tar.bz2')))   # uncomment for use on local\n",
    "    print(metamap_base_dir)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     if not all(condition.isascii() for condition in conditions):\n",
    "#         print(\"Non-ASCII chars are detected in Conditions col from ClinTrial (not checking other cols), proceed with text processing\")\n",
    "#         print(\"This step is unnecessary for MetaMap 2020+\")\n",
    "#         # process lists to remove non-ascii chars (this is not required for MetaMap 2020!!!!)\n",
    "#         conditions_translated = dict((i, preprocess_cols_for_metamap(i)) for i in conditions)\n",
    "#         interventions_translated = dict((i, preprocess_cols_for_metamap(i)) for i in interventions)\n",
    "        \n",
    "#         metamap_compatible[\"conditions\"] = conditions_translated\n",
    "#         metamap_compatible[\"interventions\"] = interventions_translated\n",
    "\n",
    "#     else:\n",
    "#         print(\"No non-ASCII chars detected or they are present, but we're not using MetaMap versions prior to 2020, no text processing required\")\n",
    "#         metamap_compatible[\"conditions\"] = conditions\n",
    "#         metamap_compatible[\"interventions\"] = interventions\n",
    "\n",
    "#     return(metamap_compatible)\n",
    "\n",
    "\n",
    "select_cols(ct_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82848b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eccb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c511a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf32a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65cc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98ad3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e591dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009017dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver():\n",
    "    metamap_dirs = check_os()\n",
    "    term_program_flag = get_raw_ct_data()\n",
    "    ct_data = merge_raw_ct_data()\n",
    "    ct_final = preprocess_ct_data(ct_data)\n",
    "    metamap_compatible_cols = select_cols(ct_final, metamap_dirs)\n",
    "\n",
    "driver()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver():\n",
    "    metamap_dirs = check_os()\n",
    "    term_program_flag = get_raw_ct_data()\n",
    "    if term_program_flag:\n",
    "        print(\"Exiting\")\n",
    "#         quit()\n",
    "    else:\n",
    "        ct_data = merge_raw_ct_data()\n",
    "        ct_final = preprocess_ct_data(ct_data)\n",
    "#         metamap_compatible_cols = select_cols(ct_final)\n",
    "\n",
    "driver()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446abfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfe15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da110c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d28908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59e644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://aact.ctti-clinicaltrials.org/static/exported_files/daily/{}_pipe-delimited-export.zip\".format(latest_file_date)\n",
    "data_dir = \"{}/data\".format(pathlib.Path.cwd().parents[0])\n",
    "data_extracted = data_dir + \"/{}_extracted\".format(latest_file_date)\n",
    "data_path = pathlib.Path.cwd().parents[0] / \"{}/{}_pipe-delimited-export.zip\".format(data_dir, latest_file_date)\n",
    "# print(data_path)\n",
    "if not os.path.exists(data_path):\n",
    "    req = requests.get(url)\n",
    "    with open(data_path, 'wb') as download:\n",
    "        download.write(req.content)      \n",
    "    \n",
    "#     driver()\n",
    "else:\n",
    "    print(\"KG is already up to date.\")\n",
    "    \n",
    "print(\"Finished download\")\n",
    "\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1599f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "browse_conditions_df = pd.read_csv(data_extracted + '/browse_conditions.txt', sep='|', index_col=False, header=0)\n",
    "browse_interventions_df = pd.read_csv(data_extracted + '/browse_interventions.txt', sep='|', index_col=False, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec91031",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = [folds for folds in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,folds))]\n",
    "# dir_list = [i.split(\"_\")[0] for i in dir_list if \"_extracted\" in i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir_list = ['20220712', '20210812']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a705f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dates = [dt.datetime.strptime(date, '%Y%m%d').date() for date in test_dir_list] # convert all strings in list into datetime objects\n",
    "file_dates\n",
    "latest_file_date = max(file_dates)\n",
    "latest_file_date = latest_file_date.strftime('%m/%d/%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_file_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62adcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f)) and  f.endswith(\".zip\")]\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [i.split(\"_\")[0] for i in dir_list]\n",
    "file_list\n",
    "# file_dates = [dt.datetime.strptime(date, '%Y%m%d').date() for date in file_list if dt.datetime.strptime(date, '%Y%m%d').date() ] # convert all strings in list into datetime objects\n",
    "\n",
    "def get_dates(date):\n",
    "    try:\n",
    "        return dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "    except ValueError:\n",
    "        pass\n",
    "  \n",
    "  \n",
    "# # list comprehension\n",
    "file_list = [get_dates(x) for x in file_list]\n",
    "file_list = list(filter(None, file_list))\n",
    "\n",
    "latest_file_date = max(file_list)\n",
    "latest_file_date = latest_file_date.strftime('%m/%d/%Y')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_file_date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
