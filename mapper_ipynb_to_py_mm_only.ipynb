{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966a883b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "# lets you preint multiple outputs per cell, not just last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263f3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "import concurrent\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import sys\n",
    "import urllib\n",
    "import zipfile\n",
    "import csv\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0709cfa-2550-4a44-8525-6b2beb10b46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_seconds_to_hms(seconds):\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bacfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_sort_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    # sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "\n",
    "def get_token_set_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_set_ratio(str1, str2)\n",
    "    except:\n",
    "        return None  \n",
    "# set_ratio = np.vectorize(get_token_set_ratio)\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "# sim_score = np.vectorize(get_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c7e3d8-1c7a-44a2-afe5-6721f7ca2438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "\n",
    "    # get all the links and associated dates of upload into a dict called date_link\n",
    "    url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "    response = requests.get(url_all)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    body = soup.find_all('option') #Find all\n",
    "    date_link = {}\n",
    "    for el in body:\n",
    "        tags = el.find('a')\n",
    "        try:\n",
    "            zip_name = tags.contents[0].split()[0]\n",
    "            date = zip_name.split(\"_\")[0]\n",
    "            date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "            date_link[date] = tags.get('href')\n",
    "        except:\n",
    "            pass\n",
    "    latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "    url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "    date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "    data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "    data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "    data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Attempting download of Clinical Trial data as of {}\\n\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"\\nFailed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory. This should be the only item besides the cache file in the directory at this time.\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                # list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    # latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    pattern = os.path.join(data_dir, \"*.zip\")\n",
    "                    zip_file = glob.glob(pattern) # look for file in directory that ends in \".zip\"\n",
    "                    zip_file = zip_file[0]\n",
    "                    print(\"File found at: \")\n",
    "                    print(zip_file)\n",
    "                    # print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(zip_file, 'r') as download:\n",
    "                            print(\"Unzipping data into\")\n",
    "                            cttime = os.path.getctime(zip_file)\n",
    "                            date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                            data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                            print(data_extracted)\n",
    "                            download.extractall(data_extracted)\n",
    "                    except:\n",
    "                        pattern = os.path.join(data_dir, \"*_extracted\")\n",
    "                        extracted_file = glob.glob(pattern) # look for file in directory that ends in \"_extracted\"\n",
    "                        data_extracted = extracted_file[0]\n",
    "                        extracted_name = os.path.basename(os.path.normpath(extracted_file[0]))\n",
    "                        date_string = extracted_name.replace('_extracted', '')\n",
    "                        print(\"Assuming data is already unzipped\")\n",
    "                        \n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bccd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "        interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0)\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            interventions_alts_df = interventions_alts_df.sample(n=subset_size)\n",
    "\n",
    "    return {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bea59a-3c2e-46dc-a65d-eff25ce70675",
   "metadata": {},
   "source": [
    "# MAP TERMS TO EXISTING CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef911bda-e40e-46f7-879f-f7f54faf206a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def term_list_to_cache(df_dict, flag_and_path):\n",
    "    cache_file = \"metamapped_terms_cache.tsv\"\n",
    "    cache_df = pd.read_csv(cache_file, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    # retrieve conditions from the new data dump and from the cache, compare, get the diff so as to map just newly encountered terms (conditions)\n",
    "    conditions_list = df_dict['conditions'].name.unique().tolist()\n",
    "    # conditions_list = map(str, conditions_list) \n",
    "    conditions_list = [str(i) for i in conditions_list]\n",
    "    conditions_list = set([i.lower() for i in conditions_list])\n",
    "\n",
    "    conditions_cache = cache_df[cache_df[\"term_type\"] == \"condition\"]\n",
    "    conditions_cache = conditions_cache['clin_trial_term'].unique().tolist()\n",
    "    conditions_cache = set([i.lower() for i in conditions_cache])\n",
    "\n",
    "    conditions_new = [x for x in conditions_list if x not in conditions_cache]\n",
    "\n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    conditions = conditions[conditions['downcase_name'].isin(conditions_new)]\n",
    "\n",
    "    # retrieve interventions from the new data dump and from the cache, compare, get the diff so as to map just newly encountered terms (interventions)\n",
    "    interventions_list = df_dict['interventions'].name.unique().tolist()\n",
    "    interventions_lists = [str(i) for i in interventions_list]\n",
    "    # interventions_list = map(str, interventions_list) \n",
    "    interventions_list = set([i.lower() for i in interventions_list])\n",
    "\n",
    "    interventions_cache = cache_df[cache_df[\"term_type\"] == \"intervention\"]\n",
    "    interventions_cache = interventions_cache['clin_trial_term'].unique().tolist()\n",
    "    interventions_cache = set([i.lower() for i in interventions_cache])\n",
    "\n",
    "    interventions_new = [x for x in interventions_list if x not in interventions_cache]\n",
    "\n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    interventions['downcase_name'] = interventions['name'].str.lower()\n",
    "    interventions = interventions[interventions['downcase_name'].isin(interventions_new)]\n",
    "    interventions = interventions[interventions.columns.drop('downcase_name')]\n",
    "\n",
    "    # retrieve alternate interventions from the new data dump and from the cache, and get the diff\n",
    "    interventions_alts_list = df_dict['interventions_alts'].name.unique().tolist()\n",
    "    interventions_alts_list = set([i.lower() for i in interventions_alts_list])\n",
    "\n",
    "    interventions_alts_cache = cache_df[cache_df[\"term_type\"] == \"alternate_intervention\"]\n",
    "    interventions_alts_cache = interventions_alts_cache.clin_trial_term.unique().tolist()\n",
    "    interventions_alts_cache = set([i.lower() for i in interventions_alts_cache])\n",
    "\n",
    "    interventions_alts_new = [x for x in interventions_alts_list if x not in interventions_alts_cache]\n",
    "    interventions_alts = df_dict[\"interventions_alts\"]\n",
    "\n",
    "    interventions_alts['downcase_name'] = interventions_alts['name'].str.lower()\n",
    "    interventions_alts = interventions_alts[interventions_alts['downcase_name'].isin(interventions_alts_new)]\n",
    "    interventions_alts = interventions_alts[interventions_alts.columns.drop('downcase_name')]\n",
    "    \n",
    "    df_dict_new_terms = {\"conditions\": conditions, \"interventions\": interventions, \"interventions_alts\": interventions_alts}\n",
    "\n",
    "    return df_dict_new_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f277771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2877eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'start']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'start']\n",
    "\n",
    "    # Start servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'stop']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'stop']\n",
    "    \n",
    "    # Stop servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e63785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ea402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metamap(input_term, params, mm, cond_or_inter, csv_writer):\n",
    "    from_metamap = []\n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],\n",
    "                                                )\n",
    "            for concept in concepts:\n",
    "                concept_info = []\n",
    "                concept = concept._asdict()\n",
    "                concept_info.extend([cond_or_inter,input_term])\n",
    "                concept_info.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "                from_metamap.append(concept_info)\n",
    "        except:\n",
    "            from_metamap.extend([input_term, None, None, None, None, None, None])\n",
    "    else:\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],\n",
    "                                                )\n",
    "            for concept in concepts:\n",
    "                concept_info = []\n",
    "                concept = concept._asdict()\n",
    "                concept_info.extend([cond_or_inter,input_term])\n",
    "                concept_info.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "                from_metamap.append(concept_info)\n",
    "        except:\n",
    "            from_metamap.extend([input_term, None, None, None, None, None, None])\n",
    "        \n",
    "    for result in from_metamap:\n",
    "        # print(result)\n",
    "        csv_writer.writerow(result)\n",
    "    return from_metamap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a937acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_metamap(term_list, params, cond_or_inter, flag_and_path, csv_writer):\n",
    "    LENGTH = len(term_list)  # Number of iterations required to fill progress bar (pbar)\n",
    "    pbar = tqdm(total=LENGTH, desc=\"% {}s mapped\".format(cond_or_inter), position=0, leave=True, mininterval = LENGTH/10)  # Init progress bar\n",
    "\n",
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "    with concurrent.futures.ThreadPoolExecutor((multiprocessing.cpu_count()*2) - 1) as executor:\n",
    "        futures = [executor.submit(run_metamap, term, params, mm, cond_or_inter, csv_writer) for term in term_list]\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            pbar.update(n=1)  # Increments counter\n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ef16423-4e25-4da7-b5d7-bd5b5614c0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_mappings_to_cache(flag_and_path):\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    with open(\"metamapped_terms_cache.tsv\", 'a+') as cache:\n",
    "        with open(f\"{relevant_date}_metamap_output.tsv\", 'r') as new_metamapped_terms:\n",
    "            # Read the first line from new_metamapped_terms to move the cursor\n",
    "            line = new_metamapped_terms.readline()\n",
    "\n",
    "            # Move the cursor to the position after the first line\n",
    "            while line:\n",
    "                line = new_metamapped_terms.readline()\n",
    "                if line:\n",
    "                    # Append the line to file_1\n",
    "                    cache.write(line)\n",
    "    \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "    cache = pd.read_csv(\"metamapped_terms_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv('metamapped_terms_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a21d47",
   "metadata": {},
   "source": [
    "# USE METAMAP LOCAL TO MAP REMAINING TERMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8f37ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_mm(df_dict_new_terms, flag_and_path):   # FIX THIS TO ACCEPT ONLY TERMS NOT EXISTING IN CACHE\n",
    "        \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    # some input terms have () with additional text, like an abbreviation, in them. split them out to facilitate better mapping using these regex patterns that we use to find substrings inside and outside ()\n",
    "    pattern_outside = r'(?<=\\().+?(?=\\))|([^(]+)'\n",
    "    pattern_inside = r'\\(([^)]+)\\)'\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "\n",
    "        # -------    CONDITIONS    ------- #\n",
    "        \n",
    "    print(\"\\nUsing UMLS MetaMap to get mappings for CONDITIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\\n\")\n",
    "    conditions = df_dict_new_terms[\"conditions\"][['id', 'nct_id', 'downcase_name']]\n",
    "    conditions = conditions.copy()\n",
    "    conditions.rename(columns = {'downcase_name':'orig_con',\n",
    "                                 'id': 'condition_id'},\n",
    "                      inplace = True)\n",
    "\n",
    "    if metamap_version[0] >= 20:\n",
    "        matches_outside = conditions['orig_con'].str.extract(pattern_outside)\n",
    "        conditions['orig_con_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = conditions['orig_con'].str.extract(pattern_inside)\n",
    "        conditions['orig_con_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "    else:\n",
    "        conditions['deascii_con'] = deasciier(conditions['orig_con'])\n",
    "        matches_outside = conditions['deascii_con'].str.extract(pattern_outside)\n",
    "        conditions['deascii_con_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = conditions['deascii_con'].str.extract(pattern_inside)\n",
    "        conditions['deascii_con_inside'] = matches_inside[0].fillna('')\n",
    "    \n",
    "#     see MetaMap Usage instructions: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/MM_2016_Usage.pdf\n",
    "#     condition_args = ['--sldi -I -C -J acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf -z -i -f']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    \n",
    "    # prep output file of Metamap results\n",
    "    filename = f\"{relevant_date}_metamap_output.tsv\"\n",
    "    metamap_output = open(filename, 'w+', newline='')\n",
    "    col_names = ['term_type', 'clin_trial_term','metamap_preferred_name', 'metamap_cui', 'metamap_score', 'metamap_semantic_type']\n",
    "    csv_writer = csv.writer(metamap_output, delimiter='\\t')\n",
    "    csv_writer.writerow(col_names)\n",
    "    \n",
    "    if metamap_version[0] >= 20:\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        orig_cons = conditions.orig_con.unique().tolist()\n",
    "        orig_cons = list(filter(None, orig_cons))\n",
    "        orig_cons = [str(i) for i in orig_cons]\n",
    "        parallelize_metamap(orig_con, params, \"condition\", flag_and_path, csv_writer)\n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        deascii_cons = conditions.deascii_con.unique().tolist()\n",
    "        deascii_cons = list(filter(None, deascii_cons))\n",
    "        deascii_cons = [str(i) for i in deascii_cons]\n",
    "        # deascii_cons = map(str, deascii_cons)\n",
    "        parallelize_metamap(deascii_cons, params, \"condition\", flag_and_path, csv_writer)\n",
    "        \n",
    "        \"\"\" If the substring that was either outside or inside the () is identical to the term from which it came from, or actually any of the columns have the same value, put None in that cell/put None where that term is duplicated \"\"\"    \n",
    "    # Iterate through each column in the DataFrame\n",
    "    for col1 in conditions.columns:\n",
    "        for col2 in conditions.columns:\n",
    "            # Skip comparing a column with itself\n",
    "            if col1 != col2:\n",
    "                # Check if the values in col2 are duplicates of col1\n",
    "                conditions[col2] = conditions.apply(lambda row: row[col2] if row[col2] != row[col1] else None, axis=1)\n",
    "    # Drop duplicate columns (keeping the first instance)\n",
    "    conditions = conditions.T.drop_duplicates().T\n",
    "\n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output conditions to TSV\n",
    "    \n",
    "        # -------    INTERVENTIONS    ------- #\n",
    "        \n",
    "    print(\"\\nUsing UMLS MetaMap to get mappings for INTERVENTIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\\n\")\n",
    "    \n",
    "    \"\"\" Interventions requires unique handling. Another table gives possible alternate names for the interventions in addition to the \"original\" names. \n",
    "        We may map on the alternate names column\n",
    "        We take the interventions, take the ascii and deasciied versions of them,\n",
    "        and split substrings in parentheses out of them. We perform MetaMapping on the\n",
    "        original term or the deasciied term dependinging on what operating system we\n",
    "        are on. If the mapped term passes the fuzzy scoring thesholds for any of the\n",
    "        terms (original, deasciied, original inside the parentheses, deasciied inside\n",
    "        the parentheses, original outside the parentheses, deasciied outside the\n",
    "        parentheses, keep it when we score \"\"\" \n",
    "\n",
    "    interventions_df = df_dict_new_terms[\"interventions\"]\n",
    "    interventions_df['orig_downcase_name'] = interventions_df['name'].str.lower()\n",
    "    interventions_alts_df = df_dict_new_terms[\"interventions_alts\"]\n",
    "    interventions_alts_df['alt_downcase_name'] = interventions_alts_df['name'].str.lower()\n",
    "    \n",
    "    orig_ints = interventions_df[\"orig_downcase_name\"]\n",
    "    orig_ints = list(orig_ints.unique())\n",
    "    orig_ints = list(filter(None, orig_ints))\n",
    "    alt_ints = interventions_alts_df[\"alt_downcase_name\"]\n",
    "    alt_ints = list(alt_ints.unique())\n",
    "    alt_ints = list(filter(None, alt_ints))\n",
    "\n",
    "    params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    \"\"\" Send the prepared interventions to MetaMap now. If we are on OSX, we have to use MetaMap 2018, which requires deasciied terms. If we are on Linux, we can use MetaMap 2020, which does not require such preprocessing \"\"\"\n",
    "    if metamap_version[0] < 20:\n",
    "        deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "        #  -------   original interventions  -------- #\n",
    "        orig_ints = [str(i) for i in orig_ints]\n",
    "        orig_ints = deasciier(orig_ints) # perform deascii-ing on original intervention names\n",
    "        orig_ints = list(orig_ints)\n",
    "        print(\"MetaMap version < 2020, conduct mapping on original interventions after removing ascii characters\")\n",
    "        parallelize_metamap(orig_ints, params, \"intervention\", flag_and_path, csv_writer)\n",
    "        #  ---------   alternate interventions ------- #\n",
    "        alt_ints = [str(i) for i in alt_ints]\n",
    "        # alt_ints = map(str, alt_ints) \n",
    "        alt_ints = deasciier(alt_ints) # perform deascii-ing on alternate intervention names\n",
    "        alt_ints = list(alt_ints)\n",
    "        parallelize_metamap(alt_ints, params, \"alternate_intervention\", flag_and_path, csv_writer)\n",
    "\n",
    "    else:\n",
    "\n",
    "        #  -------   original interventions  -------- #\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original interventions\")\n",
    "        parallelize_metamap(orig_ints, params, \"intervention\", flag_and_path, csv_writer)\n",
    "        #  ---------   alternate interventions ------- #\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on alternate interventions\")\n",
    "        parallelize_metamap(alt_ints, params, \"alternate_intervention\", flag_and_path, csv_writer)\n",
    "\n",
    "    interventions_all = pd.merge(interventions_df[[\"id\", \"nct_id\", \"intervention_type\", \"orig_downcase_name\", \"description\"]], interventions_alts_df[[\"nct_id\", \"intervention_id\", \"alt_downcase_name\"]], how='left', left_on=['id'], right_on = ['intervention_id'])\n",
    "    interventions_all = interventions_all.astype(str)\n",
    "    interventions_all = interventions_all.drop('nct_id_y', axis=1) # drop the redundant column now\n",
    "    interventions_all.rename(columns = {'nct_id_x':'nct_id'}, inplace = True)\n",
    "\n",
    "    interventions_all = interventions_all.sort_values(by='nct_id', ascending=False, na_position='last')\n",
    "    interventions_all = interventions_all.drop('intervention_id', axis=1) # drop the redundant column now\n",
    "    interventions_all.rename(columns = {'id':'intervention_id', 'orig_downcase_name':'orig_int', 'alt_downcase_name':'alt_int'}, inplace = True)\n",
    "\n",
    "    if metamap_version[0] >= 20:\n",
    "        matches_outside = interventions_all['orig_int'].str.extract(pattern_outside)\n",
    "        interventions_all['orig_int_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = interventions_all['orig_int'].str.extract(pattern_inside)\n",
    "        interventions_all['orig_int_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "        matches_outside = interventions_all['alt_int'].str.extract(pattern_outside)\n",
    "        interventions_all['alt_int_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = interventions_all['alt_in'].str.extract(pattern_inside)\n",
    "        interventions_all['alt_int_inside'] = matches_inside[0].fillna('')\n",
    "    else:\n",
    "\n",
    "        interventions_all['deascii_orig_int'] = deasciier(interventions_all['orig_int'])\n",
    "        interventions_all['deascii_alt_int'] = deasciier(interventions_all['alt_int'])\n",
    "\n",
    "        matches_outside = interventions_all['deascii_orig_int'].str.extract(pattern_outside)\n",
    "        interventions_all['deascii_orig_int_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = interventions_all['deascii_orig_int'].str.extract(pattern_inside)\n",
    "        interventions_all['deascii_orig_int_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "        matches_outside = interventions_all['deascii_alt_int'].str.extract(pattern_outside)\n",
    "        interventions_all['deascii_alt_int_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = interventions_all['deascii_alt_int'].str.extract(pattern_inside)\n",
    "        interventions_all['deascii_alt_name_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "    \"\"\" I don't want to perform mapping on strings < 4 char in length; these are ambiguous and it's hard to make a call what that concept should be \"\"\"\n",
    "    \"\"\" Get character counts of all the columns to evaluate \"\"\"    \n",
    "    for col in interventions_all.columns: # get the char counts of each column\n",
    "        char_count_col_name = col + '_char_count'\n",
    "        interventions_all[char_count_col_name] = interventions_all[col].str.len()\n",
    "\n",
    "    \"\"\" If char_count < 4, replace the string in the corresponding column with None so that we don't use it for comparison \"\"\"    \n",
    "    for col in interventions_all.columns[interventions_all.columns.str.contains(\"char_count\")]:\n",
    "        for index, value in interventions_all[col].items():\n",
    "            if value < 4:\n",
    "                # Find the column with the most similar name without \"char_count\" substring\n",
    "                most_similar_col = interventions_all.columns[interventions_all.columns.str.replace(\"_char_count\", \"\") == col.replace(\"_char_count\", \"\")].values[0]\n",
    "                # Update the value in the most similar column\n",
    "                interventions_all.at[index, most_similar_col] = None\n",
    "        interventions_all = interventions_all.drop(col, axis=1) # drop the count columns now  \n",
    "        \n",
    "    \"\"\" If the substring that was either outside or inside the () is identical to the term from which it came from, or actually any of the columns have the same value, put None in that cell/put None where that term is duplicated \"\"\"    \n",
    "    # Iterate through each column in the DataFrame\n",
    "    for col1 in interventions_all.columns:\n",
    "        for col2 in interventions_all.columns:\n",
    "            # Skip comparing a column with itself\n",
    "            if col1 != col2:\n",
    "                # Check if the values in col2 are duplicates of col1\n",
    "                interventions_all[col2] = interventions_all.apply(lambda row: row[col2] if row[col2] != row[col1] else None, axis=1)\n",
    "    # Drop duplicate columns (keeping the first instance)\n",
    "    interventions_all = interventions_all.T.drop_duplicates().T\n",
    "\n",
    "    interventions_all.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "\n",
    "    add_mappings_to_cache(flag_and_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891edff-e9af-411a-8b32-3b8ccf3f686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_trial(flag_and_path):\n",
    "    # send mappings to interventions and conditions, group CUIs that correspond to input condition or intervention\n",
    "    \n",
    "    print(\"\\nMapping UMLS CURIEs and names back to clinical trials\")\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "\n",
    "    # metamap_input = \"{}_metamap_output.tsv\".format(relevant_date)\n",
    "    metamap_input = \"metamapped_terms_cache.tsv\"\n",
    "    metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0, encoding=\"utf-8\")\n",
    "\n",
    "    # get the full names of the semantic types so we know what we're looking at\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\")\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "    sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names)\n",
    "    sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "    # Handle NaN (None) values in metamap_semantic_type column\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan)\n",
    "    # map semantic type abbreviations to the full name of the semantic type\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x)\n",
    "\n",
    "    metamapped['metamap_preferred_name'] = metamapped['metamap_preferred_name'].str.lower()\n",
    "    metamapped = metamapped.dropna(axis=0)\n",
    "    metamapped = metamapped[[\"term_type\", \"clin_trial_term\", \"metamap_cui\",\"metamap_preferred_name\", \"metamap_semantic_type\"]]\n",
    "\n",
    "    metamapped[\"metamap_term_info\"] = metamapped[[\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"]].values.tolist() \n",
    "    metamapped.drop([\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"], axis = 1, inplace = True)\n",
    "    metamapped = metamapped.groupby(['term_type', 'clin_trial_term'])['metamap_term_info'].agg(list).reset_index()\n",
    "\n",
    "    conditions = '{}_conditions.tsv'.format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    interventions = '{}_interventions.tsv'.format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    metamapped_con = metamapped.loc[metamapped['term_type'] == \"condition\"]\n",
    "    metamapped_int = metamapped.loc[(metamapped['term_type'] == \"intervention\") | (metamapped['term_type'] == \"alternate_intervention\")]\n",
    "\n",
    "    mapper_con = dict(zip(metamapped_con['clin_trial_term'], metamapped_con['metamap_term_info'])) # make a dict to map conditions\n",
    "    mapper_int = dict(zip(metamapped_int['clin_trial_term'], metamapped_int['metamap_term_info'])) # make a dict to map interventions\n",
    "\n",
    "#     cols_to_check = [ele for ele in conditions.columns if(ele not in ['id', 'nct_id', 'condition_id'])]\n",
    "    cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "\n",
    "    conditions[\"curie_info\"] = None\n",
    "\n",
    "    for index, row in conditions.iterrows():\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            if value in mapper_con:\n",
    "                curie_info = mapper_con[value]\n",
    "                conditions.at[index, \"curie_info\"] = curie_info    \n",
    "                \n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output conditions to TSV\n",
    "\n",
    "#     cols_to_check = [ele for ele in interventions.columns if(ele not in ['id', 'nct_id', 'intervention_id', 'intervention_type', 'description'])]\n",
    "    cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "\n",
    "    interventions[\"curie_info\"] = None\n",
    "\n",
    "    for index, row in interventions.iterrows():\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            if value in mapper_int:\n",
    "                curie_info = mapper_int[value]\n",
    "                interventions.at[index, \"curie_info\"] = curie_info\n",
    "    \n",
    "    interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0407073-41d0-48a0-ae6c-9c9c2f890b07",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8966566-9cb2-4738-9391-265083b4a8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metamap_dirs = check_os()\n",
    "flag_and_path = {'term_program_flag': False,\n",
    "                 'data_extracted_path': '/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/12_11_2023_extracted',\n",
    "                 'date_string':'12_11_2023'} # comment for production\n",
    "\n",
    "data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "# read in pipe-delimited files \n",
    "conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0)\n",
    "\n",
    "df_dict = {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ab0b1d3-5d68-490a-b8bf-7f1b6931e30d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nct_id</th>\n",
       "      <th>intervention_type</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69214956</td>\n",
       "      <td>NCT01324180</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Intrathecal chemotherapy</td>\n",
       "      <td>IT cytarabine given intrathecally to all patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69214957</td>\n",
       "      <td>NCT02560649</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Peginterferon alfa-2a plus Entecavir</td>\n",
       "      <td>Peginterferon alfa-2a 180μg /wk plus Entecavir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69362736</td>\n",
       "      <td>NCT03670563</td>\n",
       "      <td>Other</td>\n",
       "      <td>Short foot exercise protocol</td>\n",
       "      <td>Exercises that target intrinsic foot muscles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69362737</td>\n",
       "      <td>NCT03670563</td>\n",
       "      <td>Other</td>\n",
       "      <td>Short foot exercise plus NMES</td>\n",
       "      <td>Exercises that target intrinsic foot muscles s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69362738</td>\n",
       "      <td>NCT02567175</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Domperidone</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807170</th>\n",
       "      <td>69524733</td>\n",
       "      <td>NCT01931475</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Duloxetine</td>\n",
       "      <td>Administered orally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807171</th>\n",
       "      <td>69524734</td>\n",
       "      <td>NCT01931475</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>Administered orally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807172</th>\n",
       "      <td>69524735</td>\n",
       "      <td>NCT04622787</td>\n",
       "      <td>Other</td>\n",
       "      <td>Knowledge translation</td>\n",
       "      <td>A. Healthcare Providers Observational Study - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807173</th>\n",
       "      <td>69524736</td>\n",
       "      <td>NCT01929993</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>SWETZ</td>\n",
       "      <td>Straight wire excision of transformation zone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807174</th>\n",
       "      <td>69524737</td>\n",
       "      <td>NCT01929993</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>LLETZ cone</td>\n",
       "      <td>LLETZ cone is a electrosurgical conization met...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>807175 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       nct_id intervention_type  \\\n",
       "0       69214956  NCT01324180              Drug   \n",
       "1       69214957  NCT02560649              Drug   \n",
       "2       69362736  NCT03670563             Other   \n",
       "3       69362737  NCT03670563             Other   \n",
       "4       69362738  NCT02567175              Drug   \n",
       "...          ...          ...               ...   \n",
       "807170  69524733  NCT01931475              Drug   \n",
       "807171  69524734  NCT01931475              Drug   \n",
       "807172  69524735  NCT04622787             Other   \n",
       "807173  69524736  NCT01929993         Procedure   \n",
       "807174  69524737  NCT01929993         Procedure   \n",
       "\n",
       "                                        name  \\\n",
       "0                   Intrathecal chemotherapy   \n",
       "1       Peginterferon alfa-2a plus Entecavir   \n",
       "2               Short foot exercise protocol   \n",
       "3              Short foot exercise plus NMES   \n",
       "4                                Domperidone   \n",
       "...                                      ...   \n",
       "807170                            Duloxetine   \n",
       "807171                               Placebo   \n",
       "807172                 Knowledge translation   \n",
       "807173                                 SWETZ   \n",
       "807174                            LLETZ cone   \n",
       "\n",
       "                                              description  \n",
       "0       IT cytarabine given intrathecally to all patie...  \n",
       "1       Peginterferon alfa-2a 180μg /wk plus Entecavir...  \n",
       "2            Exercises that target intrinsic foot muscles  \n",
       "3       Exercises that target intrinsic foot muscles s...  \n",
       "4                                                     NaN  \n",
       "...                                                   ...  \n",
       "807170                                Administered orally  \n",
       "807171                                Administered orally  \n",
       "807172  A. Healthcare Providers Observational Study - ...  \n",
       "807173  Straight wire excision of transformation zone ...  \n",
       "807174  LLETZ cone is a electrosurgical conization met...  \n",
       "\n",
       "[807175 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interventions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "423df503-ac00-4485-b9fb-fc189c5b4672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping UMLS CURIEs and names back to clinical trials\n"
     ]
    }
   ],
   "source": [
    "# send mappings to interventions and conditions, group CUIs that correspond to input condition or intervention\n",
    "\n",
    "print(\"\\nMapping UMLS CURIEs and names back to clinical trials\")\n",
    "relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "\n",
    "# metamap_input = \"{}_metamap_output.tsv\".format(relevant_date)\n",
    "metamap_input = \"metamapped_terms_cache.tsv\"\n",
    "metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0, encoding=\"utf-8\")\n",
    "metamapped = metamapped.drop_duplicates()\n",
    "\n",
    "# get the full names of the semantic types so we know what we're looking at\n",
    "metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\")\n",
    "metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names)\n",
    "sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "# Handle NaN (None) values in metamap_semantic_type column\n",
    "metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan)\n",
    "# map semantic type abbreviations to the full name of the semantic type\n",
    "metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x)\n",
    "metamapped['metamap_preferred_name'] = metamapped['metamap_preferred_name'].str.lower()\n",
    "metamapped = metamapped.dropna(axis=0)\n",
    "metamapped = metamapped[[\"term_type\", \"clin_trial_term\", \"metamap_cui\",\"metamap_preferred_name\", \"metamap_semantic_type\"]]\n",
    "\n",
    "metamapped[\"metamap_term_info\"] = metamapped[[\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"]].values.tolist() # aggregate/collect metamapped CURIE info into list\n",
    "metamapped.drop([\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"], axis = 1, inplace = True)\n",
    "metamapped = metamapped.groupby(['term_type', 'clin_trial_term'])['metamap_term_info'].agg(list).reset_index() # get 1 term per list of CURIEs and associated info\n",
    "\n",
    "metamapped_con = metamapped.loc[metamapped['term_type'] == \"condition\"]\n",
    "metamapped_int = metamapped.loc[(metamapped['term_type'] == \"intervention\") | (metamapped['term_type'] == \"alternate_intervention\")]\n",
    "mapper_con = dict(zip(metamapped_con['clin_trial_term'], metamapped_con['metamap_term_info'])) # make a dict to map conditions\n",
    "mapper_int = dict(zip(metamapped_int['clin_trial_term'], metamapped_int['metamap_term_info'])) # make a dict to map interventions\n",
    "\n",
    "    # -------    CONDITIONS    ------- #\n",
    "\n",
    "conditions = df_dict[\"conditions\"]\n",
    "conditions.rename(columns = {'downcase_name':'orig_con', 'id': 'condition_id'}, inplace = True)\n",
    "pattern_outside = r'(?<=\\().+?(?=\\))|([^(]+)'\n",
    "pattern_inside = r'\\(([^)]+)\\)'\n",
    "matches_outside = conditions['orig_con'].str.extract(pattern_outside)\n",
    "conditions['orig_con_outside'] = matches_outside[0].fillna('')\n",
    "matches_inside = conditions['orig_con'].str.extract(pattern_inside)\n",
    "conditions['orig_con_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "\n",
    "conditions[\"curie_info\"] = None\n",
    "\n",
    "for index, row in conditions.iterrows():\n",
    "    for col_name in cols_to_check:\n",
    "        value = row[col_name]\n",
    "        if value in mapper_con:\n",
    "            curie_info = mapper_con[value]\n",
    "            conditions.at[index, \"curie_info\"] = curie_info    \n",
    "\n",
    "# conditions_actually_mapped = conditions[~conditions['curie_info'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output conditions to TSV\n",
    "\n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "\n",
    "interventions = df_dict[\"interventions\"]\n",
    "interventions['downcase_name'] = interventions['name'].str.lower()\n",
    "\n",
    "interventions.rename(columns = {'downcase_name':'orig_int', 'id': 'intervention_id'}, inplace = True)\n",
    "interventions\n",
    "pattern_outside = r'(?<=\\().+?(?=\\))|([^(]+)'\n",
    "pattern_inside = r'\\(([^)]+)\\)'\n",
    "matches_outside = interventions['orig_int'].str.extract(pattern_outside)\n",
    "interventions['orig_int_outside'] = matches_outside[0].fillna('')\n",
    "matches_inside = interventions['orig_int'].str.extract(pattern_inside)\n",
    "interventions['orig_int_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "\n",
    "interventions[\"curie_info\"] = None\n",
    "\n",
    "for index, row in interventions.iterrows():\n",
    "    for col_name in cols_to_check:\n",
    "        value = row[col_name]\n",
    "        if value in mapper_int:\n",
    "            curie_info = mapper_int[value]\n",
    "            interventions.at[index, \"curie_info\"] = curie_info    \n",
    "            \n",
    "interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c1a6a38-6f5c-463c-9c0b-29bf72639d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intervention_id</th>\n",
       "      <th>nct_id</th>\n",
       "      <th>intervention_type</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>orig_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69214956</td>\n",
       "      <td>NCT01324180</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Intrathecal chemotherapy</td>\n",
       "      <td>IT cytarabine given intrathecally to all patie...</td>\n",
       "      <td>intrathecal chemotherapy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69214957</td>\n",
       "      <td>NCT02560649</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Peginterferon alfa-2a plus Entecavir</td>\n",
       "      <td>Peginterferon alfa-2a 180μg /wk plus Entecavir...</td>\n",
       "      <td>peginterferon alfa-2a plus entecavir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69362736</td>\n",
       "      <td>NCT03670563</td>\n",
       "      <td>Other</td>\n",
       "      <td>Short foot exercise protocol</td>\n",
       "      <td>Exercises that target intrinsic foot muscles</td>\n",
       "      <td>short foot exercise protocol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69362737</td>\n",
       "      <td>NCT03670563</td>\n",
       "      <td>Other</td>\n",
       "      <td>Short foot exercise plus NMES</td>\n",
       "      <td>Exercises that target intrinsic foot muscles s...</td>\n",
       "      <td>short foot exercise plus nmes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69362738</td>\n",
       "      <td>NCT02567175</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Domperidone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>domperidone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807170</th>\n",
       "      <td>69524733</td>\n",
       "      <td>NCT01931475</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Duloxetine</td>\n",
       "      <td>Administered orally</td>\n",
       "      <td>duloxetine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807171</th>\n",
       "      <td>69524734</td>\n",
       "      <td>NCT01931475</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>Administered orally</td>\n",
       "      <td>placebo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807172</th>\n",
       "      <td>69524735</td>\n",
       "      <td>NCT04622787</td>\n",
       "      <td>Other</td>\n",
       "      <td>Knowledge translation</td>\n",
       "      <td>A. Healthcare Providers Observational Study - ...</td>\n",
       "      <td>knowledge translation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807173</th>\n",
       "      <td>69524736</td>\n",
       "      <td>NCT01929993</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>SWETZ</td>\n",
       "      <td>Straight wire excision of transformation zone ...</td>\n",
       "      <td>swetz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807174</th>\n",
       "      <td>69524737</td>\n",
       "      <td>NCT01929993</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>LLETZ cone</td>\n",
       "      <td>LLETZ cone is a electrosurgical conization met...</td>\n",
       "      <td>lletz cone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>807175 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        intervention_id       nct_id intervention_type  \\\n",
       "0              69214956  NCT01324180              Drug   \n",
       "1              69214957  NCT02560649              Drug   \n",
       "2              69362736  NCT03670563             Other   \n",
       "3              69362737  NCT03670563             Other   \n",
       "4              69362738  NCT02567175              Drug   \n",
       "...                 ...          ...               ...   \n",
       "807170         69524733  NCT01931475              Drug   \n",
       "807171         69524734  NCT01931475              Drug   \n",
       "807172         69524735  NCT04622787             Other   \n",
       "807173         69524736  NCT01929993         Procedure   \n",
       "807174         69524737  NCT01929993         Procedure   \n",
       "\n",
       "                                        name  \\\n",
       "0                   Intrathecal chemotherapy   \n",
       "1       Peginterferon alfa-2a plus Entecavir   \n",
       "2               Short foot exercise protocol   \n",
       "3              Short foot exercise plus NMES   \n",
       "4                                Domperidone   \n",
       "...                                      ...   \n",
       "807170                            Duloxetine   \n",
       "807171                               Placebo   \n",
       "807172                 Knowledge translation   \n",
       "807173                                 SWETZ   \n",
       "807174                            LLETZ cone   \n",
       "\n",
       "                                              description  \\\n",
       "0       IT cytarabine given intrathecally to all patie...   \n",
       "1       Peginterferon alfa-2a 180μg /wk plus Entecavir...   \n",
       "2            Exercises that target intrinsic foot muscles   \n",
       "3       Exercises that target intrinsic foot muscles s...   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "807170                                Administered orally   \n",
       "807171                                Administered orally   \n",
       "807172  A. Healthcare Providers Observational Study - ...   \n",
       "807173  Straight wire excision of transformation zone ...   \n",
       "807174  LLETZ cone is a electrosurgical conization met...   \n",
       "\n",
       "                                    orig_int  \n",
       "0                   intrathecal chemotherapy  \n",
       "1       peginterferon alfa-2a plus entecavir  \n",
       "2               short foot exercise protocol  \n",
       "3              short foot exercise plus nmes  \n",
       "4                                domperidone  \n",
       "...                                      ...  \n",
       "807170                            duloxetine  \n",
       "807171                               placebo  \n",
       "807172                 knowledge translation  \n",
       "807173                                 swetz  \n",
       "807174                            lletz cone  \n",
       "\n",
       "[807175 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interventions = df_dict[\"interventions\"]\n",
    "interventions['downcase_name'] = interventions['name'].str.lower()\n",
    "\n",
    "interventions.rename(columns = {'downcase_name':'orig_int', 'id': 'intervention_id'}, inplace = True)\n",
    "interventions\n",
    "pattern_outside = r'(?<=\\().+?(?=\\))|([^(]+)'\n",
    "pattern_inside = r'\\(([^)]+)\\)'\n",
    "matches_outside = interventions['orig_int'].str.extract(pattern_outside)\n",
    "interventions['orig_int_outside'] = matches_outside[0].fillna('')\n",
    "matches_inside = interventions['orig_int'].str.extract(pattern_inside)\n",
    "interventions['orig_int_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "# # metamapped_con = metamapped.loc[metamapped['term_type'] == \"condition\"]\n",
    "# # metamapped_int = metamapped.loc[(metamapped['term_type'] == \"intervention\") | (metamapped['term_type'] == \"alternate_intervention\")]\n",
    "# # mapper_con = dict(zip(metamapped_con['clin_trial_term'], metamapped_con['metamap_term_info'])) # make a dict to map interventions\n",
    "# # mapper_int = dict(zip(metamapped_int['clin_trial_term'], metamapped_int['metamap_term_info'])) # make a dict to map interventions\n",
    "\n",
    "cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "\n",
    "interventions[\"curie_info\"] = None\n",
    "\n",
    "for index, row in interventions.iterrows():\n",
    "    for col_name in cols_to_check:\n",
    "        value = row[col_name]\n",
    "        if value in mapper_int:\n",
    "            curie_info = mapper_int[value]\n",
    "            interventions.at[index, \"curie_info\"] = curie_info    \n",
    "\n",
    "# interventions_actually_mapped = interventions[~interventions['curie_info'].isnull()] # check if the interventions got mapped to any CURIEs\n",
    "# # # interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a025da4-1582-45bd-ba45-c1b5e45e968f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intervention_id</th>\n",
       "      <th>nct_id</th>\n",
       "      <th>intervention_type</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>orig_int</th>\n",
       "      <th>orig_int_outside</th>\n",
       "      <th>orig_int_inside</th>\n",
       "      <th>curie_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>69524746</td>\n",
       "      <td>NCT01646177</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>Administered SC</td>\n",
       "      <td>placebo</td>\n",
       "      <td>placebo</td>\n",
       "      <td></td>\n",
       "      <td>[[C0032042, placebos, Therapeutic or Preventiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>69524747</td>\n",
       "      <td>NCT01646177</td>\n",
       "      <td>Drug</td>\n",
       "      <td>50 mg etanercept</td>\n",
       "      <td>Administered SC</td>\n",
       "      <td>50 mg etanercept</td>\n",
       "      <td>50 mg etanercept</td>\n",
       "      <td></td>\n",
       "      <td>[[C0717758, etanercept, Amino Acid, Peptide, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>69039445</td>\n",
       "      <td>NCT00510614</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>one pill twice weekly for 12 weeks</td>\n",
       "      <td>placebo</td>\n",
       "      <td>placebo</td>\n",
       "      <td></td>\n",
       "      <td>[[C0032042, placebos, Therapeutic or Preventiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>69686256</td>\n",
       "      <td>NCT01450566</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Lidocaine</td>\n",
       "      <td>Use of lidocaine</td>\n",
       "      <td>lidocaine</td>\n",
       "      <td>lidocaine</td>\n",
       "      <td></td>\n",
       "      <td>[[C0023660, lidocaine, Organic Chemical|Pharma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>69686257</td>\n",
       "      <td>NCT01450566</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Saline</td>\n",
       "      <td>saline</td>\n",
       "      <td>saline</td>\n",
       "      <td></td>\n",
       "      <td>[[C0036082, saline solution, Substance]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807155</th>\n",
       "      <td>69524718</td>\n",
       "      <td>NCT04771351</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>Diluent solution</td>\n",
       "      <td>placebo</td>\n",
       "      <td>placebo</td>\n",
       "      <td></td>\n",
       "      <td>[[C0032042, placebos, Therapeutic or Preventiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807157</th>\n",
       "      <td>69524720</td>\n",
       "      <td>NCT05107687</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>Smoking cessation</td>\n",
       "      <td>Assisted Smoking Cessation- Subject is provide...</td>\n",
       "      <td>smoking cessation</td>\n",
       "      <td>smoking cessation</td>\n",
       "      <td></td>\n",
       "      <td>[[C0085134, cessation of smoking, Individual B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807160</th>\n",
       "      <td>69524723</td>\n",
       "      <td>NCT04773067</td>\n",
       "      <td>Biological</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>Around 550 adult subjects and 55 adolescent su...</td>\n",
       "      <td>placebo</td>\n",
       "      <td>placebo</td>\n",
       "      <td></td>\n",
       "      <td>[[C0032042, placebos, Therapeutic or Preventiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807167</th>\n",
       "      <td>69524730</td>\n",
       "      <td>NCT02243371</td>\n",
       "      <td>Drug</td>\n",
       "      <td>nivolumab</td>\n",
       "      <td>3 mg/kg administered IV on Day 1 of Cycles 1-6</td>\n",
       "      <td>nivolumab</td>\n",
       "      <td>nivolumab</td>\n",
       "      <td></td>\n",
       "      <td>[[C3657270, nivolumab, Amino Acid, Peptide, or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807171</th>\n",
       "      <td>69524734</td>\n",
       "      <td>NCT01931475</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>Administered orally</td>\n",
       "      <td>placebo</td>\n",
       "      <td>placebo</td>\n",
       "      <td></td>\n",
       "      <td>[[C0032042, placebos, Therapeutic or Preventiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126414 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        intervention_id       nct_id intervention_type               name  \\\n",
       "9              69524746  NCT01646177              Drug            Placebo   \n",
       "10             69524747  NCT01646177              Drug   50 mg etanercept   \n",
       "28             69039445  NCT00510614              Drug            Placebo   \n",
       "45             69686256  NCT01450566              Drug          Lidocaine   \n",
       "46             69686257  NCT01450566              Drug             Saline   \n",
       "...                 ...          ...               ...                ...   \n",
       "807155         69524718  NCT04771351              Drug            Placebo   \n",
       "807157         69524720  NCT05107687        Behavioral  Smoking cessation   \n",
       "807160         69524723  NCT04773067        Biological            Placebo   \n",
       "807167         69524730  NCT02243371              Drug          nivolumab   \n",
       "807171         69524734  NCT01931475              Drug            Placebo   \n",
       "\n",
       "                                              description           orig_int  \\\n",
       "9                                         Administered SC            placebo   \n",
       "10                                        Administered SC   50 mg etanercept   \n",
       "28                     one pill twice weekly for 12 weeks            placebo   \n",
       "45                                       Use of lidocaine          lidocaine   \n",
       "46                                                 Saline             saline   \n",
       "...                                                   ...                ...   \n",
       "807155                                   Diluent solution            placebo   \n",
       "807157  Assisted Smoking Cessation- Subject is provide...  smoking cessation   \n",
       "807160  Around 550 adult subjects and 55 adolescent su...            placebo   \n",
       "807167     3 mg/kg administered IV on Day 1 of Cycles 1-6          nivolumab   \n",
       "807171                                Administered orally            placebo   \n",
       "\n",
       "         orig_int_outside orig_int_inside  \\\n",
       "9                 placebo                   \n",
       "10       50 mg etanercept                   \n",
       "28                placebo                   \n",
       "45              lidocaine                   \n",
       "46                 saline                   \n",
       "...                   ...             ...   \n",
       "807155            placebo                   \n",
       "807157  smoking cessation                   \n",
       "807160            placebo                   \n",
       "807167          nivolumab                   \n",
       "807171            placebo                   \n",
       "\n",
       "                                               curie_info  \n",
       "9       [[C0032042, placebos, Therapeutic or Preventiv...  \n",
       "10      [[C0717758, etanercept, Amino Acid, Peptide, o...  \n",
       "28      [[C0032042, placebos, Therapeutic or Preventiv...  \n",
       "45      [[C0023660, lidocaine, Organic Chemical|Pharma...  \n",
       "46               [[C0036082, saline solution, Substance]]  \n",
       "...                                                   ...  \n",
       "807155  [[C0032042, placebos, Therapeutic or Preventiv...  \n",
       "807157  [[C0085134, cessation of smoking, Individual B...  \n",
       "807160  [[C0032042, placebos, Therapeutic or Preventiv...  \n",
       "807167  [[C3657270, nivolumab, Amino Acid, Peptide, or...  \n",
       "807171  [[C0032042, placebos, Therapeutic or Preventiv...  \n",
       "\n",
       "[126414 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interventions_actually_mapped = interventions[~interventions['curie_info'].isnull()] # check if the interventions got mapped to any CURIEs\n",
    "# # # interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "interventions_actually_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a279a538-0737-4adb-b671-a4d36bcaac3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(df_dict[\"conditions\"]) == id(conditions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fcc6c86-82a4-4237-ba72-c896d8d76efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_to_trial(flag_and_path):\n",
    "    # send mappings to interventions and conditions, group CUIs that correspond to input condition or intervention\n",
    "    \n",
    "    print(\"\\nMapping UMLS CURIEs and names back to clinical trials\")\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "\n",
    "    # metamap_input = \"{}_metamap_output.tsv\".format(relevant_date)\n",
    "    metamap_input = \"metamapped_terms_cache.tsv\"\n",
    "    metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0, encoding=\"utf-8\")\n",
    "\n",
    "    # get the full names of the semantic types so we know what we're looking at\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\")\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "    sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names)\n",
    "    sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "    # Handle NaN (None) values in metamap_semantic_type column\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan)\n",
    "    # map semantic type abbreviations to the full name of the semantic type\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x)\n",
    "\n",
    "    metamapped['metamap_preferred_name'] = metamapped['metamap_preferred_name'].str.lower()\n",
    "    metamapped = metamapped.dropna(axis=0)\n",
    "    metamapped = metamapped[[\"term_type\", \"clin_trial_term\", \"metamap_cui\",\"metamap_preferred_name\", \"metamap_semantic_type\"]]\n",
    "\n",
    "    metamapped[\"metamap_term_info\"] = metamapped[[\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"]].values.tolist() \n",
    "    metamapped.drop([\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"], axis = 1, inplace = True)\n",
    "    metamapped = metamapped.groupby(['term_type', 'clin_trial_term'])['metamap_term_info'].agg(list).reset_index()\n",
    "\n",
    "    conditions = '{}_conditions.tsv'.format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    interventions = '{}_interventions.tsv'.format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    metamapped_con = metamapped.loc[metamapped['term_type'] == \"condition\"]\n",
    "    metamapped_int = metamapped.loc[(metamapped['term_type'] == \"intervention\") | (metamapped['term_type'] == \"alternate_intervention\")]\n",
    "\n",
    "    mapper_con = dict(zip(metamapped_con['clin_trial_term'], metamapped_con['metamap_term_info'])) # make a dict to map conditions\n",
    "    mapper_int = dict(zip(metamapped_int['clin_trial_term'], metamapped_int['metamap_term_info'])) # make a dict to map interventions\n",
    "\n",
    "#     cols_to_check = [ele for ele in conditions.columns if(ele not in ['id', 'nct_id', 'condition_id'])]\n",
    "    cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "\n",
    "    conditions[\"curie_info\"] = None\n",
    "\n",
    "    for index, row in conditions.iterrows():\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            if value in mapper_con:\n",
    "                curie_info = mapper_con[value]\n",
    "                conditions.at[index, \"curie_info\"] = curie_info    \n",
    "                \n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output conditions to TSV\n",
    "\n",
    "#     cols_to_check = [ele for ele in interventions.columns if(ele not in ['id', 'nct_id', 'intervention_id', 'intervention_type', 'description'])]\n",
    "    cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "\n",
    "    interventions[\"curie_info\"] = None\n",
    "\n",
    "    for index, row in interventions.iterrows():\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            if value in mapper_int:\n",
    "                curie_info = mapper_int[value]\n",
    "                interventions.at[index, \"curie_info\"] = curie_info\n",
    "    \n",
    "    interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ee0faf7-5940-4302-9539-e308db5c23fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_mappings(flag_and_path):\n",
    "    \n",
    "    print(\"Scoring mappings\")\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    \n",
    "    #   -- --- --   CONDITIONS   -- --- -- #\n",
    "    conditions = \"{}_conditions.tsv\".format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "    conditions = conditions.where(pd.notnull(conditions), None)\n",
    "\n",
    "    for index, row in conditions.iterrows():\n",
    "        curies_sublists_scored = []\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            curie_info = row[\"curie_info\"]\n",
    "            if None not in [value, curie_info]:\n",
    "                curie_sublists = ast.literal_eval(curie_info)\n",
    "                for sublist in curie_sublists:\n",
    "                    sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "                    sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "                    curies_sublists_scored.append(sublist)\n",
    "        conditions.at[index, \"curie_info\"] = curies_sublists_scored\n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    #   -- --- --   INTERVENTIONS   -- --- -- #\n",
    "    \n",
    "    interventions = \"{}_interventions.tsv\".format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "    cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "    interventions = interventions.where(pd.notnull(interventions), None)\n",
    "\n",
    "    # for index, row in interventions.iterrows():\n",
    "    #     curies_sublists_scored = []\n",
    "    #     for col_name in cols_to_check:\n",
    "    #         value = row[col_name]\n",
    "    #         curie_info = row[\"curie_info\"]\n",
    "    #         if None not in [value, curie_info]:\n",
    "    #             curie_sublists = ast.literal_eval(curie_info)\n",
    "    #             for sublist in curie_sublists:\n",
    "    #                 sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "    #                 sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "    #                 curies_sublists_scored.append(sublist)\n",
    "    #         interventions.at[index, \"curie_info\"] = curies_sublists_scored\n",
    "\n",
    "    \n",
    "    for index, row in interventions.iterrows():\n",
    "        curies_sublists_scored = []\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            curie_info = row[\"curie_info\"]\n",
    "            try:\n",
    "                if None not in [value, curie_info]:\n",
    "                    curie_sublists = ast.literal_eval(curie_info)\n",
    "                    for sublist in curie_sublists:\n",
    "                        sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "                        sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "                        curies_sublists_scored.append(sublist)\n",
    "            except:\n",
    "                print(value)\n",
    "                print(curie_info)\n",
    "\n",
    "    interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6029dc5-da3e-4a40-81c0-be16b436ef87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TESTING END END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e42025af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_trial(flag_and_path):\n",
    "    # send mappings to interventions and conditions, group CUIs that correspond to input condition or intervention\n",
    "    \n",
    "    print(\"\\nMapping UMLS CURIEs and names back to clinical trials\")\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "\n",
    "    metamap_input = \"{}_metamap_output.tsv\".format(relevant_date)\n",
    "    metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0, encoding=\"utf-8\")\n",
    "\n",
    "    # get the full names of the semantic types so we know what we're looking at\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\")\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "    sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names)\n",
    "    sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "    # Handle NaN (None) values in metamap_semantic_type column\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan)\n",
    "    # map semantic type abbreviations to the full name of the semantic type\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x)\n",
    "\n",
    "    metamapped['metamap_preferred_name'] = metamapped['metamap_preferred_name'].str.lower()\n",
    "    metamapped = metamapped.dropna(axis=0)\n",
    "    metamapped = metamapped[[\"term_type\", \"clin_trial_term\", \"metamap_cui\",\"metamap_preferred_name\", \"metamap_semantic_type\"]]\n",
    "\n",
    "    metamapped[\"metamap_term_info\"] = metamapped[[\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"]].values.tolist() \n",
    "    metamapped.drop([\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"], axis = 1, inplace = True)\n",
    "    metamapped = metamapped.groupby(['term_type', 'clin_trial_term'])['metamap_term_info'].agg(list).reset_index()\n",
    "\n",
    "    conditions = '{}_conditions.tsv'.format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    interventions = '{}_interventions.tsv'.format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    metamapped_con = metamapped.loc[metamapped['term_type'] == \"condition\"]\n",
    "    metamapped_int = metamapped.loc[(metamapped['term_type'] == \"intervention\") | (metamapped['term_type'] == \"alternate_intervention\")]\n",
    "\n",
    "    mapper_con = dict(zip(metamapped_con['clin_trial_term'], metamapped_con['metamap_term_info'])) # make a dict to map conditions\n",
    "    mapper_int = dict(zip(metamapped_int['clin_trial_term'], metamapped_int['metamap_term_info'])) # make a dict to map interventions\n",
    "\n",
    "#     cols_to_check = [ele for ele in conditions.columns if(ele not in ['id', 'nct_id', 'condition_id'])]\n",
    "    cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "\n",
    "    conditions[\"curie_info\"] = None\n",
    "\n",
    "    for index, row in conditions.iterrows():\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            if value in mapper_con:\n",
    "                curie_info = mapper_con[value]\n",
    "                conditions.at[index, \"curie_info\"] = curie_info    \n",
    "                \n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output conditions to TSV\n",
    "\n",
    "#     cols_to_check = [ele for ele in interventions.columns if(ele not in ['id', 'nct_id', 'intervention_id', 'intervention_type', 'description'])]\n",
    "    cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "\n",
    "    interventions[\"curie_info\"] = None\n",
    "\n",
    "    for index, row in interventions.iterrows():\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            if value in mapper_int:\n",
    "                curie_info = mapper_int[value]\n",
    "                interventions.at[index, \"curie_info\"] = curie_info\n",
    "    \n",
    "    interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcfdd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mappings(flag_and_path):\n",
    "    \n",
    "    print(\"Scoring mappings\")\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    \n",
    "    #   -- --- --   CONDITIONS   -- --- -- #\n",
    "    conditions = \"{}_conditions.tsv\".format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "    conditions = conditions.where(pd.notnull(conditions), None)\n",
    "\n",
    "    for index, row in conditions.iterrows():\n",
    "        curies_sublists_scored = []\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            curie_info = row[\"curie_info\"]\n",
    "            if None not in [value, curie_info]:\n",
    "                curie_sublists = ast.literal_eval(curie_info)\n",
    "                for sublist in curie_sublists:\n",
    "                    sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "                    sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "                    curies_sublists_scored.append(sublist)\n",
    "        conditions.at[index, \"curie_info\"] = curies_sublists_scored\n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    #   -- --- --   INTERVENTIONS   -- --- -- #\n",
    "    \n",
    "    interventions = \"{}_interventions.tsv\".format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "    cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "    interventions = interventions.where(pd.notnull(interventions), None)\n",
    "\n",
    "    for index, row in interventions.iterrows():\n",
    "        curies_sublists_scored = []\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            curie_info = row[\"curie_info\"]\n",
    "            if None not in [value, curie_info]:\n",
    "                curie_sublists = ast.literal_eval(curie_info)\n",
    "                for sublist in curie_sublists:\n",
    "                    sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "                    sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "                    curies_sublists_scored.append(sublist)\n",
    "\n",
    "        interventions.at[index, \"curie_info\"] = curies_sublists_scored\n",
    "    interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "561880fd-8d95-4f53-8214-561603ffb92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring mappings\n",
      "0    atopic dermatitis eczema\n",
      "1                         NaN\n",
      "dtype: object\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0    neonates analgesy\n",
      "1                  NaN\n",
      "dtype: object\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0    indolent plasma cell myeloma\n",
      "1                             NaN\n",
      "dtype: object\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0    drug dependence\n",
      "1                NaN\n",
      "dtype: object\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0    hypoxemia\n",
      "1          NaN\n",
      "dtype: object\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n",
      "0   NaN\n",
      "1   NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Scoring mappings\")\n",
    "relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "\n",
    "#   -- --- --   CONDITIONS   -- --- -- #\n",
    "conditions = \"{}_conditions.tsv\".format(relevant_date)\n",
    "conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "conditions = conditions.where(pd.notnull(conditions), None)\n",
    "\n",
    "for index, row in conditions[:5].iterrows():\n",
    "    curies_sublists_scored = []\n",
    "\n",
    "    for col_name in cols_to_check:\n",
    "        value = row[col_name]\n",
    "        curie_info = row[\"curie_info\"]\n",
    "        # if not pd.isna(curie_info) and not pd.isna(value):\n",
    "        temp_series = pd.Series([value, curie_info])\n",
    "        print(temp_series)\n",
    "        if not temp_series.all():\n",
    "            print(\"true\")\n",
    "            \n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "# for index, row in conditions.iterrows():\n",
    "#     curies_sublists_scored = []\n",
    "#     for col_name in cols_to_check:\n",
    "#         value = row[col_name]\n",
    "#         curie_info = row[\"curie_info\"]\n",
    "#         # if None not in [value, curie_info]:\n",
    "#             print(curie_info)\n",
    "#             # if name == '' or pd.isnull(name)\n",
    "#             # curie_sublists = ast.literal_eval(curie_info)\n",
    "#             # for sublist in curie_sublists:\n",
    "#             for sublist in curie_info:\n",
    "#                 sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "#                 sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "#                 curies_sublists_scored.append(sublist)\n",
    "#     conditions.at[index, \"curie_info\"] = curies_sublists_scored\n",
    "# conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "# #   -- --- --   INTERVENTIONS   -- --- -- #\n",
    "\n",
    "# interventions = \"{}_interventions.tsv\".format(relevant_date)\n",
    "# interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "# cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "# interventions = interventions.where(pd.notnull(interventions), None)\n",
    "\n",
    "# # for index, row in interventions.iterrows():\n",
    "# #     curies_sublists_scored = []\n",
    "# #     for col_name in cols_to_check:\n",
    "# #         value = row[col_name]\n",
    "# #         curie_info = row[\"curie_info\"]\n",
    "# #         if None not in [value, curie_info]:\n",
    "# #             curie_sublists = ast.literal_eval(curie_info)\n",
    "# #             for sublist in curie_sublists:\n",
    "# #                 sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "# #                 sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "# #                 curies_sublists_scored.append(sublist)\n",
    "\n",
    "# for index, row in interventions.iterrows():\n",
    "#     curies_sublists_scored = []\n",
    "#     for col_name in cols_to_check:\n",
    "#         value = row[col_name]\n",
    "#         curie_info = row[\"curie_info\"]\n",
    "#         try:\n",
    "#             if None not in [value, curie_info]:\n",
    "#                 # curie_sublists = ast.literal_eval(curie_info)\n",
    "#                 print(curie_info)\n",
    "#                 # for sublist in curie_sublists:\n",
    "#                 for sublist in curie_info:\n",
    "#                     sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "#                     sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "#                     curies_sublists_scored.append(sublist)\n",
    "#         except:\n",
    "#             print(value)\n",
    "#             print(curie_info)\n",
    "\n",
    "#     interventions.at[index, \"curie_info\"] = curies_sublists_scored\n",
    "# interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51bb4e-42e2-4e31-8ba6-8e8fe42aec0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e75aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_select_curies(flag_and_path):\n",
    "    print(\"Auto-selecting high scoring CURIEs\")\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    \n",
    "    def filter_and_select_sublist(sublists):  # function to find the highest score of a CURIE, and pick that curie if it's greater than threshold of 88\n",
    "        if sublists is None or len(sublists) == 0:\n",
    "            return None\n",
    "\n",
    "        high_score = -1\n",
    "        selected_sublist = None\n",
    "\n",
    "        sublists = ast.literal_eval(sublists)\n",
    "        for sublist in sublists:\n",
    "\n",
    "            if len(sublist) >= 4:\n",
    "                sort_ratio = int(sublist[3].split(\": \")[1])\n",
    "                sim_score = int(sublist[4].split(\": \")[1])\n",
    "                max_score = max(sort_ratio, sim_score)\n",
    "                if max_score > 88: \n",
    "                    if max_score > high_score:\n",
    "                        high_score = max_score\n",
    "                        selected_sublist = sublist\n",
    "        return selected_sublist\n",
    "\n",
    "    #   -----   -----    -----   -----   CONDITIONS   -----   -----    -----   -----  #\n",
    "\n",
    "    conditions = \"{}_conditions.tsv\".format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    \"\"\"  Create an output TSV of CURIEs that are auto-selected based on passing the threshold of scoring > 88  \"\"\"\n",
    "    conditions['auto_selected_curie'] = conditions['curie_info'].apply(filter_and_select_sublist)  # select CURIE that scores highest using filter_and_select_sublist function = auto-select\n",
    "    auto_selected_conditions = conditions[conditions[['auto_selected_curie']].notnull().all(1)]   # get the rows where a CURIE has been auto-selected\n",
    "    auto_selected_conditions = auto_selected_conditions[[\"condition_id\", \"nct_id\", \"orig_con\", \"curie_info\", \"auto_selected_curie\"]]  # subset dataframe\n",
    "    auto_selected_conditions.to_csv('{}_conditions_auto_selected.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    conditions_manual_review = conditions[conditions[\"auto_selected_curie\"].isna()]   # select rows where no CURIE was auto-selected\n",
    "    conditions_manual_review = conditions_manual_review[[\"orig_con\", \"curie_info\"]]  # subset\n",
    "\n",
    "    \"\"\"  Create an output TSV of possible CURIEs available for each term that was not auto-selected  \"\"\"\n",
    "    conditions_manual_review['curie_info'] = conditions_manual_review['curie_info'].apply(ast.literal_eval)   # in order to multi-index, we have to group-by the original input term. To do this, first convert the column to list of lists\n",
    "    conditions_manual_review = conditions_manual_review.explode('curie_info')  # explode that column so every sublist is on a separate row\n",
    "    conditions_manual_review['curie_info'] = conditions_manual_review['curie_info'].apply(lambda x: x[:3] if isinstance(x, list) else None)   # remove the scores (sort_ratio and similarity score) from the list, don't need them and they compromise readability of manual outputs \n",
    "    conditions_manual_review['curie_info'] = conditions_manual_review['curie_info'].apply(lambda x: ', '.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "\n",
    "    conditions_manual_review['temp'] = \"temp\"   # create a temp column to facilitate multi-indexing\n",
    "    conditions_manual_review.set_index([\"orig_con\", 'curie_info'], inplace=True)   # create index\n",
    "    conditions_manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    conditions_manual_review['manually_selected_CURIE'] = None # make a column \n",
    "\n",
    "    conditions_manual_review.to_excel('{}_conditions_manual_review.xlsx'.format(relevant_date), engine='xlsxwriter', index=True)\n",
    "\n",
    "    #   -----   -----    -----   -----   INTERVENTIONS   -----   -----    -----   -----  #\n",
    "    interventions = \"{}_interventions.tsv\".format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "    \"\"\"  Create an output TSV of CURIEs that are auto-selected based on passing the threshold of scoring > 88  \"\"\"\n",
    "    interventions['auto_selected_curie'] = interventions['curie_info'].apply(filter_and_select_sublist)\n",
    "    auto_selected_interventions = interventions[interventions[['auto_selected_curie']].notnull().all(1)]\n",
    "    auto_selected_interventions = auto_selected_interventions[[\"intervention_id\", \"nct_id\", \"intervention_type\", \"orig_int\", \"description\", \"curie_info\", \"auto_selected_curie\"]]\n",
    "    auto_selected_interventions.to_csv('{}_interventions_auto_selected.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV, avoid storing in memory\n",
    "\n",
    "    interventions_manual_review = interventions[interventions[\"auto_selected_curie\"].isna()]\n",
    "    interventions_manual_review = interventions_manual_review[[\"intervention_type\", \"orig_int\", \"description\", \"curie_info\"]]\n",
    "\n",
    "    \"\"\"  Create an output TSV of possible CURIEs available for each term that was not auto-selected  \"\"\"\n",
    "    interventions_manual_review['curie_info'] = interventions_manual_review['curie_info'].apply(ast.literal_eval)\n",
    "    interventions_manual_review = interventions_manual_review.explode('curie_info')\n",
    "    interventions_manual_review['curie_info'] = interventions_manual_review['curie_info'].apply(lambda x: x[:3] if isinstance(x, list) else None)   # remove the scores (sort_ratio and similarity score) from the list, don't need them and they compromise readability of manual outputs \n",
    "    interventions_manual_review['curie_info'] = interventions_manual_review['curie_info'].apply(lambda x: ', '.join(x) if isinstance(x, list) else None)\n",
    "\n",
    "    interventions_manual_review['temp'] = \"temp\"\n",
    "    interventions_manual_review.set_index([\"intervention_type\", \"orig_int\", \"description\", 'curie_info'], inplace=True)\n",
    "    interventions_manual_review.drop([\"temp\"], axis = 1, inplace = True)\n",
    "    interventions_manual_review['manually_selected_CURIE'] = None\n",
    "\n",
    "    interventions_manual_review.to_excel('{}_interventions_manual_review.xlsx'.format(relevant_date), engine='xlsxwriter', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcb52468-d5ca-4b69-97b4-9f72be46d9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39379f69-f9e1-46dd-ac29-f344bda9c1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522d0e6-50f4-477d-a60c-a4f99693cac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fcdcb0-471a-4ea0-b865-aced689a1906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c321dc-d297-48d6-9b49-803bbbc3c274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd66ab5-7281-424b-bfbd-01b5e2369139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9791f-5d81-420e-a688-5b3825626754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca767f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ec933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca117c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e11ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d033364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47173a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2399892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c91774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d202e581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download of Clinical Trial data as of 01_19_2024\n",
      "\n",
      "\n",
      "Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\n",
      "Please store the downloaded zip in the /data directory. This should be the only item besides the cache file in the directory at this time.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Done when done:  Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at: \n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/9opmph4n5l7055moqnfu3n6kxnc0.zip\n",
      "Please make sure this the correct zip file from AACT\n",
      "Unzipping data into\n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/12_11_2023_extracted\n",
      "\n",
      "Using UMLS MetaMap to get mappings for CONDITIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "\n",
      "MetaMap version < 2020, conduct mapping on terms after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% conditions mapped: 100%|█████████████| 35/35 [00:40<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using UMLS MetaMap to get mappings for INTERVENTIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "\n",
      "MetaMap version < 2020, conduct mapping on original interventions after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% interventions mapped: 100%|██████████| 42/42 [01:03<00:00,  1.50s/it]\n",
      "% alternate_interventions mapped: 100%|█| 47/47 [00:49<00:00,  1.05s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping UMLS CURIEs and names back to clinical trials\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flag_and_path = get_raw_ct_data() # download raw data\n",
    "\n",
    "global metamap_dirs\n",
    "metamap_dirs = check_os()\n",
    "subset_size = 100\n",
    "df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "df_dict_new_terms = term_list_to_cache(df_dict, flag_and_path) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "\n",
    "term_list_to_mm(df_dict_new_terms, flag_and_path) # map new terms using MetaMap\n",
    "\n",
    "map_to_trial(flag_and_path) # map MetaMap terms back to trial \n",
    "# score_mappings(flag_and_path) # score the mappings\n",
    "# auto_select_curies(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "# add_mappings_to_cache(flag_and_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1b5ed-fcb6-4165-b8ec-a116e99bdfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict_new_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae96f93e-06e6-4941-b624-299244d6a146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict_new_terms['conditions']\n",
    "df_dict_new_terms['interventions']\n",
    "df_dict_new_terms['interventions_alts']\n",
    "\n",
    "\n",
    "df_dict_new_terms.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c3d2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ETL(subset_size):\n",
    "    \n",
    "    start_time_begin = time.time()\n",
    "    flag_and_path = get_raw_ct_data() # download raw data\n",
    "    end_time_download = time.time()\n",
    "    elapsed_time = end_time_download - start_time_begin\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    print(f\"\\nApproximate runtime for downloading or locating raw data: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "    \n",
    "    global metamap_dirs\n",
    "    metamap_dirs = check_os()\n",
    "    df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "    df_dict_new_terms = term_list_to_cache(df_dict, flag_and_path) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "    \n",
    "    start_time_mm = time.time()\n",
    "    term_list_to_mm(df_dict_new_terms, flag_and_path) # map new terms using MetaMap\n",
    "    end_time_mm = time.time()\n",
    "    elapsed_time = end_time_mm - start_time_mm\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    print(f\"Approximate runtime for mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "    \n",
    "    map_to_trial(flag_and_path) # map MetaMap terms back to trial \n",
    "    score_mappings(flag_and_path) # score the mappings\n",
    "    auto_select_curies(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "    add_mappings_to_cache(flag_and_path)\n",
    "    \n",
    "    end_time_end = time.time()\n",
    "    elapsed_time = end_time_end - start_time_begin\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    # generate_mapped_cache()\n",
    "    print(f\"Approximate runtime for overall mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_ETL(subset_size):\n",
    "    \n",
    "#     start_time_begin = time.time()\n",
    "#     flag_and_path = get_raw_ct_data() # download raw data\n",
    "#     end_time_download = time.time()\n",
    "#     elapsed_time = end_time_download - start_time_begin\n",
    "#     hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "#     print(f\"\\nApproximate runtime for downloading or locating raw data: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "    \n",
    "#     global metamap_dirs\n",
    "#     metamap_dirs = check_os()\n",
    "#     df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "    \n",
    "#     # term_list_to_cache()\n",
    "    \n",
    "#     start_time_mm = time.time()\n",
    "#     term_list_to_mm(df_dict, flag_and_path) # map using MetaMap\n",
    "#     end_time_mm = time.time()\n",
    "#     elapsed_time = end_time_mm - start_time_mm\n",
    "#     hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "#     print(f\"Approximate runtime for mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "    \n",
    "#     map_to_trial(df_dict, flag_and_path) # map MetaMap terms back to trial \n",
    "#     score_mappings(flag_and_path) # score the mappings\n",
    "#     auto_select_curies(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "#     end_time_end = time.time()\n",
    "#     elapsed_time = end_time_end - start_time_begin\n",
    "#     hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "#     # generate_mapped_cache()\n",
    "#     print(f\"Approximate runtime for overall mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af03e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_or_prod():\n",
    "    global subset_size\n",
    "    subset_size = 100\n",
    "    print(f\"The test run of this code performs the construction of the KG on a random subset of {subset_size} Conditions, {subset_size} Interventions, and {subset_size} Alternate Interventions from Clinical Trials.\")\n",
    "    test_or_prod = input(\"Is this a test run or the production of a new version of the KG? Enter Test for test, or Prod for production: \")\n",
    "    if test_or_prod == \"Test\":\n",
    "        run_ETL(subset_size)\n",
    "    elif test_or_prod == \"Prod\":\n",
    "        subset_size = None\n",
    "        run_ETL(subset_size)\n",
    "    else:\n",
    "        print(\"Bad input\")\n",
    "        sys.exit(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05525299-5c4c-47cb-a5af-26cbc4c04fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nblock out placebo etc\\ninclude knowledge_level and agent type\\nretrieve/create cache\\ntrack progress of creation\\ngive report out of how many conditions, interventions, etc. (how many new terms are mapped, how many terms are in the cache)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "delete duplicate rows from metamapped_terms_cache.tsv\n",
    "block out placebo etc\n",
    "include knowledge_level and agent type\n",
    "retrieve/create cache\n",
    "track progress of creation\n",
    "give report out of how many conditions, interventions, etc. (how many new terms are mapped, how many terms are in the cache)\n",
    "add knowledge level and agent type\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c5078bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test run of this code performs the construction of the KG on a random subset of 100 Conditions, 100 Interventions, and 100 Alternate Interventions from Clinical Trials.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is this a test run or the production of a new version of the KG? Enter Test for test, or Prod for production:  Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download of Clinical Trial data as of 01_18_2024\n",
      "\n",
      "\n",
      "Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\n",
      "Please store the downloaded zip in the /data directory. This should be the only item besides the cache file in the directory at this time.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Done when done:  Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at: \n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/9opmph4n5l7055moqnfu3n6kxnc0.zip\n",
      "Please make sure this the correct zip file from AACT\n",
      "Unzipping data into\n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/12_11_2023_extracted\n",
      "\n",
      "Approximate runtime for downloading or locating raw data: 0.0 hours, 0.0 minutes, 55.51743292808533 seconds\n",
      "\n",
      "Using UMLS MetaMap to get mappings for CONDITIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "\n",
      "MetaMap version < 2020, conduct mapping on terms after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% conditions mapped: 100%|█████████████| 70/70 [01:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using UMLS MetaMap to get mappings for INTERVENTIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "\n",
      "MetaMap version < 2020, conduct mapping on original interventions after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% interventions mapped: 100%|██████████| 83/83 [01:00<00:00,  1.37it/s]\n",
      "% alternate_interventions mapped: 100%|█| 87/87 [01:06<00:00,  1.31it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate runtime for mapping: 0.0 hours, 3.0 minutes, 13.456087112426758 seconds\n",
      "\n",
      "Mapping UMLS CURIEs and names back to clinical trials\n",
      "Scoring mappings\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: nan",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_or_prod()\n",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m, in \u001b[0;36mtest_or_prod\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m test_or_prod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs this a test run or the production of a new version of the KG? Enter Test for test, or Prod for production: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_or_prod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     run_ETL(subset_size)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_or_prod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      9\u001b[0m     subset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 23\u001b[0m, in \u001b[0;36mrun_ETL\u001b[0;34m(subset_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApproximate runtime for mapping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhours\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m hours, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminutes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m map_to_trial(flag_and_path) \u001b[38;5;66;03m# map MetaMap terms back to trial \u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m score_mappings(flag_and_path) \u001b[38;5;66;03m# score the mappings\u001b[39;00m\n\u001b[1;32m     24\u001b[0m auto_select_curies(flag_and_path) \u001b[38;5;66;03m# select CURIEs automatically that pass score threshold\u001b[39;00m\n\u001b[1;32m     25\u001b[0m add_mappings_to_cache(flag_and_path)\n",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m, in \u001b[0;36mscore_mappings\u001b[0;34m(flag_and_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m curie_info \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurie_info\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [value, curie_info]:\n\u001b[0;32m---> 18\u001b[0m     curie_sublists \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(curie_info)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m curie_sublists:\n\u001b[1;32m     20\u001b[0m         sublist\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msort_ratio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_token_sort_ratio(value,\u001b[38;5;250m \u001b[39msublist[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _convert(node_or_string)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ast.py:109\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ast.py:83\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _convert_num(node)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ast.py:74\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[0;32m---> 74\u001b[0m         _raise_malformed_node(node)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ast.py:71\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lno \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     70\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: malformed node or string: nan"
     ]
    }
   ],
   "source": [
    "test_or_prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7eef9367-fb9a-4285-968b-7a38a81bd309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using UMLS MetaMap to get mappings for CONDITIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "\n",
      "MetaMap version < 2020, conduct mapping on terms after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% conditions mapped: 100%|████████████| 14/14 [00:20<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using UMLS MetaMap to get mappings for INTERVENTIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "\n",
      "MetaMap version < 2020, conduct mapping on original interventions after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% interventions mapped: 100%|█████████| 20/20 [00:24<00:00,  1.20s/it]\n",
      "% alternate_interventions mapped: 100%|█| 19/19 [00:19<00:00,  1.05s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate runtime for mapping: 0.0 hours, 1.0 minutes, 5.00973105430603 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time_begin = time.time()\n",
    "# flag_and_path = get_raw_ct_data() # download raw data\n",
    "# end_time_download = time.time()\n",
    "# elapsed_time = end_time_download - start_time_begin\n",
    "# hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "# print(f\"Approximate runtime for downloading or locating raw data: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "# flag_and_path = get_raw_ct_data() # download raw data\n",
    "\n",
    "\n",
    "flag_and_path = {'term_program_flag': False,\n",
    "                 # 'data_extracted_path': '/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/09_26_2023_extracted',\n",
    "                 'data_extracted_path': '/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/12_11_2023_extracted',\n",
    "                 'date_string':'12_11_2023'} # comment for production\n",
    "subset_size = 20\n",
    "\n",
    "global metamap_dirs\n",
    "metamap_dirs = check_os()\n",
    "df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "\n",
    "df_dict_new_terms = term_list_to_cache(df_dict, flag_and_path)\n",
    "\n",
    "start_time_mm = time.time()\n",
    "term_list_to_mm(df_dict_new_terms, flag_and_path) # map using MetaMap\n",
    "end_time_mm = time.time()\n",
    "elapsed_time = end_time_mm - start_time_mm\n",
    "hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "print(f\"Approximate runtime for mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "map_to_trial(flag_and_path) # map MetaMap terms back to trial \n",
    "# score_mappings(flag_and_path) # score the mappings\n",
    "# auto_select_curies(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "# end_time_end = time.time()\n",
    "# elapsed_time = end_time_end - start_time_begin\n",
    "# hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "# print(f\"Approximate runtime for overall mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3df17-570e-40e8-86a5-1436144edf16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa868a-abb2-43a0-baca-6bb92f4b27ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f845e1a-bf3e-4c0e-a32a-da4d169337cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4fc1f-dcc2-408d-b897-caa9cb9b7466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffb409-16ec-432b-bcfc-14f93e19b2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0758ab-12ba-4d5a-a901-bf3914235212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88caed-2e32-4d89-a69d-d2f76bb93edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):  # more options can be specified also\n",
    "    display(interventions_manual_review[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408b038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f341e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1da2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15b6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514e180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_stats(df_dict, flag_and_path):\n",
    "    \"\"\" Report counts of conditions, interventions\"\"\"\n",
    "    relevant_date = flag_and_path[\"date_string\"] # get date\n",
    "    \n",
    "    total_conditions = df_dict[\"conditions\"].downcase_name\n",
    "    total_conditions = list(total_conditions.unique())\n",
    "    total_conditions = list(filter(None, total_conditions))\n",
    "    \n",
    "    orig_interventions = df_dict[\"interventions\"]\n",
    "    orig_interventions = orig_interventions['name'].str.lower()\n",
    "    orig_interventions = list(orig_interventions.unique())\n",
    "    orig_interventions = list(filter(None, orig_interventions))\n",
    "    \n",
    "    alt_interventions = df_dict[\"interventions_alts\"].alt_downcase_name\n",
    "    alt_interventions = list(alt_interventions.unique())\n",
    "    alt_interventions = list(filter(None, alt_interventions))\n",
    "    \n",
    "#     metamap_input = \"{}_metamap_output.tsv\".format(relevant_date)\n",
    "    \n",
    "#     \"\"\" Get the full names of the semantic types and replace the abbreviations with the full names \"\"\"\n",
    "#     metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    print(\"Clinical Trial Data from: {}\".format(relevant_date))\n",
    "    print(\"Total # of unique conditions : {}\".format(len(total_conditions)))\n",
    "    print(\"Total # of unique interventions : {}\".format(len(orig_interventions) + len(alt_interventions)))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
