{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966a883b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "# lets you preint multiple outputs per cell, not just last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263f3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "import concurrent\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import sys\n",
    "import urllib\n",
    "import zipfile\n",
    "import csv\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36ea5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48301bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global metamap_dirs\n",
    "# global metamap_pos_server_dir\n",
    "# global metamap_wsd_server_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0709cfa-2550-4a44-8525-6b2beb10b46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_seconds_to_hms(seconds):\n",
    "\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bacfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "def get_token_sort_ratio(str1, str2):\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "\n",
    "def get_token_set_ratio(str1, str2):\n",
    "    try:\n",
    "        return fuzz.token_set_ratio(str1, str2)\n",
    "    except:\n",
    "        return None  \n",
    "set_ratio = np.vectorize(get_token_set_ratio)\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "sim_score = np.vectorize(get_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51b6e9-b1be-4d68-96ca-70c453ff87d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# term_program_flag = True\n",
    "# global data_dir\n",
    "# global data_extracted\n",
    "\n",
    "# # get all the links and associated dates of upload into a dict called date_link\n",
    "# url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "# response = requests.get(url_all)\n",
    "# soup = BeautifulSoup(response.text)\n",
    "# body = soup.find_all('option') #Find all\n",
    "# date_link = {}\n",
    "# for el in body:\n",
    "#     tags = el.find('a')\n",
    "#     try:\n",
    "#         zip_name = tags.contents[0].split()[0]\n",
    "#         date = zip_name.split(\"_\")[0]\n",
    "#         date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "#         date_link[date] = tags.get('href')\n",
    "#     except:\n",
    "#         pass\n",
    "# latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "# url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "# date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "# data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "# data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "# data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "\n",
    "# if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "#     term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "#     print(\"Attempting download of Clinical Trial data as of {}\".format(date_string))\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             with open(data_path, 'wb') as file:\n",
    "#                 file.write(response.content)\n",
    "#             print(\"Finished download of zip\")\n",
    "#             with zipfile.ZipFile(data_path, 'r') as download:\n",
    "#                 print(\"Unzipping data\")\n",
    "#                 download.extractall(data_extracted)\n",
    "#     except:\n",
    "#         print(\"Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "#         print(\"Please store the downloaded zip in the /data folder\")\n",
    "#         done = input(\"Type Done when done: \")\n",
    "#         if done == \"Done\":\n",
    "#             data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "#             list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "#             try:\n",
    "#                 latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "#                 print(\"File found:\")\n",
    "#                 with zipfile.ZipFile(latest_file, 'r') as download:\n",
    "#                     print(\"Unzipping data into\")\n",
    "#                     cttime = os.path.getctime(latest_file)\n",
    "#                     date_string = dt.datetime.fromtimestamp(cttime).strftime('%Y_%m_%d')\n",
    "#                     data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "#                     print(data_extracted)\n",
    "#                     download.extractall(data_extracted)\n",
    "#             except:\n",
    "#                 print(\"Unable to download and extract Clincal Trial data.\")\n",
    "#                 print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "# else:\n",
    "#     print(\"KG is already up to date.\")\n",
    "    \n",
    "# return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c7e3d8-1c7a-44a2-afe5-6721f7ca2438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "\n",
    "    # get all the links and associated dates of upload into a dict called date_link\n",
    "    url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "    response = requests.get(url_all)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    body = soup.find_all('option') #Find all\n",
    "    date_link = {}\n",
    "    for el in body:\n",
    "        tags = el.find('a')\n",
    "        try:\n",
    "            zip_name = tags.contents[0].split()[0]\n",
    "            date = zip_name.split(\"_\")[0]\n",
    "            date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "            date_link[date] = tags.get('href')\n",
    "        except:\n",
    "            pass\n",
    "    latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "    url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "    date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "    data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "    data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "    data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Attempting download of Clinical Trial data as of {}\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    print(\"File found at: \")\n",
    "                    print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(latest_file, 'r') as download:\n",
    "                            print(\"Unzipping data into\")\n",
    "                            cttime = os.path.getctime(latest_file)\n",
    "                            date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                            data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                            print(data_extracted)\n",
    "                            download.extractall(data_extracted)\n",
    "                    except:\n",
    "                        print(\"Assuming data is already unzipped\")\n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer working bc AACT reformatted its page\n",
    "\n",
    "# def get_raw_ct_data():\n",
    "#     term_program_flag = True\n",
    "#     global data_dir\n",
    "#     global data_extracted\n",
    "    \n",
    "#     # get all the links and associated dates of upload into a dict called date_link\n",
    "#     url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "#     response = requests.get(url_all)\n",
    "#     soup = BeautifulSoup(response.text)\n",
    "#     body = soup.find_all('option') #Find all\n",
    "#     date_link = {}\n",
    "#     for el in body:\n",
    "#         tags = el.find('a')\n",
    "#         try:\n",
    "#             zip_name = tags.contents[0].split()[0]\n",
    "#             date = zip_name.split(\"_\")[0]\n",
    "#             date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "#             date_link[date] = tags.get('href')\n",
    "#         except:\n",
    "#             pass\n",
    "#     latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "#     url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "#     date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "#     data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "#     data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "#     data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    \n",
    "#     if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "        \n",
    "#         term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "#         print(\"Downloading Clinical Trial data as of {}\".format(date_string))\n",
    "#         response = requests.get(url)\n",
    "#         if response.status_code == 200:\n",
    "#             with open(data_path, 'wb') as file:\n",
    "#                 file.write(response.content)\n",
    "#             print(\"Finished download of zip\")\n",
    "#             with zipfile.ZipFile(data_path, 'r') as download:\n",
    "#                 print(\"Unzipping data\")\n",
    "#                 download.extractall(data_extracted)\n",
    "#         else:\n",
    "#             print(\"KG is already up to date.\")\n",
    "#     return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bccd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        \n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "        alternate_interventions_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0)\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            alternate_interventions_df = alternate_interventions_df.sample(n=subset_size)\n",
    "\n",
    "    return {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": alternate_interventions_df}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f277771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2877eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    # Start servers\n",
    "    os.system(metamap_dirs['metamap_base_dir'] + metamap_pos_server_dir + ' start') # Part of speech tagger\n",
    "    os.system(metamap_dirs['metamap_base_dir'] + metamap_wsd_server_dir + ' start') # Word sense disambiguation \n",
    "    # # Sleep a bit to give time for these servers to start up\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    # Stop servers\n",
    "    os.system(metamap_dirs['metamap_base_dir'] + metamap_pos_server_dir + ' stop') # Part of speech tagger\n",
    "    os.system(metamap_dirs['metamap_base_dir'] + metamap_wsd_server_dir + ' stop') # Word sense disambiguation \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42e63785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ea402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metamap(input_term, params, mm, cond_or_inter, csv_writer):\n",
    "    from_metamap = []\n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],\n",
    "                                                )\n",
    "\n",
    "            for concept in concepts:\n",
    "                concept_info = []\n",
    "                concept = concept._asdict()\n",
    "                concept_info.extend([cond_or_inter,input_term])\n",
    "                concept_info.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "                from_metamap.append(concept_info)\n",
    "        except:\n",
    "            from_metamap.extend([input_term, None, None, None, None, None, None])\n",
    "    else:\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],\n",
    "                                                )\n",
    "\n",
    "            for concept in concepts:\n",
    "                concept_info = []\n",
    "                concept = concept._asdict()\n",
    "                concept_info.extend([cond_or_inter,input_term])\n",
    "                concept_info.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "                from_metamap.append(concept_info)\n",
    "        except:\n",
    "            from_metamap.extend([input_term, None, None, None, None, None, None])\n",
    "        \n",
    "    for result in from_metamap:\n",
    "        # print(result)\n",
    "        csv_writer.writerow(result)\n",
    "    return from_metamap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a937acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_metamap(term_list, params, cond_or_inter, flag_and_path, csv_writer):\n",
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "    with concurrent.futures.ThreadPoolExecutor((multiprocessing.cpu_count()*2) - 1) as executor:\n",
    "        _ = [executor.submit(run_metamap, term, params, mm, cond_or_inter, csv_writer) for term in term_list]\n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a21d47",
   "metadata": {},
   "source": [
    "# USE METAMAP LOCAL TO MAP REMAINING TERMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f37ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_mm(df_dict, flag_and_path):\n",
    "        \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    # some input terms have () with additional text, like an abbreviation, in them. split them out to facilitate better mapping using these regex patterns that we use to find substrings inside and outside ()\n",
    "    pattern_outside = r'(?<=\\().+?(?=\\))|([^(]+)'\n",
    "    pattern_inside = r'\\(([^)]+)\\)'\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "\n",
    "    # -------    CONDITIONS    ------- #\n",
    "    conditions = df_dict[\"conditions\"][['id', 'nct_id', 'downcase_name']]\n",
    "    conditions.rename(columns = {'downcase_name':'orig_con'}, inplace = True)\n",
    "\n",
    "    if metamap_version[0] >= 20:\n",
    "        matches_outside = conditions['orig_con'].str.extract(pattern_outside)\n",
    "        conditions['orig_con_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = conditions['orig_con'].str.extract(pattern_inside)\n",
    "        conditions['orig_con_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "    else:\n",
    "        conditions['deascii_con'] = deasciier(conditions['orig_con'])\n",
    "        matches_outside = conditions['deascii_con'].str.extract(pattern_outside)\n",
    "        conditions['deascii_con_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = conditions['deascii_con'].str.extract(pattern_inside)\n",
    "        conditions['deascii_con_inside'] = matches_inside[0].fillna('')\n",
    "    \n",
    "#     see MetaMap Usage instructions: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/MM_2016_Usage.pdf\n",
    "#     condition_args = ['--sldi -I -C -J acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf -z -i -f']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    \n",
    "    # prep output file of Metamap results\n",
    "    filename = f\"{relevant_date}_metamap_output.tsv\"\n",
    "    metamap_output = open(filename, 'w+', newline='')\n",
    "    col_names = ['term_type', 'clin_trial_term','metamap_preferred_name', 'metamap_cui', 'metamap_score', 'metamap_semantic_type']\n",
    "    csv_writer = csv.writer(metamap_output, delimiter='\\t')\n",
    "    csv_writer.writerow(col_names)\n",
    "    \n",
    "    if metamap_version[0] >= 20:\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        orig_cons = conditions.orig_con.unique().tolist()\n",
    "        orig_cons = list(filter(None, orig_cons))\n",
    "        orig_cons = [str(i) for i in orig_cons]\n",
    "        parallelize_metamap(orig_con, params, \"condition\", flag_and_path, csv_writer)\n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        deascii_cons = conditions.deascii_con.unique().tolist()\n",
    "        deascii_cons = list(filter(None, deascii_cons))\n",
    "        deascii_cons = [str(i) for i in deascii_cons]\n",
    "        parallelize_metamap(deascii_cons, params, \"condition\", flag_and_path, csv_writer)\n",
    "        \n",
    "        \"\"\" If the substring that was either outside or inside the () is identical to the term from which it came from, or actually any of the columns have the same value, put None in that cell/put None where that term is duplicated \"\"\"    \n",
    "    # Iterate through each column in the DataFrame\n",
    "    for col1 in conditions.columns:\n",
    "        for col2 in conditions.columns:\n",
    "            # Skip comparing a column with itself\n",
    "            if col1 != col2:\n",
    "                # Check if the values in col2 are duplicates of col1\n",
    "                conditions[col2] = conditions.apply(lambda row: row[col2] if row[col2] != row[col1] else None, axis=1)\n",
    "    # Drop duplicate columns (keeping the first instance)\n",
    "    conditions = conditions.T.drop_duplicates().T\n",
    "\n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "    \n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    print(\"Using UMLS MetaMap to get mappings for INTERVENTIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\")\n",
    "    \n",
    "    \"\"\" Interventions requires unique handling. Another table gives possible alternate names for the interventions in addition to the \"original\" names. \n",
    "        We may map on the alternate names column\n",
    "        We take the interventions, take the ascii and deasciied versions of them,\n",
    "        and split substrings in parentheses out of them. We perform MetaMapping on the\n",
    "        original term or the deasciied term dependinging on what operating system we\n",
    "        are on. If the mapped term passes the fuzzy scoring thesholds for any of the\n",
    "        terms (original, deasciied, original inside the parentheses, deasciied inside\n",
    "        the parentheses, original outside the parentheses, deasciied outside the\n",
    "        parentheses\"\"\" \n",
    "\n",
    "    interventions_df = df_dict[\"interventions\"]\n",
    "    interventions_df['orig_downcase_name'] = interventions_df['name'].str.lower()\n",
    "    interventions_alts = df_dict[\"interventions_alts\"]\n",
    "    interventions_alts['alt_downcase_name'] = interventions_alts['name'].str.lower()\n",
    "\n",
    "    orig_ints = interventions_df[\"orig_downcase_name\"]\n",
    "    orig_ints = list(orig_ints.unique())\n",
    "    orig_ints = list(filter(None, orig_ints))\n",
    "    alt_ints = interventions_alts[\"alt_downcase_name\"]\n",
    "    alt_ints = list(alt_ints.unique())\n",
    "    alt_ints = list(filter(None, alt_ints))\n",
    "\n",
    "    params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    \"\"\" Send the prepared interventions to MetaMap now. If we are on OSX, we have to use MetaMap 2018, which requires deasciied terms. If we are on Linux, we can use MetaMap 2020, which does not require such preprocessing \"\"\"\n",
    "    if metamap_version[0] < 20:\n",
    "        deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "        #  -------   original interventions  -------- #\n",
    "        orig_ints = [str(i) for i in orig_ints]\n",
    "        orig_ints = deasciier(orig_ints) # perform deascii-ing on original intervention names\n",
    "        orig_ints = list(orig_ints)\n",
    "        print(\"MetaMap version < 2020, conduct mapping on original interventions after removing ascii characters\")\n",
    "        parallelize_metamap(orig_ints, params, \"intervention\", flag_and_path, csv_writer)\n",
    "        #  ---------   alternate interventions ------- #\n",
    "        alt_ints = [str(i) for i in alt_ints]\n",
    "        alt_ints = deasciier(alt_ints) # perform deascii-ing on alternate intervention names\n",
    "        alt_ints = list(alt_ints)\n",
    "        parallelize_metamap(alt_ints, params, \"alternate_intervention\", flag_and_path, csv_writer)\n",
    "\n",
    "    else:\n",
    "        #  -------   original interventions  -------- #\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original interventions\")\n",
    "        parallelize_metamap(orig_ints, params, \"intervention\", flag_and_path, csv_writer)\n",
    "        #  ---------   alternate interventions ------- #\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on alternate interventions\")\n",
    "        parallelize_metamap(alt_ints, params, \"alternate_intervention\", flag_and_path, csv_writer)\n",
    "\n",
    "    interventions_all = pd.merge(interventions_df[[\"id\", \"nct_id\", \"intervention_type\", \"orig_downcase_name\", \"description\"]], interventions_alts[[\"nct_id\", \"intervention_id\", \"alt_downcase_name\"]], how='left', left_on=['id'], right_on = ['intervention_id'])\n",
    "    interventions_all = interventions_all.astype(str)\n",
    "    interventions_all = interventions_all.drop('nct_id_y', axis=1) # drop the redundant column now\n",
    "    interventions_all.rename(columns = {'nct_id_x':'nct_id'}, inplace = True)\n",
    "\n",
    "    interventions_all = interventions_all.sort_values(by='nct_id', ascending=False, na_position='last')\n",
    "    interventions_all = interventions_all.drop('intervention_id', axis=1) # drop the redundant column now\n",
    "    interventions_all.rename(columns = {'id':'intervention_id', 'orig_downcase_name':'orig_int', 'alt_downcase_name':'alt_int'}, inplace = True)\n",
    "\n",
    "    if metamap_version[0] >= 20:\n",
    "        matches_outside = interventions_all['orig_int'].str.extract(pattern_outside)\n",
    "        interventions_all['orig_int_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = interventions_all['orig_int'].str.extract(pattern_inside)\n",
    "        interventions_all['orig_int_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "        matches_outside = interventions_all['alt_int'].str.extract(pattern_outside)\n",
    "        interventions_all['alt_int_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = interventions_all['alt_in'].str.extract(pattern_inside)\n",
    "        interventions_all['alt_int_inside'] = matches_inside[0].fillna('')\n",
    "    else:\n",
    "        interventions_all['deascii_orig_int'] = deasciier(interventions_all['orig_int'])\n",
    "        interventions_all['deascii_alt_int'] = deasciier(interventions_all['alt_int'])\n",
    "\n",
    "        matches_outside = interventions_all['deascii_orig_int'].str.extract(pattern_outside)\n",
    "        interventions_all['deascii_orig_int_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = interventions_all['deascii_orig_int'].str.extract(pattern_inside)\n",
    "        interventions_all['deascii_orig_int_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "        matches_outside = interventions_all['deascii_alt_int'].str.extract(pattern_outside)\n",
    "        interventions_all['deascii_alt_int_outside'] = matches_outside[0].fillna('')\n",
    "        matches_inside = interventions_all['deascii_alt_int'].str.extract(pattern_inside)\n",
    "        interventions_all['deascii_alt_name_inside'] = matches_inside[0].fillna('')\n",
    "\n",
    "    \"\"\" I don't want to perform mapping on strings < 4 char in length; these are ambiguous and it's hard to make a call what that concept should be \"\"\"\n",
    "    \"\"\" Get character counts of all the columns to evaluate \"\"\"    \n",
    "    for col in interventions_all.columns: # get the char counts of each column\n",
    "        char_count_col_name = col + '_char_count'\n",
    "        interventions_all[char_count_col_name] = interventions_all[col].str.len()\n",
    "\n",
    "    \"\"\" If char_count < 4, replace the string in the corresponding column with None so that we don't use it for comparison \"\"\"    \n",
    "    for col in interventions_all.columns[interventions_all.columns.str.contains(\"char_count\")]:\n",
    "        for index, value in interventions_all[col].items():\n",
    "            if value < 4:\n",
    "                # Find the column with the most similar name without \"char_count\" substring\n",
    "                most_similar_col = interventions_all.columns[interventions_all.columns.str.replace(\"_char_count\", \"\") == col.replace(\"_char_count\", \"\")].values[0]\n",
    "                # Update the value in the most similar column\n",
    "                interventions_all.at[index, most_similar_col] = None\n",
    "        interventions_all = interventions_all.drop(col, axis=1) # drop the count columns now  \n",
    "        \n",
    "    \"\"\" If the substring that was either outside or inside the () is identical to the term from which it came from, or actually any of the columns have the same value, put None in that cell/put None where that term is duplicated \"\"\"    \n",
    "    # Iterate through each column in the DataFrame\n",
    "    for col1 in interventions_all.columns:\n",
    "        for col2 in interventions_all.columns:\n",
    "            # Skip comparing a column with itself\n",
    "            if col1 != col2:\n",
    "                # Check if the values in col2 are duplicates of col1\n",
    "                interventions_all[col2] = interventions_all.apply(lambda row: row[col2] if row[col2] != row[col1] else None, axis=1)\n",
    "    # Drop duplicate columns (keeping the first instance)\n",
    "    interventions_all = interventions_all.T.drop_duplicates().T\n",
    "\n",
    "\n",
    "    interventions_all.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42025af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_trial(df_dict, flag_and_path):\n",
    "    # send mappings to interventions and conditions, group CUIs that correspond to input condition or intervention\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "\n",
    "    metamap_input = \"{}_metamap_output.tsv\".format(relevant_date)\n",
    "    metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    # get the full names of the semantic types so we know what we're looking at\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\")\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "    sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "    metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names)\n",
    "    sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "    # Handle NaN (None) values in metamap_semantic_type column\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan)\n",
    "    # map semantic type abbreviations to the full name of the semantic type\n",
    "    metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x)\n",
    "\n",
    "    metamapped['metamap_preferred_name'] = metamapped['metamap_preferred_name'].str.lower()\n",
    "    metamapped = metamapped.dropna(axis=0)\n",
    "    metamapped = metamapped[[\"term_type\", \"clin_trial_term\", \"metamap_cui\",\"metamap_preferred_name\", \"metamap_semantic_type\"]]\n",
    "\n",
    "    metamapped[\"metamap_term_info\"] = metamapped[[\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"]].values.tolist() \n",
    "    metamapped.drop([\"metamap_cui\", \"metamap_preferred_name\", \"metamap_semantic_type\"], axis = 1, inplace = True)\n",
    "    metamapped = metamapped.groupby(['term_type', 'clin_trial_term'])['metamap_term_info'].agg(list).reset_index()\n",
    "\n",
    "    conditions = '{}_conditions.tsv'.format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    interventions = '{}_interventions.tsv'.format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    metamapped_con = metamapped.loc[metamapped['term_type'] == \"condition\"]\n",
    "    metamapped_int = metamapped.loc[(metamapped['term_type'] == \"intervention\") | (metamapped['term_type'] == \"alternate_intervention\")]\n",
    "\n",
    "    mapper_con = dict(zip(metamapped_con['clin_trial_term'], metamapped_con['metamap_term_info'])) # make a dict to map conditions\n",
    "    mapper_int = dict(zip(metamapped_int['clin_trial_term'], metamapped_int['metamap_term_info'])) # make a dict to map interventions\n",
    "\n",
    "#     cols_to_check = [ele for ele in conditions.columns if(ele not in ['id', 'nct_id', 'condition_id'])]\n",
    "    cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "\n",
    "    conditions[\"curie_info\"] = None\n",
    "\n",
    "    for index, row in conditions.iterrows():\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            if value in mapper_con:\n",
    "                curie_info = mapper_con[value]\n",
    "                conditions.at[index, \"curie_info\"] = curie_info    \n",
    "                \n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output conditions to TSV\n",
    "\n",
    "#     cols_to_check = [ele for ele in interventions.columns if(ele not in ['id', 'nct_id', 'intervention_id', 'intervention_type', 'description'])]\n",
    "    cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "\n",
    "    interventions[\"curie_info\"] = None\n",
    "\n",
    "    for index, row in interventions.iterrows():\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            if value in mapper_int:\n",
    "                curie_info = mapper_int[value]\n",
    "                interventions.at[index, \"curie_info\"] = curie_info\n",
    "    \n",
    "    interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcfdd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mappings(flag_and_path):\n",
    "    \n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    \n",
    "    #   -- --- --   CONDITIONS   -- --- -- #\n",
    "    conditions = \"{}_conditions.tsv\".format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    cols_to_check = [ele for ele in conditions.columns if any([substr in ele for substr in ['_con']])]\n",
    "    conditions = conditions.where(pd.notnull(conditions), None)\n",
    "\n",
    "    for index, row in conditions.iterrows():\n",
    "        curies_sublists_scored = []\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            curie_info = row[\"curie_info\"]\n",
    "            if None not in [value, curie_info]:\n",
    "#                 print(value)\n",
    "#                 print(curie_info)\n",
    "#                 print()\n",
    "                curie_sublists = ast.literal_eval(curie_info)\n",
    "                for sublist in curie_sublists:\n",
    "                    sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "                    sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "                    curies_sublists_scored.append(sublist)\n",
    "        conditions.at[index, \"curie_info\"] = curies_sublists_scored\n",
    "    conditions.to_csv('{}_conditions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    #   -- --- --   INTERVENTIONS   -- --- -- #\n",
    "    \n",
    "    interventions = \"{}_interventions.tsv\".format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "    cols_to_check = [ele for ele in interventions.columns if any([substr in ele for substr in ['_int']])]\n",
    "    interventions = interventions.where(pd.notnull(interventions), None)\n",
    "\n",
    "    for index, row in interventions.iterrows():\n",
    "        curies_sublists_scored = []\n",
    "        for col_name in cols_to_check:\n",
    "            value = row[col_name]\n",
    "            curie_info = row[\"curie_info\"]\n",
    "            if None not in [value, curie_info]:\n",
    "#                 print(value)\n",
    "#                 print(curie_info)\n",
    "#                 print()\n",
    "                curie_sublists = ast.literal_eval(curie_info)\n",
    "                for sublist in curie_sublists:\n",
    "                    sublist.append(f'sort_ratio: {get_token_sort_ratio(value, sublist[1])}')\n",
    "                    sublist.append(f'similarity_score: {get_similarity_score(value, sublist[1])}')\n",
    "                    curies_sublists_scored.append(sublist)\n",
    "\n",
    "        interventions.at[index, \"curie_info\"] = curies_sublists_scored\n",
    "    interventions.to_csv('{}_interventions.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e75aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_select_curies(flag_and_path):\n",
    "\n",
    "    relevant_date = flag_and_path[\"date_string\"]   # get date of bulk download of clinical trial data\n",
    "    \n",
    "    def filter_and_select_sublist(sublists):  # function to find the highest score of a CURIE, and pick that curie if it's greater than threshold of 88\n",
    "        if sublists is None or len(sublists) == 0:\n",
    "            return None\n",
    "\n",
    "        high_score = -1\n",
    "        selected_sublist = None\n",
    "\n",
    "        sublists = ast.literal_eval(sublists)\n",
    "        for sublist in sublists:\n",
    "\n",
    "            if len(sublist) >= 4:\n",
    "                sort_ratio = int(sublist[3].split(\": \")[1])\n",
    "                sim_score = int(sublist[4].split(\": \")[1])\n",
    "                max_score = max(sort_ratio, sim_score)\n",
    "                if max_score > 88: \n",
    "                    if max_score > high_score:\n",
    "                        high_score = max_score\n",
    "                        selected_sublist = sublist\n",
    "        return selected_sublist\n",
    "\n",
    "    #   -----   -----    -----   -----   CONDITIONS   -----   -----    -----   -----  #\n",
    "\n",
    "    conditions = \"{}_conditions.tsv\".format(relevant_date)\n",
    "    conditions = pd.read_csv(conditions, sep='\\t', index_col=False, header=0)\n",
    "    \"\"\"  Create an output TSV of CURIEs that are auto-selected based on passing the threshold of scoring > 88  \"\"\"\n",
    "    conditions['auto_selected_curie'] = conditions['curie_info'].apply(filter_and_select_sublist)  # select CURIE that scores highest using filter_and_select_sublist function = auto-select\n",
    "    auto_selected_conditions = conditions[conditions[['auto_selected_curie']].notnull().all(1)]   # get the rows where a CURIE has been auto-selected\n",
    "    auto_selected_conditions = auto_selected_conditions[[\"id\", \"nct_id\", \"orig_con\", \"curie_info\", \"auto_selected_curie\"]]  # subset dataframe\n",
    "    auto_selected_conditions.to_csv('{}_conditions_auto_selected.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    conditions_manual_review = conditions[conditions[\"auto_selected_curie\"].isna()]   # select rows where no CURIE was auto-selected\n",
    "    conditions_manual_review = conditions_manual_review[[\"orig_con\", \"curie_info\"]]  # subset\n",
    "\n",
    "    \"\"\"  Create an output TSV of possible CURIEs available for each term that was not auto-selected  \"\"\"\n",
    "    conditions_manual_review['curie_info'] = conditions_manual_review['curie_info'].apply(ast.literal_eval)   # in order to multi-index, we have to group-by the original input term. To do this, first convert the column to list of lists\n",
    "    conditions_manual_review = conditions_manual_review.explode('curie_info')  # explode that column so every sublist is on a separate row\n",
    "    conditions_manual_review['curie_info'] = conditions_manual_review['curie_info'].apply(lambda x: x[:3] if isinstance(x, list) else None)   # remove the scores (sort_ratio and similarity score) from the list, don't need them and they compromise readability of manual outputs \n",
    "    conditions_manual_review['curie_info'] = conditions_manual_review['curie_info'].apply(lambda x: ', '.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "\n",
    "    conditions_manual_review['temp'] = \"temp\"   # create a temp column to facilitate multi-indexing\n",
    "    conditions_manual_review.set_index([\"orig_con\", 'curie_info'], inplace=True)   # create index\n",
    "    conditions_manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    conditions_manual_review['manually_selected_CURIE'] = None # make a column \n",
    "\n",
    "    conditions_manual_review.to_excel('{}_conditions_manual_review.xlsx'.format(relevant_date), engine='xlsxwriter', index=True)\n",
    "\n",
    "    #   -----   -----    -----   -----   INTERVENTIONS   -----   -----    -----   -----  #\n",
    "    interventions = \"{}_interventions.tsv\".format(relevant_date)\n",
    "    interventions = pd.read_csv(interventions, sep='\\t', index_col=False, header=0)\n",
    "    \"\"\"  Create an output TSV of CURIEs that are auto-selected based on passing the threshold of scoring > 88  \"\"\"\n",
    "    interventions['auto_selected_curie'] = interventions['curie_info'].apply(filter_and_select_sublist)\n",
    "    auto_selected_interventions = interventions[interventions[['auto_selected_curie']].notnull().all(1)]\n",
    "    auto_selected_interventions = auto_selected_interventions[[\"intervention_id\", \"nct_id\", \"intervention_type\", \"orig_int\", \"description\", \"curie_info\", \"auto_selected_curie\"]]\n",
    "    auto_selected_interventions.to_csv('{}_interventions_auto_selected.tsv'.format(relevant_date), sep=\"\\t\", index=False, header=True) # output interventions to TSV, avoid storing in memory\n",
    "\n",
    "    interventions_manual_review = interventions[interventions[\"auto_selected_curie\"].isna()]\n",
    "    interventions_manual_review = interventions_manual_review[[\"intervention_type\", \"orig_int\", \"description\", \"curie_info\"]]\n",
    "\n",
    "    \"\"\"  Create an output TSV of possible CURIEs available for each term that was not auto-selected  \"\"\"\n",
    "    interventions_manual_review['curie_info'] = interventions_manual_review['curie_info'].apply(ast.literal_eval)\n",
    "    interventions_manual_review = interventions_manual_review.explode('curie_info')\n",
    "    interventions_manual_review['curie_info'] = interventions_manual_review['curie_info'].apply(lambda x: x[:3] if isinstance(x, list) else None)   # remove the scores (sort_ratio and similarity score) from the list, don't need them and they compromise readability of manual outputs \n",
    "    interventions_manual_review['curie_info'] = interventions_manual_review['curie_info'].apply(lambda x: ', '.join(x) if isinstance(x, list) else None)\n",
    "\n",
    "    interventions_manual_review['temp'] = \"temp\"\n",
    "    interventions_manual_review.set_index([\"intervention_type\", \"orig_int\", \"description\", 'curie_info'], inplace=True)\n",
    "    interventions_manual_review.drop([\"temp\"], axis = 1, inplace = True)\n",
    "    interventions_manual_review['manually_selected_CURIE'] = None\n",
    "\n",
    "    interventions_manual_review.to_excel('{}_interventions_manual_review.xlsx'.format(relevant_date), engine='xlsxwriter', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd031435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930e56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e9602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed7a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aeb087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4f877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1907806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710580bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eb126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056876c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512e030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e346f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5ae65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279aa7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213d679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca767f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ec933",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):  # more options can be specified also\n",
    "    display(interventions_manual_review[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca117c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e11ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d033364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380043c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477b58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c18874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904724a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399340fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d510b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78185069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027aa8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595a17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f24e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014de22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d05cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b171ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c589cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b86250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a831c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488ae46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e48fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032f4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593963b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed6368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6a0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fde5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c5919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd56e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e5d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e3c609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8882ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4ed8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd74361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2509f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b128aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb019cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc8c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb296fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ad8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710b95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb0aebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce18055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2bc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b90f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b1276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7810c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e519da39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962dc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b066f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0ec0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b112f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f9856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3223ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80c1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddd7be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47173a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2399892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4098323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fb92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb26a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c91774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e2840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36037a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30301d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43207cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d60082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202e581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d9178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a41a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1acec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97702834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5909d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d282944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2351c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd2c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679118d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c7ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84bdba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2c975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61570ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949f1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ed925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9f0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39bb5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb926b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8146f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1eb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20851297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe13dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df97302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71e361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c9977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7ccb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba2567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d2172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17c36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b512307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ETL(subset_size):\n",
    "    \n",
    "    start_time_begin = time.time()\n",
    "    flag_and_path = get_raw_ct_data() # download raw data\n",
    "    end_time_download = time.time()\n",
    "    elapsed_time = end_time_download - start_time_begin\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    print(f\"Approximate runtime for downloading or locating raw data: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "    \n",
    "    global metamap_dirs\n",
    "    metamap_dirs = check_os()\n",
    "    df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "    \n",
    "    start_time_mm = time.time()\n",
    "    term_list_to_mm(df_dict, flag_and_path) # map using MetaMap\n",
    "    end_time_mm = time.time()\n",
    "    elapsed_time = end_time_mm - start_time_mm\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    print(f\"Approximate runtime for mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "    \n",
    "    map_to_trial(df_dict, flag_and_path) # map MetaMap terms back to trial \n",
    "    score_mappings(flag_and_path) # score the mappings\n",
    "    auto_select_curies(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "    end_time_end = time.time()\n",
    "    elapsed_time = end_time_end - start_time_begin\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "    print(f\"Approximate runtime for overall mapping: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af03e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_or_prod():\n",
    "    print(\"The test run of this code performs the construction of the KG on a random subset of 200 Conditions, 200 Interventions, and 200 Alternate Interventions from Clinical Trials.\\n\")\n",
    "    test_or_prod = input(\"Is this a test run or the production of a new version of the KG? Enter Test for test, or Prod for production: \")\n",
    "    if test_or_prod == \"Test\":\n",
    "        subset_size = 200\n",
    "        run_ETL(subset_size)\n",
    "    elif test_or_prod == \"Prod\":\n",
    "        subset_size = None\n",
    "        run_ETL(subset_size)\n",
    "    else:\n",
    "        print(\"Bad input\")\n",
    "        sys.exit(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c5078bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test run of this code performs the construction of the KG on a random subset of 200 Conditions, 200 Interventions, and 200 Alternate Interventions from Clinical Trials.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is this a test run or the production of a new version of the KG? Enter Test for test, or Prod for production:  Test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download of Clinical Trial data as of 12_12_2023\n",
      "Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\n",
      "Please store the downloaded zip in the /data directory\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Done when done:  Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at: \n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/9opmph4n5l7055moqnfu3n6kxnc0.zip\n",
      "Please make sure this the correct zip file from AACT\n",
      "Unzipping data into\n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/12_11_2023_extracted\n",
      "Approximate runtime for downloading: 0.0 hours, 1.0 minutes, 49.11586093902588 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/49/18qlr0bn7xg_mk2pzxvrb0f80000gn/T/ipykernel_63903/3837732174.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  conditions.rename(columns = {'downcase_name':'orig_con'}, inplace = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaMap version < 2020, conduct mapping on terms after removing ascii characters\n",
      "Starting skrmedpostctl: \n",
      "started.\n",
      "Starting wsdserverctl: \n",
      "started.\n",
      "loading properties file /Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/WSD_Server/config/disambServer.cfg\n",
      "WSD Server initializing disambiguation methods.\n",
      "WSD Server databases and disambiguation methods have been initialized.\n",
      "Could not listen on port : 5554 : Address already in use\n",
      "Stopping skrmedpostctl: \n",
      "Stopping Tagger Server process..\n",
      "Process 67315 stopped\n",
      "Stopping wsdserverctl: \n",
      "Stopping WSD Server process..\n",
      "Process 67317 stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin/skrmedpostctl: line 50: kill: (67315) - No such process\n",
      "/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin/wsdserverctl: line 55: kill: (67317) - No such process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UMLS MetaMap to get mappings for INTERVENTIONS. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "MetaMap version < 2020, conduct mapping on original interventions after removing ascii characters\n",
      "Starting skrmedpostctl: \n",
      "started.\n",
      "Starting wsdserverctl: \n",
      "started.\n",
      "loading properties file /Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/WSD_Server/config/disambServer.cfg\n",
      "WSD Server initializing disambiguation methods.\n",
      "WSD Server databases and disambiguation methods have been initialized.\n",
      "Could not listen on port : 5554 : Address already in use\n",
      "Stopping skrmedpostctl: \n",
      "Stopping Tagger Server process..\n",
      "Process 70746 stopped\n",
      "Stopping wsdserverctl: \n",
      "Stopping WSD Server process..\n",
      "Process 70748 stopped\n",
      "Starting skrmedpostctl: \n",
      "started.\n",
      "Starting wsdserverctl: \n",
      "started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin/skrmedpostctl: line 50: kill: (70746) - No such process\n",
      "/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin/wsdserverctl: line 55: kill: (70748) - No such process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading properties file /Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/WSD_Server/config/disambServer.cfg\n",
      "WSD Server initializing disambiguation methods.\n",
      "WSD Server databases and disambiguation methods have been initialized.\n",
      "Could not listen on port : 5554 : Address already in use\n",
      "Stopping skrmedpostctl: \n",
      "Stopping Tagger Server process..\n",
      "Process 74285 stopped\n",
      "Stopping wsdserverctl: \n",
      "Stopping WSD Server process..\n",
      "Process 74287 stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin/skrmedpostctl: line 50: kill: (74285) - No such process\n",
      "/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin/wsdserverctl: line 55: kill: (74287) - No such process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate runtime for mapping: 0.0 hours, 9.0 minutes, 52.96195721626282 seconds\n",
      "Approximate runtime for overall mapping: 0.0 hours, 11.0 minutes, 48.02316904067993 seconds\n"
     ]
    }
   ],
   "source": [
    "test_or_prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9946bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_ETL_mapping(flag_and_path):\n",
    "#     df_dict = read_raw_ct_data(flag_and_path)\n",
    "#     ct_terms = exact_match_mesh(df_dict)\n",
    "#     ct_terms = inexact_match_mesh(df_dict, ct_terms)\n",
    "\n",
    "#     # pull the available MeSH terms per study out of the returned ct_terms dict \n",
    "#     mesh_conditions_per_study = ct_terms[\"mesh_conditions_per_study\"]\n",
    "#     mesh_interventions_per_study = ct_terms[\"mesh_interventions_per_study\"]\n",
    "\n",
    "#     ct_terms = term_list_to_nr(df_dict, ct_terms)\n",
    "#     ct_terms = term_list_to_mm(df_dict, ct_terms)\n",
    "\n",
    "#     # pull the available UMLS terms per study out of the returned ct_terms dict \n",
    "#     all_metamapped_conditions = ct_terms[\"all_metamapped_conditions\"]\n",
    "#     all_metamapped_interventions = ct_terms[\"all_metamapped_interventions\"]\n",
    "\n",
    "#     remaining_unmapped_possible = {\"mesh_conditions_per_study\": mesh_conditions_per_study,\n",
    "#                                    \"mesh_interventions_per_study\": mesh_interventions_per_study,\n",
    "#                                    \"all_metamapped_conditions\": all_metamapped_conditions,\n",
    "#                                    \"all_metamapped_interventions\": all_metamapped_interventions}\n",
    "#     compile_and_output(df_dict, ct_terms, remaining_unmapped_possible)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29911c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f144ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a94f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8b9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5607f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e73280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408b038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f341e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1da2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15b6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514e180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d75b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flag_and_path = get_raw_ct_data() # uncomment for production\n",
    "flag_and_path = {'term_program_flag': False,\n",
    "                 'data_extracted_path': '/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/09_26_2023_extracted',\n",
    "                 'date_string':'09_26_2023'} # comment for production\n",
    "metamap_dirs = check_os()\n",
    "df_dict = read_raw_ct_data(flag_and_path)\n",
    "term_list_to_mm(df_dict, flag_and_path)\n",
    "map_to_trial(df_dict, flag_and_path)\n",
    "score_mappings(flag_and_path)\n",
    "auto_select_curies(flag_and_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ed3c8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp of script start: 12-12-2023, 13:17:31\n",
      "Runtime: 0.0 hours, 0.0 minutes, 2.7894973754882812e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "def convert_seconds_to_hms(seconds):\n",
    "\n",
    "    \"\"\" converts the elapsed or run_time to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "current = dt.datetime.now()\n",
    "ts = dt.datetime.timestamp(current)\n",
    "d = dt.datetime.fromtimestamp(ts)\n",
    "str_date_time = d.strftime(\"%d-%m-%Y, %H:%M:%S\")\n",
    "print(\"Timestamp of script start: {}\".format(str_date_time))\n",
    "\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "hours, minutes, seconds = convert_seconds_to_hms(elapsed_time)\n",
    "print(f\"Runtime: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0122932a-4e56-4da0-9b68-142169612ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class TimerError(Exception):\n",
    "    \"\"\"A custom exception used to report errors in use of Timer class\"\"\"\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start a new timer\"\"\"\n",
    "        if self._start_time is not None:\n",
    "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
    "\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n",
    "        if self._start_time is None:\n",
    "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
    "\n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        print(f\"Elapsed time: {elapsed_time:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_stats(df_dict, flag_and_path):\n",
    "    \"\"\" Report counts of conditions, interventions\"\"\"\n",
    "    relevant_date = flag_and_path[\"date_string\"] # get date\n",
    "    \n",
    "    total_conditions = df_dict[\"conditions\"].downcase_name\n",
    "    total_conditions = list(total_conditions.unique())\n",
    "    total_conditions = list(filter(None, total_conditions))\n",
    "    \n",
    "    orig_interventions = df_dict[\"interventions\"]\n",
    "    orig_interventions = orig_interventions['name'].str.lower()\n",
    "    orig_interventions = list(orig_interventions.unique())\n",
    "    orig_interventions = list(filter(None, orig_interventions))\n",
    "    \n",
    "    alt_interventions = df_dict[\"interventions_alts\"].alt_downcase_name\n",
    "    alt_interventions = list(alt_interventions.unique())\n",
    "    alt_interventions = list(filter(None, alt_interventions))\n",
    "    \n",
    "#     metamap_input = \"{}_metamap_output.tsv\".format(relevant_date)\n",
    "    \n",
    "#     \"\"\" Get the full names of the semantic types and replace the abbreviations with the full names \"\"\"\n",
    "#     metamapped = pd.read_csv(metamap_input, sep='\\t', index_col=False, header=0)\n",
    "\n",
    "    print(\"Clinical Trial Data from: {}\".format(relevant_date))\n",
    "    print(\"Total # of unique conditions : {}\".format(len(total_conditions)))\n",
    "    print(\"Total # of unique interventions : {}\".format(len(orig_interventions) + len(alt_interventions)))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
