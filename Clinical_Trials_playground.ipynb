{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install git+https://github.com/biothings/biothings_explorer#egg=biothings_explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import re\n",
    "from collections import namedtuple\n",
    "# import py4j\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\"--name job_name --master local --conf spark.dynamicAllocation.enabled=true pyspark-shell\"\"\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk1.8.0_251.jdk/Contents/Home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding Folder_2/subfolder to the system path\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "from pymetamap import MetaMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas.io.sql as sqlio\n",
    "# from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "  \n",
    "from pymetamap import MetaMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import lxml\n",
    "import config\n",
    "import pickle\n",
    "\n",
    "# from Authentication import *\n",
    "import requests\n",
    "import json\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://aact.ctti-clinicaltrials.org/pipe_files\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text)\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://aact.ctti-clinicaltrials.org/static/exported_files/monthly/20220301_pipe-delimited-export.zip\n",
    "# https://aact.ctti-clinicaltrials.org/static/exported_files/monthly/20220801_pipe-delimited-export.zip\n",
    "upload_dates = []\n",
    "zip_files = []\n",
    "links = []\n",
    "body = soup.find_all('td', attrs={'class': 'file-archive'}) #Find all\n",
    "for el in body:\n",
    "    tags = el.find('a')\n",
    "    try:\n",
    "        if 'href' in tags.attrs:   # looking for href inside anchor tag    \n",
    "            link = \"https://aact.ctti-clinicaltrials.org\" + tags.get('href')\n",
    "            links.append(link)\n",
    "            last_upload = link.split(\"/\")[-1]\n",
    "            zip_files.append(last_upload)\n",
    "            date_upload = last_upload.split(\"_\")[0]\n",
    "            upload_dates.append(date_upload)    # appending link to list of links\n",
    "    except:    # pass if list missing anchor tag or anchor tag does not has a href params \n",
    "        pass\n",
    "    \n",
    "print(upload_dates)\n",
    "\n",
    "upload_dates = [dt.datetime.strptime(date, '%Y%m%d').date() for date in upload_dates] # convert all strings in list into datetime objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_date = max(upload_dates)\n",
    "# print(most_recent_date)\n",
    "\n",
    "today = dt.date.today()\n",
    "\n",
    "# UNCOMMENT LINE BELOW WHEN IN PRODUCTION, THIS IS FOR TESTING PURPOSES\n",
    "most_recent_date = min(upload_dates)\n",
    "\n",
    "# fix this to run entire script should the current date be \n",
    "if most_recent_date < today:\n",
    "#     date_file_df = pd.DataFrame(list(zip(upload_dates, zip_files, links)))\n",
    "#     print(links[0])\n",
    "    resp = urlopen(links[0])\n",
    "else:\n",
    "    print(\"Local instance of clinical trials data is already up to date.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 3000)\n",
    "# date_file_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to DB and get the column names of the table\n",
    "con = None\n",
    "con = psycopg2.connect(database=\"aact\")\n",
    "con.rollback()\n",
    "cursor = con.cursor()\n",
    "\n",
    "con.autocommit = True # SQL statement is treated as a transaction and is automatically committed right after it is executed\n",
    "# grab the conditions\n",
    "sql = '''SELECT * FROM ctgov.conditions;'''\n",
    "cursor.execute(sql)\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "tuples = cursor.fetchall()\n",
    "conditions_df = pd.DataFrame(tuples, columns=column_names)\n",
    "\n",
    "#grab the browse_conditions\n",
    "sql = '''SELECT * FROM ctgov.browse_conditions;'''\n",
    "cursor.execute(sql)\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "tuples = cursor.fetchall()\n",
    "browse_conditions_df = pd.DataFrame(tuples, columns=column_names)\n",
    "\n",
    "#grab the interventions\n",
    "sql = '''SELECT * FROM ctgov.interventions;'''\n",
    "cursor.execute(sql)\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "tuples = cursor.fetchall()\n",
    "interventions_df = pd.DataFrame(tuples, columns=column_names)\n",
    "\n",
    "#grab the browse_interventions\n",
    "sql = '''SELECT * FROM ctgov.browse_interventions;'''\n",
    "cursor.execute(sql)\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "tuples = cursor.fetchall()\n",
    "browse_interventions_df = pd.DataFrame(tuples, columns=column_names)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_id</th>\n",
       "      <th>int_nctid</th>\n",
       "      <th>int_type</th>\n",
       "      <th>int_name</th>\n",
       "      <th>int_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4547568</td>\n",
       "      <td>NCT01340339</td>\n",
       "      <td>Device</td>\n",
       "      <td>Phototherapy</td>\n",
       "      <td>Fluorescent reverse phototherapy (7 white ligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4547569</td>\n",
       "      <td>NCT01340339</td>\n",
       "      <td>Device</td>\n",
       "      <td>Phototherapy</td>\n",
       "      <td>super LED reverse phototherapy (17 bulbs arran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4547570</td>\n",
       "      <td>NCT01340365</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>Tai Chi Exercise</td>\n",
       "      <td>Practicing Tai Chi exercise 4 times a week for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4547571</td>\n",
       "      <td>NCT01340365</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>Tai Chi</td>\n",
       "      <td>Usual care, individuals attend testing session...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4547572</td>\n",
       "      <td>NCT01340391</td>\n",
       "      <td>Device</td>\n",
       "      <td>Omnicast</td>\n",
       "      <td>dorsal splint 2-5 weeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685271</th>\n",
       "      <td>4139681</td>\n",
       "      <td>NCT03104712</td>\n",
       "      <td>Other</td>\n",
       "      <td>Rice + Pesto</td>\n",
       "      <td>50g available carbohydrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685272</th>\n",
       "      <td>4139682</td>\n",
       "      <td>NCT03104465</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>Mindfulness Training</td>\n",
       "      <td>interactive. web-based mindfulness training co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685273</th>\n",
       "      <td>4139683</td>\n",
       "      <td>NCT03104361</td>\n",
       "      <td>Biological</td>\n",
       "      <td>Platelet-Rich Plasma</td>\n",
       "      <td>Patients who meet all eligible requirements fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685274</th>\n",
       "      <td>4139684</td>\n",
       "      <td>NCT03104283</td>\n",
       "      <td>Drug</td>\n",
       "      <td>Apatinib</td>\n",
       "      <td>take apatinib orally until disease progression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685275</th>\n",
       "      <td>4139685</td>\n",
       "      <td>NCT03104231</td>\n",
       "      <td>Other</td>\n",
       "      <td>three-dimensional (3D) movie</td>\n",
       "      <td>Brief VR-3D will be shown using the oculus gla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>685276 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         int_id    int_nctid    int_type                      int_name  \\\n",
       "0       4547568  NCT01340339      Device                  Phototherapy   \n",
       "1       4547569  NCT01340339      Device                  Phototherapy   \n",
       "2       4547570  NCT01340365  Behavioral              Tai Chi Exercise   \n",
       "3       4547571  NCT01340365  Behavioral                       Tai Chi   \n",
       "4       4547572  NCT01340391      Device                      Omnicast   \n",
       "...         ...          ...         ...                           ...   \n",
       "685271  4139681  NCT03104712       Other                  Rice + Pesto   \n",
       "685272  4139682  NCT03104465  Behavioral          Mindfulness Training   \n",
       "685273  4139683  NCT03104361  Biological          Platelet-Rich Plasma   \n",
       "685274  4139684  NCT03104283        Drug                      Apatinib   \n",
       "685275  4139685  NCT03104231       Other  three-dimensional (3D) movie   \n",
       "\n",
       "                                          int_description  \n",
       "0       Fluorescent reverse phototherapy (7 white ligh...  \n",
       "1       super LED reverse phototherapy (17 bulbs arran...  \n",
       "2       Practicing Tai Chi exercise 4 times a week for...  \n",
       "3       Usual care, individuals attend testing session...  \n",
       "4                                 dorsal splint 2-5 weeks  \n",
       "...                                                   ...  \n",
       "685271                         50g available carbohydrate  \n",
       "685272  interactive. web-based mindfulness training co...  \n",
       "685273  Patients who meet all eligible requirements fo...  \n",
       "685274  take apatinib orally until disease progression...  \n",
       "685275  Brief VR-3D will be shown using the oculus gla...  \n",
       "\n",
       "[685276 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interventions_df = interventions_df.rename(columns={'id': 'int_id',\n",
    "                                                    'nct_id': 'int_nctid',\n",
    "                                                    'intervention_type': 'int_type',\n",
    "                                                    'name': 'int_name',\n",
    "                                                    'description': 'int_description'})\n",
    "interventions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interventions_df = interventions_df.drop(columns=['int_id', 'int_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con_id</th>\n",
       "      <th>con_nctid</th>\n",
       "      <th>con_name</th>\n",
       "      <th>con_downcase_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3072834</td>\n",
       "      <td>NCT03254264</td>\n",
       "      <td>Autism Spectrum Disorder</td>\n",
       "      <td>autism spectrum disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3072835</td>\n",
       "      <td>NCT03254329</td>\n",
       "      <td>Human Milk Nutrient Reference Values</td>\n",
       "      <td>human milk nutrient reference values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3349485</td>\n",
       "      <td>NCT03460652</td>\n",
       "      <td>ADHD</td>\n",
       "      <td>adhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3349486</td>\n",
       "      <td>NCT03460899</td>\n",
       "      <td>Diabetes Mellitus With Hypoglycemia</td>\n",
       "      <td>diabetes mellitus with hypoglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3072836</td>\n",
       "      <td>NCT03254342</td>\n",
       "      <td>Major Depressive Disorder</td>\n",
       "      <td>major depressive disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688783</th>\n",
       "      <td>3072052</td>\n",
       "      <td>NCT03261687</td>\n",
       "      <td>Pregnancy Related</td>\n",
       "      <td>pregnancy related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688784</th>\n",
       "      <td>3072053</td>\n",
       "      <td>NCT03261687</td>\n",
       "      <td>Pelvic Girdle Pain</td>\n",
       "      <td>pelvic girdle pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688785</th>\n",
       "      <td>3072054</td>\n",
       "      <td>NCT03261622</td>\n",
       "      <td>Faecal Incontinence</td>\n",
       "      <td>faecal incontinence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688786</th>\n",
       "      <td>3072055</td>\n",
       "      <td>NCT03261622</td>\n",
       "      <td>Fecal Incontinence</td>\n",
       "      <td>fecal incontinence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688787</th>\n",
       "      <td>3072056</td>\n",
       "      <td>NCT03261934</td>\n",
       "      <td>Risk Assessment of Chronic Atrophic Gastritis ...</td>\n",
       "      <td>risk assessment of chronic atrophic gastritis ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>688788 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         con_id    con_nctid  \\\n",
       "0       3072834  NCT03254264   \n",
       "1       3072835  NCT03254329   \n",
       "2       3349485  NCT03460652   \n",
       "3       3349486  NCT03460899   \n",
       "4       3072836  NCT03254342   \n",
       "...         ...          ...   \n",
       "688783  3072052  NCT03261687   \n",
       "688784  3072053  NCT03261687   \n",
       "688785  3072054  NCT03261622   \n",
       "688786  3072055  NCT03261622   \n",
       "688787  3072056  NCT03261934   \n",
       "\n",
       "                                                 con_name  \\\n",
       "0                                Autism Spectrum Disorder   \n",
       "1                    Human Milk Nutrient Reference Values   \n",
       "2                                                    ADHD   \n",
       "3                     Diabetes Mellitus With Hypoglycemia   \n",
       "4                               Major Depressive Disorder   \n",
       "...                                                   ...   \n",
       "688783                                  Pregnancy Related   \n",
       "688784                                 Pelvic Girdle Pain   \n",
       "688785                                Faecal Incontinence   \n",
       "688786                                 Fecal Incontinence   \n",
       "688787  Risk Assessment of Chronic Atrophic Gastritis ...   \n",
       "\n",
       "                                        con_downcase_name  \n",
       "0                                autism spectrum disorder  \n",
       "1                    human milk nutrient reference values  \n",
       "2                                                    adhd  \n",
       "3                     diabetes mellitus with hypoglycemia  \n",
       "4                               major depressive disorder  \n",
       "...                                                   ...  \n",
       "688783                                  pregnancy related  \n",
       "688784                                 pelvic girdle pain  \n",
       "688785                                faecal incontinence  \n",
       "688786                                 fecal incontinence  \n",
       "688787  risk assessment of chronic atrophic gastritis ...  \n",
       "\n",
       "[688788 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions_df = conditions_df.rename(columns={'id': 'con_id',\n",
    "                                              'nct_id': 'con_nctid',\n",
    "                                              'name': 'con_name',\n",
    "                                              'downcase_name': 'con_downcase_name'})\n",
    "conditions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_df = conditions_df.drop(columns=['con_id', 'con_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>browseint_id</th>\n",
       "      <th>browseint_nctid</th>\n",
       "      <th>browseint_meshterm</th>\n",
       "      <th>browseint_meshterm_downcase</th>\n",
       "      <th>browseint_meshtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4935968</td>\n",
       "      <td>NCT04327843</td>\n",
       "      <td>Haloperidol</td>\n",
       "      <td>haloperidol</td>\n",
       "      <td>mesh-list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4935969</td>\n",
       "      <td>NCT04327843</td>\n",
       "      <td>Haloperidol decanoate</td>\n",
       "      <td>haloperidol decanoate</td>\n",
       "      <td>mesh-list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4935970</td>\n",
       "      <td>NCT04327843</td>\n",
       "      <td>Antiemetics</td>\n",
       "      <td>antiemetics</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5198960</td>\n",
       "      <td>NCT00808223</td>\n",
       "      <td>Alefacept</td>\n",
       "      <td>alefacept</td>\n",
       "      <td>mesh-list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5198961</td>\n",
       "      <td>NCT00808223</td>\n",
       "      <td>Dermatologic Agents</td>\n",
       "      <td>dermatologic agents</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335262</th>\n",
       "      <td>4400668</td>\n",
       "      <td>NCT03100786</td>\n",
       "      <td>Antineoplastic Agents, Hormonal</td>\n",
       "      <td>antineoplastic agents, hormonal</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335263</th>\n",
       "      <td>4400669</td>\n",
       "      <td>NCT03100786</td>\n",
       "      <td>Antineoplastic Agents</td>\n",
       "      <td>antineoplastic agents</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335264</th>\n",
       "      <td>4400670</td>\n",
       "      <td>NCT03100565</td>\n",
       "      <td>Lidocaine</td>\n",
       "      <td>lidocaine</td>\n",
       "      <td>mesh-list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335265</th>\n",
       "      <td>4400671</td>\n",
       "      <td>NCT03100565</td>\n",
       "      <td>Anesthetics, Local</td>\n",
       "      <td>anesthetics, local</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335266</th>\n",
       "      <td>4400672</td>\n",
       "      <td>NCT03100565</td>\n",
       "      <td>Anesthetics</td>\n",
       "      <td>anesthetics</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1335267 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         browseint_id browseint_nctid               browseint_meshterm  \\\n",
       "0             4935968     NCT04327843                      Haloperidol   \n",
       "1             4935969     NCT04327843            Haloperidol decanoate   \n",
       "2             4935970     NCT04327843                      Antiemetics   \n",
       "3             5198960     NCT00808223                        Alefacept   \n",
       "4             5198961     NCT00808223              Dermatologic Agents   \n",
       "...               ...             ...                              ...   \n",
       "1335262       4400668     NCT03100786  Antineoplastic Agents, Hormonal   \n",
       "1335263       4400669     NCT03100786            Antineoplastic Agents   \n",
       "1335264       4400670     NCT03100565                        Lidocaine   \n",
       "1335265       4400671     NCT03100565               Anesthetics, Local   \n",
       "1335266       4400672     NCT03100565                      Anesthetics   \n",
       "\n",
       "             browseint_meshterm_downcase browseint_meshtype  \n",
       "0                            haloperidol          mesh-list  \n",
       "1                  haloperidol decanoate          mesh-list  \n",
       "2                            antiemetics      mesh-ancestor  \n",
       "3                              alefacept          mesh-list  \n",
       "4                    dermatologic agents      mesh-ancestor  \n",
       "...                                  ...                ...  \n",
       "1335262  antineoplastic agents, hormonal      mesh-ancestor  \n",
       "1335263            antineoplastic agents      mesh-ancestor  \n",
       "1335264                        lidocaine          mesh-list  \n",
       "1335265               anesthetics, local      mesh-ancestor  \n",
       "1335266                      anesthetics      mesh-ancestor  \n",
       "\n",
       "[1335267 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browse_interventions_df = browse_interventions_df.rename(columns={'id': 'browseint_id',\n",
    "                                                                  'nct_id': 'browseint_nctid',\n",
    "                                                                  'mesh_term': 'browseint_meshterm',\n",
    "                                                                  'downcase_mesh_term': 'browseint_meshterm_downcase',\n",
    "                                                                  'mesh_type': 'browseint_meshtype'})\n",
    "browse_interventions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "browse_interventions_df = browse_interventions_df.drop(columns=['browseint_id', 'browseint_meshterm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>browsecon_id</th>\n",
       "      <th>browsecon_nctid</th>\n",
       "      <th>browsecon_meshterm</th>\n",
       "      <th>browsecon_meshterm_downcase</th>\n",
       "      <th>browsecon_meshtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9631397</td>\n",
       "      <td>NCT04035564</td>\n",
       "      <td>Hyponatremia</td>\n",
       "      <td>hyponatremia</td>\n",
       "      <td>mesh-list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9631398</td>\n",
       "      <td>NCT04035564</td>\n",
       "      <td>Water-Electrolyte Imbalance</td>\n",
       "      <td>water-electrolyte imbalance</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9631399</td>\n",
       "      <td>NCT04035564</td>\n",
       "      <td>Metabolic Diseases</td>\n",
       "      <td>metabolic diseases</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9631400</td>\n",
       "      <td>NCT04032652</td>\n",
       "      <td>Colitis</td>\n",
       "      <td>colitis</td>\n",
       "      <td>mesh-list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9631401</td>\n",
       "      <td>NCT04032652</td>\n",
       "      <td>Colitis, Ulcerative</td>\n",
       "      <td>colitis, ulcerative</td>\n",
       "      <td>mesh-list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573556</th>\n",
       "      <td>8601072</td>\n",
       "      <td>NCT03213899</td>\n",
       "      <td>Disease Attributes</td>\n",
       "      <td>disease attributes</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573557</th>\n",
       "      <td>8601073</td>\n",
       "      <td>NCT03213899</td>\n",
       "      <td>Pathologic Processes</td>\n",
       "      <td>pathologic processes</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573558</th>\n",
       "      <td>8601074</td>\n",
       "      <td>NCT03214016</td>\n",
       "      <td>Hypertension</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>mesh-list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573559</th>\n",
       "      <td>8601075</td>\n",
       "      <td>NCT03214016</td>\n",
       "      <td>Vascular Diseases</td>\n",
       "      <td>vascular diseases</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573560</th>\n",
       "      <td>8601076</td>\n",
       "      <td>NCT03214016</td>\n",
       "      <td>Cardiovascular Diseases</td>\n",
       "      <td>cardiovascular diseases</td>\n",
       "      <td>mesh-ancestor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2573561 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         browsecon_id browsecon_nctid           browsecon_meshterm  \\\n",
       "0             9631397     NCT04035564                 Hyponatremia   \n",
       "1             9631398     NCT04035564  Water-Electrolyte Imbalance   \n",
       "2             9631399     NCT04035564           Metabolic Diseases   \n",
       "3             9631400     NCT04032652                      Colitis   \n",
       "4             9631401     NCT04032652          Colitis, Ulcerative   \n",
       "...               ...             ...                          ...   \n",
       "2573556       8601072     NCT03213899           Disease Attributes   \n",
       "2573557       8601073     NCT03213899         Pathologic Processes   \n",
       "2573558       8601074     NCT03214016                 Hypertension   \n",
       "2573559       8601075     NCT03214016            Vascular Diseases   \n",
       "2573560       8601076     NCT03214016      Cardiovascular Diseases   \n",
       "\n",
       "         browsecon_meshterm_downcase browsecon_meshtype  \n",
       "0                       hyponatremia          mesh-list  \n",
       "1        water-electrolyte imbalance      mesh-ancestor  \n",
       "2                 metabolic diseases      mesh-ancestor  \n",
       "3                            colitis          mesh-list  \n",
       "4                colitis, ulcerative          mesh-list  \n",
       "...                              ...                ...  \n",
       "2573556           disease attributes      mesh-ancestor  \n",
       "2573557         pathologic processes      mesh-ancestor  \n",
       "2573558                 hypertension          mesh-list  \n",
       "2573559            vascular diseases      mesh-ancestor  \n",
       "2573560      cardiovascular diseases      mesh-ancestor  \n",
       "\n",
       "[2573561 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browse_conditions_df = browse_conditions_df.rename(columns={'id': 'browsecon_id',\n",
    "                                                            'nct_id': 'browsecon_nctid',\n",
    "                                                            'mesh_term': 'browsecon_meshterm',\n",
    "                                                            'downcase_mesh_term': 'browsecon_meshterm_downcase',\n",
    "                                                            'mesh_type': 'browsecon_meshtype'})\n",
    "browse_conditions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "browse_conditions_df = browse_conditions_df.drop(columns=['browsecon_id', 'browsecon_meshterm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350361\n",
      "95250\n",
      "4280\n",
      "3744\n"
     ]
    }
   ],
   "source": [
    "print(len(interventions_df['int_name'].unique()))\n",
    "print(len(conditions_df['con_downcase_name'].unique()))\n",
    "print(len(browse_conditions_df['browsecon_meshterm_downcase'].unique()))\n",
    "print(len(browse_interventions_df['browseint_meshterm_downcase'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con_nctid</th>\n",
       "      <th>con_downcase_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT03254264</td>\n",
       "      <td>autism spectrum disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT03254329</td>\n",
       "      <td>human milk nutrient reference values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT03460652</td>\n",
       "      <td>adhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT03460899</td>\n",
       "      <td>diabetes mellitus with hypoglycemia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT03254342</td>\n",
       "      <td>major depressive disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688783</th>\n",
       "      <td>NCT03261687</td>\n",
       "      <td>pregnancy related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688784</th>\n",
       "      <td>NCT03261687</td>\n",
       "      <td>pelvic girdle pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688785</th>\n",
       "      <td>NCT03261622</td>\n",
       "      <td>faecal incontinence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688786</th>\n",
       "      <td>NCT03261622</td>\n",
       "      <td>fecal incontinence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688787</th>\n",
       "      <td>NCT03261934</td>\n",
       "      <td>risk assessment of chronic atrophic gastritis ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>688788 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          con_nctid                                  con_downcase_name\n",
       "0       NCT03254264                           autism spectrum disorder\n",
       "1       NCT03254329               human milk nutrient reference values\n",
       "2       NCT03460652                                               adhd\n",
       "3       NCT03460899                diabetes mellitus with hypoglycemia\n",
       "4       NCT03254342                          major depressive disorder\n",
       "...             ...                                                ...\n",
       "688783  NCT03261687                                  pregnancy related\n",
       "688784  NCT03261687                                 pelvic girdle pain\n",
       "688785  NCT03261622                                faecal incontinence\n",
       "688786  NCT03261622                                 fecal incontinence\n",
       "688787  NCT03261934  risk assessment of chronic atrophic gastritis ...\n",
       "\n",
       "[688788 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(conditions_df, interventions_df, left_on='con_nctid', right_on = 'int_nctid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1256248, 5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup = df.drop_duplicates(subset = ['con_downcase_name', 'int_name'],\n",
    "                                      keep = 'first').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df_dedup.head(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedup[['con_downcase_name', 'int_name']].to_csv('ClinTrials_KG_edges.csv', sep ='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes on creating edge file\n",
    "\n",
    "go to /proj/arivale/gglusman/KGs\n",
    "\n",
    "once there look into wellness/source/wellness_kg_edges_v1.6.tsv\n",
    "\n",
    "notice the first line: subject\tpredicate\tobject\trelation\tsubject_name\tobject_name\tcategory\tN\tType_of_relationship\tStrength_of_relationship\tqualifiers\tqualifier_value\tBonferroni_pval\n",
    "\n",
    "subject, predicate, object are required (maybe some others, don’t remember)\n",
    "\n",
    "…and they should be CURIEs\n",
    "\n",
    "once we map concepts to biolink, mesh etc we’ll have curies\n",
    "\n",
    "meanwhile, the content you have fits subject_name, object_name\n",
    "\n",
    "the predicate is what we made up… let’s say ‘evaluated_for’\n",
    "\n",
    "for short\n",
    "\n",
    "we’ll have to find a suitable biolink predicate, or suggest a new one\n",
    "\n",
    "so interventions would be subjects, and conditions would be objects\n",
    "\n",
    "looking at the several first lines in your file, I see some that make sense, others that seem to suggest there are data problems\n",
    "\n",
    "for example, “metastatic lung cancer\tPathways” -> Pathways doesn’t seem to be an intervention\n",
    "\n",
    "‘placebo’ we’ll have to suppress\n",
    "\n",
    "make sure to include the NCT id in a (non-required) column\n",
    "\n",
    "it’s essentially part of the provenance\n",
    "\n",
    "we’ll want to add multiple additional columns with things like FDA approval status, sample size, etc etc etc\n",
    "\n",
    "look also into EHR/source/ehr_risk_kg_edges_2021_05_07.tsv\n",
    "\n",
    "…and of course replace ‘edges’ -> ‘nodes’ for their cognate nodes files\n",
    "\n",
    "wellness/source/wellness_kg_edges_v1.6.tsv -> wellness/source/wellness_kg_nodes_v1.6.tsv\n",
    "\n",
    "drop \"control\"\n",
    "\n",
    "‘control’ is clear, ‘control group’ too… but ‘Control Ostomy Barrier’ is likely not a ‘control’\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My own notes\n",
    "\n",
    "Use pool/multiprocess to make faster\n",
    "resolve multiple possible CURIEs per concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get only relevant columns from DB\n",
    "ct_extract = pd.DataFrame(df_dedup[['con_nctid', 'con_downcase_name', 'int_type', 'int_name']])\n",
    "ct_extract = ct_extract.rename(columns={'con_nctid': 'nctid'})\n",
    "# get CURIE column for nct_id column (https://bioregistry.io/registry/clinicaltrials)\n",
    "\n",
    "ct_extract['nctid_curie'] = ct_extract['nctid']\n",
    "ct_extract['nctid_curie'] = 'clinicaltrials:' + ct_extract['nctid'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nctid</th>\n",
       "      <th>con_downcase_name</th>\n",
       "      <th>int_type</th>\n",
       "      <th>int_name</th>\n",
       "      <th>nctid_curie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT03254264</td>\n",
       "      <td>autism spectrum disorder</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>Early Start Denver Model</td>\n",
       "      <td>clinicaltrials:NCT03254264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT03254264</td>\n",
       "      <td>autism spectrum disorder</td>\n",
       "      <td>Behavioral</td>\n",
       "      <td>'as-usual' intervention</td>\n",
       "      <td>clinicaltrials:NCT03254264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT03254329</td>\n",
       "      <td>human milk nutrient reference values</td>\n",
       "      <td>Other</td>\n",
       "      <td>Assessment of human milk nutrient composition</td>\n",
       "      <td>clinicaltrials:NCT03254329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT03460652</td>\n",
       "      <td>adhd</td>\n",
       "      <td>Drug</td>\n",
       "      <td>KP415 oral capsule</td>\n",
       "      <td>clinicaltrials:NCT03460652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT03460899</td>\n",
       "      <td>diabetes mellitus with hypoglycemia</td>\n",
       "      <td>Other</td>\n",
       "      <td>Euglycaemic Clamp</td>\n",
       "      <td>clinicaltrials:NCT03460899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972680</th>\n",
       "      <td>NCT03261687</td>\n",
       "      <td>pelvic girdle pain</td>\n",
       "      <td>Other</td>\n",
       "      <td>water based exercise</td>\n",
       "      <td>clinicaltrials:NCT03261687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972681</th>\n",
       "      <td>NCT03261687</td>\n",
       "      <td>pelvic girdle pain</td>\n",
       "      <td>Other</td>\n",
       "      <td>land based exercise</td>\n",
       "      <td>clinicaltrials:NCT03261687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972682</th>\n",
       "      <td>NCT03261622</td>\n",
       "      <td>faecal incontinence</td>\n",
       "      <td>Device</td>\n",
       "      <td>Sacral nerve stimulation at different stimulat...</td>\n",
       "      <td>clinicaltrials:NCT03261622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972683</th>\n",
       "      <td>NCT03261622</td>\n",
       "      <td>fecal incontinence</td>\n",
       "      <td>Device</td>\n",
       "      <td>Sacral nerve stimulation at different stimulat...</td>\n",
       "      <td>clinicaltrials:NCT03261622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972684</th>\n",
       "      <td>NCT03261934</td>\n",
       "      <td>risk assessment of chronic atrophic gastritis ...</td>\n",
       "      <td>Other</td>\n",
       "      <td>No intervention</td>\n",
       "      <td>clinicaltrials:NCT03261934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>972685 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              nctid                                  con_downcase_name  \\\n",
       "0       NCT03254264                           autism spectrum disorder   \n",
       "1       NCT03254264                           autism spectrum disorder   \n",
       "2       NCT03254329               human milk nutrient reference values   \n",
       "3       NCT03460652                                               adhd   \n",
       "4       NCT03460899                diabetes mellitus with hypoglycemia   \n",
       "...             ...                                                ...   \n",
       "972680  NCT03261687                                 pelvic girdle pain   \n",
       "972681  NCT03261687                                 pelvic girdle pain   \n",
       "972682  NCT03261622                                faecal incontinence   \n",
       "972683  NCT03261622                                 fecal incontinence   \n",
       "972684  NCT03261934  risk assessment of chronic atrophic gastritis ...   \n",
       "\n",
       "          int_type                                           int_name  \\\n",
       "0       Behavioral                           Early Start Denver Model   \n",
       "1       Behavioral                            'as-usual' intervention   \n",
       "2            Other      Assessment of human milk nutrient composition   \n",
       "3             Drug                                 KP415 oral capsule   \n",
       "4            Other                                  Euglycaemic Clamp   \n",
       "...            ...                                                ...   \n",
       "972680       Other                               water based exercise   \n",
       "972681       Other                                land based exercise   \n",
       "972682      Device  Sacral nerve stimulation at different stimulat...   \n",
       "972683      Device  Sacral nerve stimulation at different stimulat...   \n",
       "972684       Other                                    No intervention   \n",
       "\n",
       "                       nctid_curie  \n",
       "0       clinicaltrials:NCT03254264  \n",
       "1       clinicaltrials:NCT03254264  \n",
       "2       clinicaltrials:NCT03254329  \n",
       "3       clinicaltrials:NCT03460652  \n",
       "4       clinicaltrials:NCT03460899  \n",
       "...                            ...  \n",
       "972680  clinicaltrials:NCT03261687  \n",
       "972681  clinicaltrials:NCT03261687  \n",
       "972682  clinicaltrials:NCT03261622  \n",
       "972683  clinicaltrials:NCT03261622  \n",
       "972684  clinicaltrials:NCT03261934  \n",
       "\n",
       "[972685 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's map all the conditions to BTE\n",
    "# NOTE: we are attempting to match to BTE terms, but many of the conditions terms are not standardized and are misspelled\n",
    "# First map as many terms as possible to BTE. Whatever is left over, will use other ontologies to create CURIEs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommedation to first format the output as close to sample as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_final = pd.DataFrame(columns=['subject','predicate','object','relation','subject_name','object_name','category'])\n",
    "# no_col_names_df.columns = col_names_df.columns\n",
    "# chosen predicate = \"related_to\", bc \"associated_with\" implies a statistical relationship \n",
    "ct_final['subject'] = 'condition:' + ct_extract['con_downcase_name'].astype(str)\n",
    "ct_final['predicate'] = 'biolink:related_to biolink:clinically_demonstrated_to biolink: hypothesized_to'\n",
    "ct_final['object'] = 'intervention:' + ct_extract['int_name'].astype(str)   # this will not all be RxNorm CURIEs since some interventions are not drugs\n",
    "ct_final['relation'] = 'RO:????'\n",
    "ct_final.subject_name = ct_extract.con_downcase_name\n",
    "ct_final.object_name = ct_extract.int_name\n",
    "ct_final.category = 'biolink:Association'\n",
    "ct_final['nctid'] = ct_extract['nctid']\n",
    "ct_final['nctid_curie'] = ct_extract['nctid_curie']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>relation</th>\n",
       "      <th>subject_name</th>\n",
       "      <th>object_name</th>\n",
       "      <th>category</th>\n",
       "      <th>nctid</th>\n",
       "      <th>nctid_curie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>condition:autism spectrum disorder</td>\n",
       "      <td>biolink:related_to biolink:clinically_demonstr...</td>\n",
       "      <td>intervention:Early Start Denver Model</td>\n",
       "      <td>RO:????</td>\n",
       "      <td>autism spectrum disorder</td>\n",
       "      <td>Early Start Denver Model</td>\n",
       "      <td>biolink:Association</td>\n",
       "      <td>NCT03254264</td>\n",
       "      <td>clinicaltrials:NCT03254264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              subject  \\\n",
       "0  condition:autism spectrum disorder   \n",
       "\n",
       "                                           predicate  \\\n",
       "0  biolink:related_to biolink:clinically_demonstr...   \n",
       "\n",
       "                                  object relation              subject_name  \\\n",
       "0  intervention:Early Start Denver Model  RO:????  autism spectrum disorder   \n",
       "\n",
       "                object_name             category        nctid  \\\n",
       "0  Early Start Denver Model  biolink:Association  NCT03254264   \n",
       "\n",
       "                  nctid_curie  \n",
       "0  clinicaltrials:NCT03254264  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_final[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interventions will have to be mapped to more than 1 vocabulary unlike conditions, which seems more easily mapped to SNOMED\n",
    "# first look at the types of interventions (unique on int_type in ct_extract dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding Folder_2/subfolder to the system path\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "  \n",
    "from pymetamap import MetaMap\n",
    "\n",
    "# NOTE: YOU MUST START the SKR/Medpost Part-of-Speech Tagger Server and the Word Sense Disambiguation (WSD) Server IN TERMINAL\n",
    "\n",
    "# % ./bin/skrmedpostctl start\n",
    "# % ./bin/wsdserverctl start\n",
    "\n",
    "\n",
    "# stop these servers afterwards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pausing\n",
    "from time import sleep\n",
    "\n",
    "# Setup UMLS Server\n",
    "metamap_base_dir = '/gwshare/umls_2021/metamap/public_mm/'\n",
    "metamap_bin_dir = 'bin/metamap20'\n",
    "metamap_pos_server_dir = 'bin/skrmedpostctl'\n",
    "metamap_wsd_server_dir = 'bin/wsdserverctl'\n",
    "\n",
    "# Start servers\n",
    "os.system(metamap_base_dir + metamap_pos_server_dir + ' start') # Part of speech tagger\n",
    "os.system(metamap_base_dir + metamap_wsd_server_dir + ' start') # Word sense disambiguation \n",
    "\n",
    "# Sleep a bit to give time for these servers to start up\n",
    "sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python\n",
      "../../../../../../../../Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin\n"
     ]
    }
   ],
   "source": [
    "# get the directory of the the module\n",
    "print(os.getcwd())\n",
    "# Path\n",
    "path = \"/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin\"  \n",
    "# Path of Start directory\n",
    "start = os.getcwd()\n",
    "print(os.path.relpath(\"/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin\", os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pausing\n",
    "from time import sleep\n",
    "\n",
    "# Setup UMLS Server\n",
    "metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm'\n",
    "# metamap_bin_dir = 'bin/metamap18'\n",
    "metamap_pos_server_dir = 'bin/skrmedpostctl'\n",
    "metamap_wsd_server_dir = 'bin/wsdserverctl'\n",
    "\n",
    "\n",
    "# Start servers\n",
    "os.system(metamap_base_dir + metamap_pos_server_dir + ' start') # Part of speech tagger\n",
    "os.system(metamap_base_dir + metamap_wsd_server_dir + ' start') # Word sense disambiguation \n",
    "\n",
    "# Sleep a bit to give time for these servers to start up\n",
    "sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = MetaMap.get_instance('/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/bin/metamap18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['myocardial infarction']  # this has to be 1 sentence\n",
    "concepts,error = mm.extract_concepts(sents)\n",
    "for concept in concepts:\n",
    "    print(concept)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check char encoding, is ascii??\n",
    "    \n",
    "for ct_string in ct_final['subject_name'][:5]:\n",
    "#     print(type(ct_string))\n",
    "    print(ct_string.isascii())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = list(set(list(ct_final['subject_name'])))\n",
    "interventions = list(filter(None, ct_final['object_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['osteopetrosis', 'urinary incontinence']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diseases[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ascii_char_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2de826b1cc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"split-virion, non-adjuvanted H1N1 vaccine of 15 μg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_make_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascii_char_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ascii_char_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# remove non-ASCII char! MetaMap does not like non-ASCII...\n",
    "\n",
    "import re\n",
    "non_ascii = \"[^\\x00-\\x7F]\"\n",
    "pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "\n",
    "\n",
    "def custom_make_translation(text, translation):\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "\n",
    "text = \"split-virion, non-adjuvanted H1N1 vaccine of 15 μg\"\n",
    "translated = custom_make_translation(text, ascii_char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "non_ascii = \"[^\\x00-\\x7F]\"\n",
    "pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "\n",
    "def custom_make_translation(text):\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "\n",
    "text = \"split-virion, non-adjuvanted H1N1 vaccine of 15 μg\"\n",
    "translated = custom_make_translation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'split-virion, non-adjuvanted H1N1 vaccine of 15  g'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_translated = dict((i, custom_make_translation(i)) for i in diseases)\n",
    "interventions_translated = dict((i, custom_make_translation(i)) for i in interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diseases_translated = dict((i, custom_make_translation(i)) for i in diseases[:20])\n",
    "# interventions_translated = dict((i, custom_make_translation(i)) for i in interventions[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_metamap(input_term, source_restriction):\n",
    "#     concepts_dict = dict()\n",
    "#     try:\n",
    "#         concepts,error = mm.extract_concepts(sentences = [input_term], restrict_to_sources = source_restriction)\n",
    "#         concepts_dict[input_term] = concepts\n",
    "#     except:\n",
    "#         concepts_dict[input_term] = None \n",
    "#     return(concepts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metamap_orig(input_term):\n",
    "    concepts_dict = dict()\n",
    "    try:\n",
    "        concepts,error = mm.extract_concepts(sentences = [input_term])\n",
    "        concepts_dict[input_term] = concepts\n",
    "    except:\n",
    "        concepts_dict[input_term] = None \n",
    "    return(concepts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_metamap(input_term, source_restriction):\n",
    "    concepts_dict = dict()\n",
    "    if all(x is None for x in source_restriction):\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 word_sense_disambiguation = True,\n",
    "                                                 prune = 10,\n",
    "                                                 composite_phrase = 1)\n",
    "            concepts_dict[input_term] = concepts\n",
    "        except:\n",
    "            concepts_dict[input_term] = None \n",
    "    else:\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 word_sense_disambiguation = True,\n",
    "                                                 restrict_to_sources=source_restriction,\n",
    "                                                 prune = 10,\n",
    "                                                 composite_phrase = 1)\n",
    "            concepts_dict[input_term] = concepts\n",
    "        except:\n",
    "            concepts_dict[input_term] = None \n",
    "    return(concepts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()  \n",
    "\n",
    "# pool = multiprocessing.Pool(num_workers)\n",
    "# for task in tasks:\n",
    "#     pool.apply_async(func, args = (task,))\n",
    "\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_processes = 4\n",
    "# metamapped_diseases = multiprocessing.Pool(number_of_processes).starmap(run_metamap,\n",
    "#                                                                         zip(list(diseases_translated.values()), [['SNOMEDCT_US', 'SNOMEDCT_VET', 'ICD10CM', 'ICD10CM', 'ICD10PCS', 'ICD9CM', 'ICD9CM']]*len(list(diseases_translated.values()))))\n",
    "# metamapped_interventions = multiprocessing.Pool(number_of_processes).starmap(run_metamap, \n",
    "#                                                                              zip(list(interventions_translated.values()),\n",
    "#                                                                                  [[None]]*len(list(interventions_translated.values()))))\n",
    "                                                                             \n",
    "# Stop MetaMap servers\n",
    "os.system(metamap_base_dir + metamap_pos_server_dir + ' stop') # Part of speech tagger\n",
    "os.system(metamap_base_dir + metamap_wsd_server_dir + ' stop') # Word sense disambiguation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'infertile high-risk patients with polycystic ovaries': [ConceptMMI(index='USER', mm='MMI', score='22.36', preferred_name='Polycystic Ovary Syndrome', cui='C0032460', semtypes='[dsyn]', trigger='[\"Polycystic ovary syndrome\"-tx-1-\"polycystic ovaries\"-noun-0]', location='TX', pos_info='38/18', tree_codes='C04.182.612.765;C13.351.500.056.630.580.765;C19.391.630.580.765'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='12.88', preferred_name='Infertility', cui='C0021359', semtypes='[patf]', trigger='[\"Infertility\"-tx-1-\"infertile\"-adj-0]', location='TX', pos_info='4/9', tree_codes='C12.294.365;C13.351.500.365'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='6.70', preferred_name='Patients', cui='C0030705', semtypes='[podg]', trigger='[\"Patient\"-tx-1-\"patients\"-noun-0]', location='TX', pos_info='24/8', tree_codes='M01.643'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.57', preferred_name='High risk', cui='C4319571', semtypes='[fndg]', trigger='[\"High risk\"-tx-1-\"high-risk\"-noun-0]', location='TX', pos_info='14/9', tree_codes=''),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.57', preferred_name='High risk of', cui='C0332167', semtypes='[fndg]', trigger='[\"High risk of\"-tx-1-\"high-risk\"-noun-0]', location='TX', pos_info='14/9', tree_codes='')]},\n",
       " {'anaesthetic induction': [ConceptMMI(index='USER', mm='MMI', score='19.25', preferred_name='Anesthetics', cui='C0002932', semtypes='[phsu]', trigger='[\"Anesthetic, NOS\"-tx-1-\"anaesthetic\"-adj-0]', location='TX', pos_info='4/11', tree_codes='D27.505.696.277.100;D27.505.954.427.210.100'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.63', preferred_name='Induce (action)', cui='C0205263', semtypes='[ftcn]', trigger='[\"Induced\"-tx-1-\"induction\"-noun-0]', location='TX', pos_info='16/9', tree_codes='')]}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamapped_diseases[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metamapped_diseases.txt', 'rb') as f:\n",
    "    metamapped_diseases = pickle.load(f)\n",
    "    \n",
    "with open('metamapped_interventions.txt', 'rb') as f:\n",
    "    metamapped_interventions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-222abf3c81d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# dataframe = spark.createDataFrame(data, col)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdiseases_df_mapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "# print(metamapped_diseases[:2])\n",
    "# print(\"\\n\")\n",
    "# print({kv[0]:kv[1] for i, kv in enumerate(diseases_translated.items()) if i <= 4})\n",
    "\n",
    "# import org.apache.spark.sql.types.{StringType, StructField, StructType}\n",
    "\n",
    "schema = StructType(\n",
    "    Array(\n",
    "        StructField(\"original_ct_value\", StringType,true),\n",
    "        StructField(\"metamap_input\", StringType,true),\n",
    "        StructField(\"metamap_output_preferred_name\", StringType,true),\n",
    "        StructField(\"cui\", StringType,true),\n",
    "        StructField(\"score\", StringType,true),\n",
    "        StructField(\"semtypes\", StringType,true),\n",
    "        \n",
    "))\n",
    "\n",
    "col = ['original_ct_value', 'metamap_input', 'metamap_output_preferred_name', 'cui', 'score', 'semtypes' ]\n",
    "\n",
    "# dataframe = spark.createDataFrame(data, col)\n",
    "\n",
    "diseases_df_mapped = spark.createDataFrame(col)\n",
    "\n",
    "\n",
    "for disease in metamapped_diseases[:10]:\n",
    "        for key in disease:\n",
    "            for concept in disease[key]:\n",
    "                concept = concept._asdict()\n",
    "    #             concept.get('preferred_name')\n",
    "                row = [key]\n",
    "                original_disease = (list(diseases_translated.keys())[list(diseases_translated.values()).index(key)])\n",
    "                row.append(original_disease)\n",
    "                row.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "                print(row)\n",
    "                diseases_df_mapped = diseases_df_mapped.union(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the mapping results to text files for evaluation \n",
    "from collections import namedtuple\n",
    "\n",
    "with open(\"metamapped_diseases_qual_control.csv\", \"w\") as file:\n",
    "    csv.excel.delimiter=','\n",
    "\n",
    "    writer = csv.writer(file, dialect=csv.excel)\n",
    "    writer.writerow(['original_ct_value', 'metamap_input', 'metamap_input', 'metamap_cui', 'metamap_score', 'metamap_semtypes'])\n",
    "    for disease in metamapped_diseases[:100]:\n",
    "        for key in disease:\n",
    "            for concept in disease[key]:\n",
    "                concept = concept._asdict()\n",
    "                row = [key]\n",
    "                original_disease = (list(diseases_translated.keys())[list(diseases_translated.values()).index(key)])\n",
    "                row.append(original_disease)\n",
    "                row.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "                writer.writerow(row)\n",
    "\n",
    "# with open(\"metamapped_interventions_qual_control.csv\", \"w\") as file:\n",
    "#     csv.excel.delimiter=','\n",
    "#     writer = csv.writer(file, dialect=csv.excel)\n",
    "#     writer.writerow(['input_intervention', 'metamapped_intervention_name', 'metamap_cui', 'metamap_score', 'metamap_semtypes'])\n",
    "#     for intervention in metamapped_interventions:\n",
    "#         for key in intervention:\n",
    "#             for concept in intervention[key]:\n",
    "#                 concept = concept._asdict()\n",
    "#     #             concept.get('preferred_name')\n",
    "#                 row = [key]\n",
    "#                 row.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "#                 writer.writerow(row)\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.0.119 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-139-b4ba8bab07b8>\", line 10, in <lambda>\nValueError: invalid literal for int() with base 10: 'original_ct_value'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-139-b4ba8bab07b8>\", line 10, in <lambda>\nValueError: invalid literal for int() with base 10: 'original_ct_value'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-b4ba8bab07b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                         \u001b[0mmetamap_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                         metamap_semtypes=[4]))\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mct_data_struct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#convert the RDD to DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.0.119 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-139-b4ba8bab07b8>\", line 10, in <lambda>\nValueError: invalid literal for int() with base 10: 'original_ct_value'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1562, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-139-b4ba8bab07b8>\", line 10, in <lambda>\nValueError: invalid literal for int() with base 10: 'original_ct_value'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# spark - sparkSession\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "ct_data = sc.textFile(\"metamapped_diseases_qual_control.csv\")\n",
    "#Split on delimiters\n",
    "ct_items = ct_data.map(lambda l: l.split(\",\"))\n",
    "#Convert to Row\n",
    "ct_data_struct = ct_items.map(lambda p: Row(original_ct_value=int(p[0]),\n",
    "                                        metamap_input=p[1],\n",
    "                                        metamap_cui=p[2],\n",
    "                                        metamap_score=p[3],\n",
    "                                        metamap_semtypes=[4]))\n",
    "for i in ct_data_struct.take(5): print(i)\n",
    "#convert the RDD to DataFrame\n",
    "\n",
    "ct_df = spark.createDataFrame(ct_data_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-24e6fc2c992e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m schema = StructType(\n\u001b[0;32m----> 5\u001b[0;31m     array(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"original_ct_value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metamap_input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "schema = StructType(\n",
    "    Array(\n",
    "        StructField(\"original_ct_value\", StringType,true),\n",
    "        StructField(\"metamap_input\", StringType,true),\n",
    "        StructField(\"metamap_output_preferred_name\", StringType,true),\n",
    "        StructField(\"cui\", StringType,true),\n",
    "        StructField(\"score\", StringType,true),\n",
    "        StructField(\"semtypes\", StringType,true),\n",
    "        \n",
    "))\n",
    "\n",
    "test = spark.read.csv(\n",
    "    \"metamapped_diseases_qual_control.csv\", \n",
    "    header=True, \n",
    "    mode=\"DROPMALFORMED\", \n",
    "    schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-f6bb7c0d74e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load a text file and convert each line to a Row.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0morders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/practicedata/orders\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#Split on delimiters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mtextFile\u001b[0;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m'Hello world!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \"\"\"\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         return RDD(self._jsc.textFile(name, minPartitions), self,\n\u001b[1;32m    656\u001b[0m                    UTF8Deserializer(use_unicode))\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m         reduce tasks)\n\u001b[1;32m    449\u001b[0m         \"\"\"\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# spark - sparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "orders = sc.textFile(\"/practicedata/orders\")\n",
    "#Split on delimiters\n",
    "parts = orders.map(lambda l: l.split(\",\"))\n",
    "#Convert to Row\n",
    "orders_struct = parts.map(lambda p: Row(order_id=int(p[0]), order_date=p[1], customer_id=p[2], order_status=p[3]))\n",
    "for i in orders_struct.take(5): print(i)\n",
    "#convert the RDD to DataFrame\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\") \\\n",
    "                    .appName('clinical_trials_etl') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# # test = spark.createDataFrame(Row(**x) for x in metamapped_diseases[:20]).show(truncate=False)\n",
    "# for disease in metamapped_diseases:\n",
    "#         for key in disease:\n",
    "#             for concept in disease[key]:\n",
    "#                 concept = concept._asdict()\n",
    "#                 row = [key]\n",
    "#                 row.extend([concept.get(k) for k in ['preferred_name', 'cui', 'score', 'semtypes']])\n",
    "#                 writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'infertile high-risk patients with polycystic ovaries': [ConceptMMI(index='USER', mm='MMI', score='22.36', preferred_name='Polycystic Ovary Syndrome', cui='C0032460', semtypes='[dsyn]', trigger='[\"Polycystic ovary syndrome\"-tx-1-\"polycystic ovaries\"-noun-0]', location='TX', pos_info='38/18', tree_codes='C04.182.612.765;C13.351.500.056.630.580.765;C19.391.630.580.765'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='12.88', preferred_name='Infertility', cui='C0021359', semtypes='[patf]', trigger='[\"Infertility\"-tx-1-\"infertile\"-adj-0]', location='TX', pos_info='4/9', tree_codes='C12.294.365;C13.351.500.365'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='6.70', preferred_name='Patients', cui='C0030705', semtypes='[podg]', trigger='[\"Patient\"-tx-1-\"patients\"-noun-0]', location='TX', pos_info='24/8', tree_codes='M01.643'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.57', preferred_name='High risk', cui='C4319571', semtypes='[fndg]', trigger='[\"High risk\"-tx-1-\"high-risk\"-noun-0]', location='TX', pos_info='14/9', tree_codes=''),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.57', preferred_name='High risk of', cui='C0332167', semtypes='[fndg]', trigger='[\"High risk of\"-tx-1-\"high-risk\"-noun-0]', location='TX', pos_info='14/9', tree_codes='')]},\n",
       " {'anaesthetic induction': [ConceptMMI(index='USER', mm='MMI', score='19.25', preferred_name='Anesthetics', cui='C0002932', semtypes='[phsu]', trigger='[\"Anesthetic, NOS\"-tx-1-\"anaesthetic\"-adj-0]', location='TX', pos_info='4/11', tree_codes='D27.505.696.277.100;D27.505.954.427.210.100'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.63', preferred_name='Induce (action)', cui='C0205263', semtypes='[ftcn]', trigger='[\"Induced\"-tx-1-\"induction\"-noun-0]', location='TX', pos_info='16/9', tree_codes='')]}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamapped_diseases[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'infertile high-risk patients with polycystic ovaries': [ConceptMMI(index='USER', mm='MMI', score='22.36', preferred_name='Polycystic Ovary Syndrome', cui='C0032460', semtypes='[dsyn]', trigger='[\"Polycystic ovary syndrome\"-tx-1-\"polycystic ovaries\"-noun-0]', location='TX', pos_info='38/18', tree_codes='C04.182.612.765;C13.351.500.056.630.580.765;C19.391.630.580.765'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='12.88', preferred_name='Infertility', cui='C0021359', semtypes='[patf]', trigger='[\"Infertility\"-tx-1-\"infertile\"-adj-0]', location='TX', pos_info='4/9', tree_codes='C12.294.365;C13.351.500.365'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='6.70', preferred_name='Patients', cui='C0030705', semtypes='[podg]', trigger='[\"Patient\"-tx-1-\"patients\"-noun-0]', location='TX', pos_info='24/8', tree_codes='M01.643'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.57', preferred_name='High risk', cui='C4319571', semtypes='[fndg]', trigger='[\"High risk\"-tx-1-\"high-risk\"-noun-0]', location='TX', pos_info='14/9', tree_codes=''),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.57', preferred_name='High risk of', cui='C0332167', semtypes='[fndg]', trigger='[\"High risk of\"-tx-1-\"high-risk\"-noun-0]', location='TX', pos_info='14/9', tree_codes='')]},\n",
       " {'anaesthetic induction': [ConceptMMI(index='USER', mm='MMI', score='19.25', preferred_name='Anesthetics', cui='C0002932', semtypes='[phsu]', trigger='[\"Anesthetic, NOS\"-tx-1-\"anaesthetic\"-adj-0]', location='TX', pos_info='4/11', tree_codes='D27.505.696.277.100;D27.505.954.427.210.100'),\n",
       "   ConceptMMI(index='USER', mm='MMI', score='3.63', preferred_name='Induce (action)', cui='C0205263', semtypes='[ftcn]', trigger='[\"Induced\"-tx-1-\"induction\"-noun-0]', location='TX', pos_info='16/9', tree_codes='')]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "spark.createDataFrame(Row(**x) for x in mylist).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pip show pyspark\n",
    "\n",
    "\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"test\").config(\n",
    "#     \"spark.driver.extraJavaOptions\",\n",
    "#     \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED\",\n",
    "# ).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link nct ids of studies that have the same disease and intervention pair---multiple studies may have studies the same intervention\n",
    "# restrict returned CUIs of metamapped concepts???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.environ['USER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kamileh'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'java version \"1.8.0_251\"\\nJava(TM) SE Runtime Environment (build 1.8.0_251-b08)\\nJava HotSpot(TM) 64-Bit Server VM (build 25.251-b08, mixed mode)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "version = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n",
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct_extract_env",
   "language": "python",
   "name": "ct_extract_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
