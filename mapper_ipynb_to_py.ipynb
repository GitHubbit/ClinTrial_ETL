{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19995ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799e1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import concurrent\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import config\n",
    "import configparser\n",
    "\n",
    "import urllib\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f68c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "global sublist_length \n",
    "sublist_length = 40 # Name Resolver takes batches of 1000\n",
    "\n",
    "global CAS_SERVERURL \n",
    "global II_SKR_SERVERURL \n",
    "global METAMAP_INTERACTIVE_URL \n",
    "global stserverurl \n",
    "global tgtserverurl\n",
    "global apikey \n",
    "global serviceurl \n",
    "global ksource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83cc5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAS_SERVERURL = \"https://utslogin.nlm.nih.gov/cas/v1\"\n",
    "II_SKR_SERVERURL = 'https://ii.nlm.nih.gov/cgi-bin/II/UTS_Required'\n",
    "METAMAP_INTERACTIVE_URL = II_SKR_SERVERURL + \"/API_MM_interactive.pl\"\n",
    "stserverurl = \"https://utslogin.nlm.nih.gov/cas/v1/tickets\"\n",
    "tgtserverurl = \"https://utslogin.nlm.nih.gov/cas/v1/api-key\"\n",
    "serviceurl = METAMAP_INTERACTIVE_URL\n",
    "ksource = '2020AB'\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.read('config.ini')\n",
    "apikey = cfg['METAMAP']['apikey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65461dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kamileh/opt/anaconda3/lib/python3.7/site-packages/thefuzz/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# %pip install thefuzz\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51382ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_response(chunk):\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    nr_terms = []\n",
    "    for term in chunk:\n",
    "        nr_term = {}\n",
    "        params = {'string':term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give            \n",
    "        r = requests.post(nr_url, params=params)\n",
    "        try:\n",
    "            res = r.json()\n",
    "            if res:\n",
    "                for key, val in res.items():\n",
    "                    nr_term[term] = [key, val[0]]\n",
    "                    nr_terms.append(nr_term)\n",
    "            else:\n",
    "#                 print(term + \" unable to be mapped by Name Resolver\")\n",
    "                pass\n",
    "        except Exception as e:\n",
    "#             print(e)\n",
    "#             print(term + \" unable to be mapped by Name Resolver\")\n",
    "            pass      \n",
    "    time.sleep(5)\n",
    "    return nr_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1351bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_threads_nr(unmapped_chunked):\n",
    "    # multithread implementation for retrieving Name Resolver responses\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with concurrent.futures.ThreadPoolExecutor(multiprocessing.cpu_count() - 1) as executor:\n",
    "        # Submit the get_response() function for each item in the list\n",
    "        futures = [executor.submit(get_nr_response, chunk) for chunk in unmapped_chunked]\n",
    "        # Retrieve the results as they become available\n",
    "        output = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf704ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, sublist_length):\n",
    "    return [lst[i:i+sublist_length] for i in range(0, len(lst), sublist_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711bd7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "def get_token_sort_ratio(str1, str2):\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "\n",
    "def get_token_set_ratio(str1, str2):\n",
    "    try:\n",
    "        return fuzz.token_set_ratio(str1, str2)\n",
    "    except:\n",
    "        return None  \n",
    "set_ratio = np.vectorize(get_token_set_ratio)\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "sim_score = np.vectorize(get_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f0bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    # get all the links and associated dates of upload into a dict called date_link\n",
    "    url_all = \"https://aact.ctti-clinicaltrials.org/pipe_files\"\n",
    "    response = requests.get(url_all)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    body = soup.find_all('option') #Find all\n",
    "    date_link = {}\n",
    "    for el in body:\n",
    "        tags = el.find('a')\n",
    "        try:\n",
    "            zip_name = tags.contents[0].split()[0]\n",
    "            date = zip_name.split(\"_\")[0]\n",
    "            date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "            date_link[date] = tags.get('href')\n",
    "        except:\n",
    "            pass\n",
    "    latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "    url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "    date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "    data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "    data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "    data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    \n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "        \n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Downloading Clinical Trial data as of {}\".format(date_string))\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(data_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(\"Finished download of zip\")\n",
    "            with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                print(\"Unzipping data\")\n",
    "                download.extractall(data_extracted)\n",
    "        else:\n",
    "            print(\"KG is already up to date.\")\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88959eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "#         exit()\n",
    "#         pass\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "        browse_conditions_df = pd.read_csv(data_extracted + '/browse_conditions.txt', sep='|', index_col=False, header=0)\n",
    "        browse_interventions_df = pd.read_csv(data_extracted + '/browse_interventions.txt', sep='|', index_col=False, header=0)\n",
    "        \n",
    "    ### GET RID OF....CHEAT LINE FOR TESTING\n",
    "        conditions_df = conditions_df.iloc[:200]\n",
    "        interventions_df = interventions_df.iloc[:200]\n",
    "\n",
    "    return {\"conditions\": conditions_df, \"interventions\": interventions_df, \"browse_conditions\": browse_conditions_df, \"browse_interventions\": browse_interventions_df}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b993add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_mesh(df_dict):\n",
    "    \n",
    "    # -------    CONDITIONS    ------- #\n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    browse_conditions = df_dict[\"browse_conditions\"] \n",
    "\n",
    "    tomap_conditions = conditions[\"downcase_name\"].values.tolist()\n",
    "    tomap_conditions = list(set(tomap_conditions))\n",
    "    print(\"Number of unique conditions in this Clinical Trials data dump: {}\".format(len(tomap_conditions)))\n",
    "    mesh_exact_mapped = list(set(tomap_conditions).intersection(browse_conditions.downcase_mesh_term.unique()))\n",
    "    print(\"Number of unique conditions that have an exact MeSH term match given in this dump: {}\".format(len(mesh_exact_mapped)))\n",
    "\n",
    "    print(\"since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_exact_mapped_chunked = split_list(mesh_exact_mapped, sublist_length)\n",
    "    mesh_exact_mapped_curied = run_parallel_threads_nr(mesh_exact_mapped_chunked)\n",
    "\n",
    "    mesh_exact_mapped_curied = [element for sublist in mesh_exact_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    mesh_exact_mapped_curied = {key: value for dictionary in mesh_exact_mapped_curied for key, value in dictionary.items()}\n",
    "\n",
    "    mapped_conditions = pd.DataFrame({\"condition_input\": list(mesh_exact_mapped_curied.keys()), # get dataframe of exact MeSH mapped conditions\n",
    "                                      \"condition_CURIE_id\": [value[0] for value in mesh_exact_mapped_curied.values()],\n",
    "                                      \"condition_CURIE_name\": [value[-1] for value in mesh_exact_mapped_curied.values()],\n",
    "                                      \"source\": \"MeSH term exact mapped, Name Resolver CURIE\"})\n",
    "    unmapped_conditions = list(set(tomap_conditions)-set(mapped_conditions.condition_input))\n",
    "    print(\"Number of unique conditions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_conditions)))\n",
    "\n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    browse_interventions = df_dict[\"browse_interventions\"] \n",
    "\n",
    "    tomap_interventions = interventions[\"name\"].values.tolist()\n",
    "    tomap_interventions = reduce(lambda a, b: a+[str(b)], tomap_interventions, [])\n",
    "    tomap_interventions = [string.lower() for string in tomap_interventions] # lowercase the strings\n",
    "    tomap_interventions = list(set(tomap_interventions))\n",
    "    print(\"Number of unique interventions in this Clinical Trials data dump: {}\".format(len(tomap_interventions)))\n",
    "    mesh_exact_mapped = list(set(tomap_interventions).intersection(browse_interventions.downcase_mesh_term.unique()))\n",
    "    print(\"Number of unique interventions that have an exact MeSH term match given in this dump: {}\".format(len(mesh_exact_mapped)))\n",
    "\n",
    "    print(\"since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_exact_mapped_chunked = split_list(mesh_exact_mapped, sublist_length)\n",
    "    mesh_exact_mapped_curied = run_parallel_threads_nr(mesh_exact_mapped_chunked)\n",
    "    mesh_exact_mapped_curied = [element for sublist in mesh_exact_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    mesh_exact_mapped_curied = {key: value for dictionary in mesh_exact_mapped_curied for key, value in dictionary.items()}\n",
    "    mapped_interventions = pd.DataFrame({\"intervention_input\": list(mesh_exact_mapped_curied.keys()),    # get dataframe of exact MeSH mapped interventions\n",
    "                                         \"intervention_CURIE_id\": [value[0] for value in mesh_exact_mapped_curied.values()],\n",
    "                                         \"intervention_CURIE_name\": [value[-1] for value in mesh_exact_mapped_curied.values()],\n",
    "                                         \"source\": \"MeSH term exact mapped, Name Resolver CURIE\"})\n",
    "\n",
    "    unmapped_interventions = list(set(tomap_interventions)-set(mapped_interventions.intervention_input))\n",
    "    print(\"Number of unique interventions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_interventions)))\n",
    "\n",
    "    ct_terms = {'mapped_conditions': mapped_conditions, 'unmapped_conditions': unmapped_conditions, 'mapped_interventions': mapped_interventions, 'unmapped_interventions': unmapped_interventions}\n",
    "    return ct_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b0bade17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inexact_match_mesh(df_dict, ct_terms):\n",
    "    \n",
    "    # get dataframes bc I'm going to compute fuzzy scores and dump into columns\n",
    "    # find unmapped terms AND THEIR CORRESPONDING NCITS!\n",
    "    # get the conditions that have exact MESH term matches, and conditions that don't have exact MESH term matches. We want to filter for rows that don't have exact MESH term matches bc we already captured those and don't want to run scoring on it\n",
    "\n",
    "    # -------    CONDITIONS    ------- #\n",
    "\n",
    "    print(\"Use fuzzy matching on MeSH terms from Clinical Trials dump to find more potential matches.\")\n",
    "    conditions = df_dict[\"conditions\"] \n",
    "    conditions = conditions[[\"nct_id\", \"downcase_name\"]]\n",
    "\n",
    "    browse_conditions = df_dict[\"browse_conditions\"] \n",
    "    all_mesh_conditions = browse_conditions.downcase_mesh_term.unique()\n",
    "    mask = np.isin(conditions['downcase_name'], all_mesh_conditions)\n",
    "    conditions['mesh_conditions_exact_mapped'] = np.where(mask, conditions['downcase_name'], np.nan)\n",
    "    conditions_unmapped = conditions[conditions['mesh_conditions_exact_mapped'].isnull()] # get the rows where mesh_term is empty bc there was no match there, we will run fuzzy scoring on these rows (I'm getting unmapped conditions along with their NCT IDs, not just the condition, bc I need this to find possible MeSH terms by study)\n",
    "    conditions_unmapped = conditions_unmapped.drop('mesh_conditions_exact_mapped', axis=1) # drop the empty column now\n",
    "\n",
    "    mesh_conditions_per_study = pd.DataFrame(browse_conditions[[\"nct_id\", \"downcase_mesh_term\", \"mesh_type\"]].groupby(\"nct_id\")[\"downcase_mesh_term\"].apply(list)) # get all MeSH terms available for each study\n",
    "\n",
    "    conditions_unmapped_all_mesh_terms = pd.merge(conditions_unmapped, \n",
    "                                                  mesh_conditions_per_study,\n",
    "                                                  how='left',\n",
    "                                                  left_on=['nct_id'],\n",
    "                                                  right_on = ['nct_id'])\n",
    "\n",
    "    # some clinical trials are missing from browse_conditions (those nct_ids are not present in the browse_conditions text) They have NaN in the downcase_mesh_term column\n",
    "    conditions_unmapped_all_mesh_terms = conditions_unmapped_all_mesh_terms[~conditions_unmapped_all_mesh_terms['downcase_mesh_term'].isnull()] # subset or delete rows where either column is empty/Nonetype bc fuzzymatching functions will throw error if handling\n",
    "    conditions_unmapped_all_mesh_terms = conditions_unmapped_all_mesh_terms.explode('downcase_mesh_term')\n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    conditions_unmapped_all_mesh_terms[\"sort_ratio\"] = sort_ratio(conditions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, conditions_unmapped_all_mesh_terms[[\"downcase_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    conditions_unmapped_all_mesh_terms[\"sim_score\"] = sim_score(conditions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, conditions_unmapped_all_mesh_terms[[\"downcase_name\"]].values)\n",
    "    conditions_mesh_fuzz_scored = conditions_unmapped_all_mesh_terms[(conditions_unmapped_all_mesh_terms['sim_score'] > 88) | (conditions_unmapped_all_mesh_terms['sort_ratio'] > 88)]\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(by = ['nct_id', 'downcase_name'], ascending = [True, True], na_position = 'first')\n",
    "\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(by = ['sim_score', 'sort_ratio'], ascending=[False,False], na_position='last').drop_duplicates(['nct_id', 'downcase_name']).sort_index() # there may be many mesh terms that passed the ratio and score filter; this causes duplicates bc I exploded the df...this line of code picks one and throws away other potential matches for one disease-nct_id pair\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(['nct_id'], ascending=False)\n",
    "\n",
    "    keys = list(conditions_mesh_fuzz_scored[[\"nct_id\", \"downcase_name\"]].columns.values)\n",
    "    i1 = conditions_unmapped.set_index(keys).index\n",
    "    i2 = conditions_mesh_fuzz_scored.set_index(keys).index\n",
    "\n",
    "    tomap_conditions = conditions_mesh_fuzz_scored[\"downcase_mesh_term\"].values.tolist()\n",
    "    tomap_conditions = list(set(tomap_conditions))\n",
    "    \n",
    "    print(\"Since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_fuzz_mapped_chunked = split_list(tomap_conditions, sublist_length)\n",
    "    mesh_fuzz_mapped_curied = run_parallel_threads_nr(mesh_fuzz_mapped_chunked)\n",
    "    mesh_fuzz_mapped_curied = [element for sublist in mesh_fuzz_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique conditions mapped using fuzzy matching to MeSH terms: {}\".format(len(mesh_fuzz_mapped_curied)))\n",
    "    mesh_fuzz_mapped_curied = {key: value for dictionary in mesh_fuzz_mapped_curied for key, value in dictionary.items()}\n",
    "    fuzz_mapped_conditions = pd.DataFrame({\"condition_input\": list(mesh_fuzz_mapped_curied.keys()),\n",
    "                                           \"condition_CURIE_id\": [value[0] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                           \"condition_CURIE_name\": [value[-1] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                           \"source\": \"MeSH term fuzzy mapped, Name Resolver CURIE\"})\n",
    "    previously_mapped = ct_terms[\"mapped_conditions\"]\n",
    "    combined_mapped_conditions = pd.concat([previously_mapped, fuzz_mapped_conditions], ignore_index=True) # get dataframe of combined previously mapped conditions and additional fuzzy MeSH mapped conditions\n",
    "\n",
    "    all_conditions_list = conditions[\"downcase_name\"].values.tolist()\n",
    "    all_conditions_list = list(set(all_conditions_list))\n",
    "    unmapped_conditions = list(set(all_conditions_list)-set(list(combined_mapped_conditions.condition_input.values)))\n",
    "    print(\"Number of unique conditions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_conditions)))\n",
    "\n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "\n",
    "    interventions = df_dict[\"interventions\"] \n",
    "    interventions['downcase_name'] = interventions['name'].str.lower()\n",
    "    interventions = interventions[[\"nct_id\", \"downcase_name\"]]\n",
    "    browse_interventions = df_dict[\"browse_interventions\"] \n",
    "    all_mesh_interventions = browse_interventions.downcase_mesh_term.unique()\n",
    "    mask = np.isin(interventions['downcase_name'], all_mesh_interventions)\n",
    "    interventions['mesh_interventions_exact_mapped'] = np.where(mask, interventions['downcase_name'], np.nan)\n",
    "    interventions_unmapped = interventions[interventions['mesh_interventions_exact_mapped'].isnull()] # get the rows where mesh_term is empty bc there was no match there, we will run fuzzy scoring on these rows (I'm getting unmapped interventions along with their NCT IDs, not just the condition, bc I need this to find possible MeSH terms by study)\n",
    "    interventions_unmapped = interventions_unmapped.drop('mesh_interventions_exact_mapped', axis=1) # drop the empty column now\n",
    "\n",
    "    mesh_interventions_per_study = pd.DataFrame(browse_interventions[[\"nct_id\", \"downcase_mesh_term\", \"mesh_type\"]].groupby(\"nct_id\")[\"downcase_mesh_term\"].apply(list)) # get all MeSH terms available for each study\n",
    "\n",
    "    interventions_unmapped_all_mesh_terms = pd.merge(interventions_unmapped, \n",
    "                                                  mesh_interventions_per_study,\n",
    "                                                  how='left',\n",
    "                                                  left_on=['nct_id'],\n",
    "                                                  right_on = ['nct_id'])\n",
    "\n",
    "    # # some clinical trials are missing from browse_interventions (those nct_ids are not present in the browse_interventions text) They have NaN in the downcase_mesh_term column\n",
    "    interventions_unmapped_all_mesh_terms = interventions_unmapped_all_mesh_terms[~interventions_unmapped_all_mesh_terms['downcase_mesh_term'].isnull()] # subset or delete rows where either column is empty/Nonetype bc fuzzymatching functions will throw error if handling\n",
    "    interventions_unmapped_all_mesh_terms = interventions_unmapped_all_mesh_terms.explode('downcase_mesh_term')\n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    interventions_unmapped_all_mesh_terms[\"sort_ratio\"] = sort_ratio(interventions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, interventions_unmapped_all_mesh_terms[[\"downcase_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    interventions_unmapped_all_mesh_terms[\"sim_score\"] = sim_score(interventions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, interventions_unmapped_all_mesh_terms[[\"downcase_name\"]].values)\n",
    "    interventions_mesh_fuzz_scored = interventions_unmapped_all_mesh_terms[(interventions_unmapped_all_mesh_terms['sim_score'] > 88) | (interventions_unmapped_all_mesh_terms['sort_ratio'] > 88)]\n",
    "    interventions_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(by = ['nct_id', 'downcase_name'], ascending = [True, True], na_position = 'first')\n",
    "\n",
    "    interventions_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(by = ['sim_score', 'sort_ratio'], ascending=[False,False], na_position='last').drop_duplicates(['nct_id', 'downcase_name']).sort_index() # there may be many mesh terms that passed the ratio and score filter; this causes duplicates bc I exploded the df...this line of code picks one and throws away other potential matches for one disease-nct_id pair\n",
    "    interventions_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(['nct_id'], ascending=False)\n",
    "\n",
    "    keys = list(interventions_mesh_fuzz_scored[[\"nct_id\", \"downcase_name\"]].columns.values)\n",
    "    i1 = interventions_unmapped.set_index(keys).index\n",
    "    i2 = interventions_mesh_fuzz_scored.set_index(keys).index\n",
    "\n",
    "    tomap_interventions = interventions_mesh_fuzz_scored[\"downcase_mesh_term\"].values.tolist()\n",
    "    tomap_interventions = list(set(tomap_interventions))\n",
    "    \n",
    "    print(\"Since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_fuzz_mapped_chunked = split_list(tomap_interventions, sublist_length)\n",
    "    mesh_fuzz_mapped_curied = run_parallel_threads_nr(mesh_fuzz_mapped_chunked)\n",
    "    mesh_fuzz_mapped_curied = [element for sublist in mesh_fuzz_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique interventions mapped using fuzzy matching to MeSH terms: {}\".format(len(mesh_fuzz_mapped_curied)))\n",
    "    mesh_fuzz_mapped_curied = {key: value for dictionary in mesh_fuzz_mapped_curied for key, value in dictionary.items()}\n",
    "    fuzz_mapped_interventions = pd.DataFrame({\"intervention_input\": list(mesh_fuzz_mapped_curied.keys()),\n",
    "                                              \"intervention_CURIE_id\": [value[0] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                              \"intervention_CURIE_name\": [value[-1] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                              \"source\": \"MeSH term fuzzy mapped, Name Resolver CURIE\"})\n",
    "    previously_mapped = ct_terms[\"mapped_interventions\"]\n",
    "    combined_mapped_interventions = pd.concat([previously_mapped, fuzz_mapped_interventions], ignore_index=True) # get dataframe of combined previously mapped interventions and additional fuzzy MeSH mapped interventions\n",
    "    \n",
    "    all_interventions_list = interventions[\"downcase_name\"].values.tolist()\n",
    "    all_interventions_list = list(set(all_interventions_list))\n",
    "    unmapped_interventions = list(set(all_interventions_list)-set(list(combined_mapped_interventions.intervention_input.values)))\n",
    "    print(\"Number of unique interventions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_interventions)))\n",
    "\n",
    "    ct_terms = {'mapped_conditions': combined_mapped_conditions, 'unmapped_conditions': unmapped_conditions, 'mapped_interventions': combined_mapped_interventions, 'unmapped_interventions': unmapped_interventions}\n",
    "    return ct_terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3033a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_nr(df_dict, ct_terms):\n",
    "    \n",
    "    # -------    CONDITIONS    ------- #\n",
    "\n",
    "    unmapped_conditions = ct_terms[\"unmapped_conditions\"]\n",
    "    conditions_unmapped_chunked = split_list(unmapped_conditions, sublist_length)\n",
    "    nr_conditions = run_parallel_threads_nr(conditions_unmapped_chunked)\n",
    "    nr_conditions = [element for sublist in nr_conditions for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique conditions mapped using Name Resolver: {}\".format(len(nr_conditions)))\n",
    "    nr_conditions = {key: value for dictionary in nr_conditions for key, value in dictionary.items()}\n",
    "    nr_conditions_df = pd.DataFrame({\"condition_input\": list(nr_conditions.keys()),\n",
    "                                     \"condition_CURIE_id\": [value[0] for value in nr_conditions.values()],\n",
    "                                     \"condition_CURIE_name\": [value[-1] for value in nr_conditions.values()],\n",
    "                                     \"source\": \"Name Resolver, no further curation\"})\n",
    "    \n",
    "    previously_mapped = ct_terms[\"mapped_conditions\"]\n",
    "    combined_mapped_conditions = pd.concat([previously_mapped, nr_conditions_df], ignore_index=True) # get dataframe of combined previously mapped conditions and additional fuzzy MeSH mapped conditions\n",
    "    \n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    all_conditions_list = conditions[\"downcase_name\"].values.tolist()\n",
    "    all_conditions_list = list(set(all_conditions_list))\n",
    "    unmapped_conditions = list(set(all_conditions_list)-set(list(combined_mapped_conditions.condition_input.values)))\n",
    "    print(\"Number of unique conditions that are unmapped after using Name Resolver: {}\".format(len(unmapped_conditions)))\n",
    "    \n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    \n",
    "    unmapped_interventions = ct_terms[\"unmapped_interventions\"]\n",
    "    interventions_unmapped_chunked = split_list(unmapped_interventions, sublist_length)\n",
    "    nr_interventions = run_parallel_threads_nr(interventions_unmapped_chunked)\n",
    "    nr_interventions = [element for sublist in nr_interventions for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique interventions mapped using Name Resolver: {}\".format(len(nr_interventions)))\n",
    "    nr_interventions = {key: value for dictionary in nr_interventions for key, value in dictionary.items()}\n",
    "    nr_interventions_df = pd.DataFrame({\"intervention_input\": list(nr_interventions.keys()),\n",
    "                                     \"intervention_CURIE_id\": [value[0] for value in nr_interventions.values()],\n",
    "                                     \"intervention_CURIE_name\": [value[-1] for value in nr_interventions.values()],\n",
    "                                     \"source\": \"Name Resolver, no further curation\"})\n",
    "    \n",
    "    previously_mapped = ct_terms[\"mapped_interventions\"]\n",
    "    combined_mapped_interventions = pd.concat([previously_mapped, nr_interventions_df], ignore_index=True) # get dataframe of combined previously mapped interventions and additional fuzzy MeSH mapped interventions\n",
    "    \n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    all_interventions_list = interventions[\"downcase_name\"].values.tolist()\n",
    "    all_interventions_list = list(set(all_interventions_list))\n",
    "    unmapped_interventions = list(set(all_interventions_list)-set(list(combined_mapped_interventions.intervention_input.values)))\n",
    "    print(\"Number of unique interventions that are unmapped after using Name Resolver: {}\".format(len(unmapped_interventions)))\n",
    "    \n",
    "    ct_terms = {'mapped_conditions': combined_mapped_conditions, 'unmapped_conditions': unmapped_conditions, 'mapped_interventions': combined_mapped_interventions, 'unmapped_interventions': unmapped_interventions}\n",
    "    return ct_terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af57d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9e253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0974ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eefe80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ece122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3690c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ab726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f63f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2720be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56ff32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_ticket(serverurl, ticket_granting_ticket, serviceurl):\n",
    "    \"\"\" Obtain a Single-Use Proxy Ticket (also known as service ticket).\n",
    "    Request for a Service Ticket:\n",
    "        POST /cas/v1/tickets/{TGT id} HTTP/1.0\n",
    "    data:\n",
    "           service={form encoded parameter for the service url}\n",
    "    Sucessful Response:\n",
    "        200 OK\n",
    "        ST-1-FFDFHDSJKHSDFJKSDHFJKRUEYREWUIFSD2132\n",
    "    @param serverurl authentication server\n",
    "    @param ticketGrantingTicket a Proxy Granting Ticket.\n",
    "    @param serviceurl url of service with protected resources\n",
    "    @return authentication ticket for service. \"\"\"\n",
    "    resp = requests.post(\"{}/{}\".format(serverurl, ticket_granting_ticket),\n",
    "                         {\"service\": serviceurl})\n",
    "    if resp.status_code == 200:\n",
    "        return resp.content\n",
    "    return 'Error: status: {}'.format(resp.content)\n",
    "\n",
    "\n",
    "def extract_tgt_ticket(htmlcontent):\n",
    "    \"Extract ticket granting ticket from HTML.\"\n",
    "    # print('htmlcontent: {}'.format(htmlcontent))\n",
    "    html = HTML(html=htmlcontent)\n",
    "    # get form element\n",
    "    elements = html.xpath(\"//form\")\n",
    "    if elements != []:\n",
    "        return elements[0].attrs['action'].split('/')[-1]\n",
    "    else:\n",
    "        return \"form element missing from ticket granting ticket response\"\n",
    "\n",
    "def get_ticket(cas_serverurl, apikey, serviceurl):\n",
    "    # set ticket granting ticket server url\n",
    "    tgtserverurl = cas_serverurl + \"/api-key\"\n",
    "    # set service ticket server url\n",
    "    stserverurl = cas_serverurl + \"/tickets\"\n",
    "    tgt = get_ticket_granting_ticket(tgtserverurl, apikey)\n",
    "    return get_service_ticket(stserverurl, tgt, serviceurl)\n",
    "\n",
    "def get_ticket_granting_ticket(tgtserverurl, apikey):\n",
    "    # http://serviceurl/cas/v1/tickets/{TGT id}\n",
    "    response = requests.post(tgtserverurl, {'apikey': apikey},\n",
    "                             headers={'Accept': 'test/plain'})\n",
    "    return extract_tgt_ticket(response.content)\n",
    "\n",
    "def extract_tgt_ticket(htmlcontent):\n",
    "    \"Extract ticket granting ticket from HTML.\"    \n",
    "    soup = BeautifulSoup(htmlcontent)\n",
    "#     print(soup.find('form').get(\"action\"))\n",
    "    cas_url = soup.find(\"form\").get(\"action\")\n",
    "    \"Extract ticket granting ticket out of 'action' attribute\"\n",
    "    return cas_url.rsplit('/')[-1]\n",
    "\n",
    "def get_redirect_target(resp):\n",
    "        \"\"\"Receives a Response. Returns a redirect URI or ``None``\"\"\"\n",
    "        # Due to the nature of how requests processes redirects this method will\n",
    "        # be called at least once upon the original response and at least twice\n",
    "        # on each subsequent redirect response (if any).\n",
    "        # If a custom mixin is used to handle this logic, it may be advantageous\n",
    "        # to cache the redirect location onto the response object as a private\n",
    "        # attribute.\n",
    "        if resp.is_redirect:\n",
    "            location = resp.headers[\"location\"]\n",
    "            # Currently the underlying http module on py3 decode headers\n",
    "            # in latin1, but empirical evidence suggests that latin1 is very\n",
    "            # rarely used with non-ASCII characters in HTTP headers.\n",
    "            # It is more likely to get UTF8 header rather than latin1.\n",
    "            # This causes incorrect handling of UTF8 encoded location headers.\n",
    "            # To solve this, we re-encode the location in latin1.\n",
    "#             print(location)\n",
    "            location = location.encode(\"latin1\")\n",
    "#             print(location)\n",
    "#             print(to_native_string(location, \"utf8\"))\n",
    "            return to_native_string(location, \"utf8\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426433fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metamap_mappings(chunk, args):\n",
    "    \n",
    "    form = {}\n",
    "    form['KSOURCE'] = ksource\n",
    "    form['COMMAND_ARGS'] = args\n",
    "    headers = {'Accept': 'application/json'}\n",
    "\n",
    "    mm_terms = {}\n",
    "    cui_pattern = r\"C\\d+(?=:)\"\n",
    "    name_pattern = r\"(?<=:)[^[]+\"\n",
    "    semtype_pattern = r\"\\[(.*?)\\]\"\n",
    "    \n",
    "    form['APIText'] = chunk\n",
    "    service_ticket = get_ticket(CAS_SERVERURL, apikey, serviceurl)\n",
    "    params = {'ticket': service_ticket}\n",
    "\n",
    "    s = requests.Session()\n",
    "    trycnt = 5  # max try count to receive response from MetaMap Interactive API\n",
    "    while trycnt > 0:\n",
    "        try:\n",
    "            response = s.post(serviceurl, form, headers=headers, params=params, allow_redirects=False)\n",
    "            if response.status_code == 302:\n",
    "                newurl = s.get_redirect_target(response)\n",
    "                response = s.post(newurl, form, headers=headers, params=params, allow_redirects=False)\n",
    "            trycnt = 0 # success, recieved response from MetaMap Interactive Server\n",
    "        except (ConnectionResetError,OSError) as ex:  # Catch ProtocolError or socket.error in requests that raises a ConnectionError as \"OSError\" ....https://stackoverflow.com/questions/74253820/cannot-catch-requests-exceptions-connectionerror-with-try-except\n",
    "            if trycnt <= 0: print(\"Failed to retrieve MetaMap response\\n\" + str(ex))  # done retrying\n",
    "            else: trycnt -= 1  # retry\n",
    "            time.sleep(5)  # wait 5 seconds, then retry\n",
    "            \n",
    "    for line in response.text.splitlines():\n",
    "        if not any(s in line for s in [\"Meta Mapping\", \"Processing\", \"/dmzfiler/\"]):\n",
    "            if \"Phrase:\" in line:\n",
    "                cuis_per_input = []\n",
    "                mm_input = line.split(\":\")[1].strip()\n",
    "                cui_match_count = 0\n",
    "            else:\n",
    "                cui_match = re.findall(cui_pattern, line)\n",
    "                if cui_match: \n",
    "                    cui_match_count +=1\n",
    "                    if cui_match_count > 1: # get only 1st CUI/CURIE per Phrase; continue to next loop iteration to skip lines with more available CUIs\n",
    "                        continue\n",
    "                    cui_info = []\n",
    "                    name_match = re.findall(name_pattern, line)\n",
    "                    semtype_match = re.findall(semtype_pattern, line)\n",
    "                    try: cui_info.append(cui_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    try: cui_info.append(name_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    try: cui_info.append(semtype_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    cuis_per_input.append(cui_info)\n",
    "                    mm_terms[mm_input] = cuis_per_input\n",
    "    return mm_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47971252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_threads_mm(terms_chunked, args):\n",
    "    # multithread implementation for retrieving MetaMap API responses\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with concurrent.futures.ThreadPoolExecutor(multiprocessing.cpu_count() - 1) as executor:\n",
    "        # Submit the get_response() function for each item in the list\n",
    "        futures = [executor.submit(get_metamap_mappings, term, args) for term in terms_chunked]\n",
    "\n",
    "        # Retrieve the results as they become available\n",
    "        output = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    mm_dict = reduce(lambda d1, d2: {**d1, **d2}, output) # merge the list of dicts of MetaMap responses in output into 1 dict\n",
    "    return mm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_by_char_lim(lst):\n",
    "    result = []\n",
    "    current_sublist = []\n",
    "    current_length = 0\n",
    "    for item in lst:\n",
    "        item_length = len(item)\n",
    "        if current_length + item_length > 9000: # max is 10,000 char allowed by MetaMap\n",
    "            result.append(current_sublist)\n",
    "            current_sublist = []\n",
    "            current_length = 0\n",
    "        item = item + \"\\n\"  # add a \"\\n\" for term processing option by MetaMap, the terms in the input file must be separated by blank lines (https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/TermProcessing.pdf)\n",
    "        current_sublist.append(item)\n",
    "        current_length += item_length\n",
    "    result.append(current_sublist)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22dd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_mm(ct_terms):\n",
    "    \n",
    "    unmapped_conditions = ct_terms[\"unmapped_conditions\"]\n",
    "    conditions_unmapped_chunked = split_list_by_char_lim(unmapped_conditions)\n",
    "    # see MetaMap Usage instructions: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/MM_2016_Usage.pdf\n",
    "    # removing sosy semantic type (sign or symptom) - often get MetaMap matches to the sign or symptom instead of the full disease...for example, will get back \"exercise-induced\" instead of \"immune dysfunction\" for \"exercise-induced immune dysfunction\" bc it matches the descriptive quality \"exercise-induced\" is matched on \n",
    "    condition_args = ['--sldi -I -C -J acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,sosy -z -i -f']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    mm_conditions = run_parallel_threads_mm(conditions_unmapped_chunked, condition_args)\n",
    "    \n",
    "    unmapped_interventions = ct_terms[\"unmapped_interventions\"]\n",
    "    interventions_unmapped_chunked = split_list_by_char_lim(unmapped_interventions)\n",
    "    intervention_args = ['--sldi -I -C -k acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,sosy -z -i -f']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    mm_interventions = run_parallel_threads_mm(interventions_unmapped_chunked, intervention_args)\n",
    "    \n",
    "\n",
    "#     print(mm_conditions)\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    print(mm_interventions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b4ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list_to_mm(ct_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdbb11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct_terms[\"unmapped_interventions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77523821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6267f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_terms[\"unmapped_conditions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10580f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f63234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f423bc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b59dc9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mapped_conditions', 'unmapped_conditions', 'mapped_interventions', 'unmapped_interventions'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_terms.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbdfed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3927c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a632fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dea4f080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique conditions in this Clinical Trials data dump: 288\n",
      "Number of unique conditions that have an exact MeSH term match given in this dump: 79\n",
      "since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique conditions that are unmapped after finding exact MeSH mappings and Name Resolver to get CURIES: 209\n",
      "Number of unique interventions in this Clinical Trials data dump: 364\n",
      "Number of unique interventions that have an exact MeSH term match given in this dump: 57\n",
      "since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique interventions that are unmapped after finding exact MeSH mappings and Name Resolver to get CURIES: 307\n",
      "Use fuzzy matching on MeSH terms from Clinical Trials dump to find more potential matches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kamileh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique conditions mapped using fuzzy matching to MeSH terms: 20\n",
      "Number of unique conditions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: 209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kamileh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique interventions mapped using fuzzy matching to MeSH terms: 3\n",
      "Number of unique interventions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: 307\n",
      "Number of unique conditions mapped using Name Resolver: 195\n",
      "Number of unique conditions that are unmapped after using Name Resolver: 14\n",
      "Number of unique interventions mapped using Name Resolver: 136\n",
      "Number of unique interventions that are unmapped after using Name Resolver: 171\n"
     ]
    }
   ],
   "source": [
    "# flag_and_path = get_raw_ct_data() # uncomment for production\n",
    "flag_and_path = {'term_program_flag': False, 'data_extracted_path': '/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/06_27_2023_extracted'} # comment for production\n",
    "df_dict = read_raw_ct_data(flag_and_path)\n",
    "ct_terms = exact_match_mesh(df_dict)\n",
    "ct_terms = inexact_match_mesh(df_dict, ct_terms)\n",
    "test = term_list_to_nr(df_dict, ct_terms)\n",
    "# term_list_to_mm(ct_terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58981552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['endovascular patients undergoing intracranial intervention',\n",
       " 'traumatic burst fractures of the thoracic or lumbar spine.',\n",
       " 'type2diabetes',\n",
       " \"crow's feet\",\n",
       " 'chronic genotype 4 hcv',\n",
       " 'urinary incontinence by intrinsic sphincter deficiency',\n",
       " 'increased satiety',\n",
       " 'immune related adverse events',\n",
       " 'locally advanced hpv positive oropharynx cancer',\n",
       " 'mukbang',\n",
       " 'alk-positive nsclc',\n",
       " 'healthy adult volunteers',\n",
       " 'advanced solid tumors',\n",
       " 'severe eosinophilic asthma']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"unmapped_conditions\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
