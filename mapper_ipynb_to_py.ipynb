{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48f07ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a64de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "import concurrent\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "\n",
    "import urllib\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31cd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global sublist_length \n",
    "sublist_length = 40 # Name Resolver takes batches of 1000\n",
    "\n",
    "global CAS_SERVERURL \n",
    "global II_SKR_SERVERURL \n",
    "global METAMAP_INTERACTIVE_URL \n",
    "global stserverurl \n",
    "global tgtserverurl\n",
    "global apikey \n",
    "global serviceurl \n",
    "global ksource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3449fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAS_SERVERURL = \"https://utslogin.nlm.nih.gov/cas/v1\"\n",
    "II_SKR_SERVERURL = 'https://ii.nlm.nih.gov/cgi-bin/II/UTS_Required'\n",
    "METAMAP_INTERACTIVE_URL = II_SKR_SERVERURL + \"/API_MM_interactive.pl\"\n",
    "stserverurl = \"https://utslogin.nlm.nih.gov/cas/v1/tickets\"\n",
    "tgtserverurl = \"https://utslogin.nlm.nih.gov/cas/v1/api-key\"\n",
    "serviceurl = METAMAP_INTERACTIVE_URL\n",
    "ksource = '2020AB'\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.read('config.ini')\n",
    "apikey = cfg['METAMAP']['apikey']\n",
    "apikey = apikey.strip(\"\\''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6835c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kamileh/opt/anaconda3/lib/python3.7/site-packages/thefuzz/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# %pip install thefuzz\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa003b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nr_response(chunk):\n",
    "    \n",
    "    \"\"\"Runs Name Resolver\"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    nr_terms = []\n",
    "    for term in chunk:\n",
    "        nr_term = {}\n",
    "        params = {'string':term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give            \n",
    "        r = requests.post(nr_url, params=params)\n",
    "        try:\n",
    "            res = r.json()\n",
    "            if res:\n",
    "                for key, val in res.items():\n",
    "                    nr_term[term] = [key, val[0]]\n",
    "                    nr_terms.append(nr_term)\n",
    "            else:\n",
    "#                 print(term + \" unable to be mapped by Name Resolver\")\n",
    "                pass\n",
    "        except Exception as e:\n",
    "#             print(e)\n",
    "#             print(term + \" unable to be mapped by Name Resolver\")\n",
    "            pass      \n",
    "    time.sleep(5)\n",
    "    return nr_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92fd99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_threads_nr(unmapped_chunked):\n",
    "    # multithread implementation for retrieving Name Resolver responses\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with concurrent.futures.ThreadPoolExecutor(multiprocessing.cpu_count() - 1) as executor:\n",
    "        # Submit the get_response() function for each item in the list\n",
    "        futures = [executor.submit(get_nr_response, chunk) for chunk in unmapped_chunked]\n",
    "        # Retrieve the results as they become available\n",
    "        output = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565b827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, sublist_length):\n",
    "    return [lst[i:i+sublist_length] for i in range(0, len(lst), sublist_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdc5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "def get_token_sort_ratio(str1, str2):\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "\n",
    "def get_token_set_ratio(str1, str2):\n",
    "    try:\n",
    "        return fuzz.token_set_ratio(str1, str2)\n",
    "    except:\n",
    "        return None  \n",
    "set_ratio = np.vectorize(get_token_set_ratio)\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "sim_score = np.vectorize(get_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cec96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    # get all the links and associated dates of upload into a dict called date_link\n",
    "    url_all = \"https://aact.ctti-clinicaltrials.org/pipe_files\"\n",
    "    response = requests.get(url_all)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    body = soup.find_all('option') #Find all\n",
    "    date_link = {}\n",
    "    for el in body:\n",
    "        tags = el.find('a')\n",
    "        try:\n",
    "            zip_name = tags.contents[0].split()[0]\n",
    "            date = zip_name.split(\"_\")[0]\n",
    "            date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "            date_link[date] = tags.get('href')\n",
    "        except:\n",
    "            pass\n",
    "    latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "    url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "    date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "    data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "    data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "    data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    \n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "        \n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Downloading Clinical Trial data as of {}\".format(date_string))\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(data_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(\"Finished download of zip\")\n",
    "            with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                print(\"Unzipping data\")\n",
    "                download.extractall(data_extracted)\n",
    "        else:\n",
    "            print(\"KG is already up to date.\")\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51e3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "#         exit()\n",
    "#         pass\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0)\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0)\n",
    "        browse_conditions_df = pd.read_csv(data_extracted + '/browse_conditions.txt', sep='|', index_col=False, header=0)\n",
    "        browse_interventions_df = pd.read_csv(data_extracted + '/browse_interventions.txt', sep='|', index_col=False, header=0)\n",
    "        \n",
    "    ### GET RID OF....CHEAT LINE FOR TESTING\n",
    "        conditions_df = conditions_df.iloc[:2000]\n",
    "        interventions_df = interventions_df.iloc[:2000]\n",
    "\n",
    "    return {\"conditions\": conditions_df, \"interventions\": interventions_df, \"browse_conditions\": browse_conditions_df, \"browse_interventions\": browse_interventions_df}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38bd5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_mesh(df_dict):\n",
    "    \n",
    "    # -------    CONDITIONS    ------- #\n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    browse_conditions = df_dict[\"browse_conditions\"] \n",
    "\n",
    "    tomap_conditions = conditions[\"downcase_name\"].values.tolist()\n",
    "    tomap_conditions = list(set(tomap_conditions))\n",
    "    print(\"Number of unique conditions in this Clinical Trials data dump: {}\".format(len(tomap_conditions)))\n",
    "    mesh_exact_mapped = list(set(tomap_conditions).intersection(browse_conditions.downcase_mesh_term.unique()))\n",
    "    print(\"Number of unique conditions that have an exact MeSH term match given in this dump: {}\".format(len(mesh_exact_mapped)))\n",
    "\n",
    "    print(\"since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_exact_mapped_chunked = split_list(mesh_exact_mapped, sublist_length)\n",
    "    mesh_exact_mapped_curied = run_parallel_threads_nr(mesh_exact_mapped_chunked)\n",
    "\n",
    "    mesh_exact_mapped_curied = [element for sublist in mesh_exact_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    mesh_exact_mapped_curied = {key: value for dictionary in mesh_exact_mapped_curied for key, value in dictionary.items()}\n",
    "\n",
    "    mapped_conditions = pd.DataFrame({\"condition_input\": list(mesh_exact_mapped_curied.keys()), # get dataframe of exact MeSH mapped conditions\n",
    "                                      \"condition_CURIE_id\": [value[0] for value in mesh_exact_mapped_curied.values()],\n",
    "                                      \"condition_CURIE_name\": [value[-1] for value in mesh_exact_mapped_curied.values()],\n",
    "                                      \"source\": \"MeSH term exact mapped, Name Resolver CURIE\"})\n",
    "    unmapped_conditions = list(set(tomap_conditions)-set(mapped_conditions.condition_input))\n",
    "    print(\"Number of unique conditions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_conditions)))\n",
    "\n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    browse_interventions = df_dict[\"browse_interventions\"] \n",
    "\n",
    "    tomap_interventions = interventions[\"name\"].values.tolist()\n",
    "    tomap_interventions = reduce(lambda a, b: a+[str(b)], tomap_interventions, [])\n",
    "    tomap_interventions = [string.lower() for string in tomap_interventions] # lowercase the strings\n",
    "    tomap_interventions = list(set(tomap_interventions))\n",
    "    print(\"Number of unique interventions in this Clinical Trials data dump: {}\".format(len(tomap_interventions)))\n",
    "    mesh_exact_mapped = list(set(tomap_interventions).intersection(browse_interventions.downcase_mesh_term.unique()))\n",
    "    print(\"Number of unique interventions that have an exact MeSH term match given in this dump: {}\".format(len(mesh_exact_mapped)))\n",
    "\n",
    "    print(\"since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_exact_mapped_chunked = split_list(mesh_exact_mapped, sublist_length)\n",
    "    mesh_exact_mapped_curied = run_parallel_threads_nr(mesh_exact_mapped_chunked)\n",
    "    mesh_exact_mapped_curied = [element for sublist in mesh_exact_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    mesh_exact_mapped_curied = {key: value for dictionary in mesh_exact_mapped_curied for key, value in dictionary.items()}\n",
    "    mapped_interventions = pd.DataFrame({\"intervention_input\": list(mesh_exact_mapped_curied.keys()),    # get dataframe of exact MeSH mapped interventions\n",
    "                                         \"intervention_CURIE_id\": [value[0] for value in mesh_exact_mapped_curied.values()],\n",
    "                                         \"intervention_CURIE_name\": [value[-1] for value in mesh_exact_mapped_curied.values()],\n",
    "                                         \"source\": \"MeSH term exact mapped, Name Resolver CURIE\"})\n",
    "\n",
    "    unmapped_interventions = list(set(tomap_interventions)-set(mapped_interventions.intervention_input))\n",
    "    print(\"Number of unique interventions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_interventions)))\n",
    "\n",
    "    ct_terms = {'mapped_conditions': mapped_conditions,\n",
    "                'unmapped_conditions': unmapped_conditions,\n",
    "                'mapped_interventions': mapped_interventions,\n",
    "                'unmapped_interventions': unmapped_interventions}\n",
    "    return ct_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccb00739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inexact_match_mesh(df_dict, ct_terms):\n",
    "    \n",
    "    # get dataframes bc I'm going to compute fuzzy scores and dump into columns\n",
    "    # find unmapped terms AND THEIR CORRESPONDING NCITS!\n",
    "    # get the conditions that have exact MESH term matches, and conditions that don't have exact MESH term matches. We want to filter for rows that don't have exact MESH term matches bc we already captured those and don't want to run scoring on it\n",
    "\n",
    "    # -------    CONDITIONS    ------- #\n",
    "\n",
    "    print(\"Use fuzzy matching on MeSH terms from Clinical Trials dump to find more potential matches.\")\n",
    "    conditions = df_dict[\"conditions\"] \n",
    "    conditions = conditions[[\"nct_id\", \"downcase_name\"]]\n",
    "\n",
    "    browse_conditions = df_dict[\"browse_conditions\"] \n",
    "    all_mesh_conditions = browse_conditions.downcase_mesh_term.unique()\n",
    "    mask = np.isin(conditions['downcase_name'], all_mesh_conditions)\n",
    "    conditions['mesh_conditions_exact_mapped'] = np.where(mask, conditions['downcase_name'], np.nan)\n",
    "    conditions_unmapped = conditions[conditions['mesh_conditions_exact_mapped'].isnull()] # get the rows where mesh_term is empty bc there was no match there, we will run fuzzy scoring on these rows (I'm getting unmapped conditions along with their NCT IDs, not just the condition, bc I need this to find possible MeSH terms by study)\n",
    "    conditions_unmapped = conditions_unmapped.drop('mesh_conditions_exact_mapped', axis=1) # drop the empty column now\n",
    "\n",
    "    mesh_conditions_per_study = pd.DataFrame(browse_conditions[[\"nct_id\", \"downcase_mesh_term\", \"mesh_type\"]].groupby(\"nct_id\")[\"downcase_mesh_term\"].apply(list)) # get all MeSH terms available for each study\n",
    "\n",
    "    conditions_unmapped_all_mesh_terms = pd.merge(conditions_unmapped, \n",
    "                                                  mesh_conditions_per_study,\n",
    "                                                  how='left',\n",
    "                                                  left_on=['nct_id'],\n",
    "                                                  right_on = ['nct_id'])\n",
    "\n",
    "    # some clinical trials are missing from browse_conditions (those nct_ids are not present in the browse_conditions text) They have NaN in the downcase_mesh_term column\n",
    "    conditions_unmapped_all_mesh_terms = conditions_unmapped_all_mesh_terms[~conditions_unmapped_all_mesh_terms['downcase_mesh_term'].isnull()] # subset or delete rows where either column is empty/Nonetype bc fuzzymatching functions will throw error if handling\n",
    "    conditions_unmapped_all_mesh_terms = conditions_unmapped_all_mesh_terms.explode('downcase_mesh_term')\n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    conditions_unmapped_all_mesh_terms[\"sort_ratio\"] = sort_ratio(conditions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, conditions_unmapped_all_mesh_terms[[\"downcase_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    conditions_unmapped_all_mesh_terms[\"sim_score\"] = sim_score(conditions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, conditions_unmapped_all_mesh_terms[[\"downcase_name\"]].values)\n",
    "    conditions_mesh_fuzz_scored = conditions_unmapped_all_mesh_terms[(conditions_unmapped_all_mesh_terms['sim_score'] > 88) | (conditions_unmapped_all_mesh_terms['sort_ratio'] > 88)]\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(by = ['nct_id', 'downcase_name'], ascending = [True, True], na_position = 'first')\n",
    "\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(by = ['sim_score', 'sort_ratio'], ascending=[False,False], na_position='last').drop_duplicates(['nct_id', 'downcase_name']).sort_index() # there may be many mesh terms that passed the ratio and score filter; this causes duplicates bc I exploded the df...this line of code picks one and throws away other potential matches for one disease-nct_id pair\n",
    "    conditions_mesh_fuzz_scored = conditions_mesh_fuzz_scored.sort_values(['nct_id'], ascending=False)\n",
    "\n",
    "    keys = list(conditions_mesh_fuzz_scored[[\"nct_id\", \"downcase_name\"]].columns.values)\n",
    "    i1 = conditions_unmapped.set_index(keys).index\n",
    "    i2 = conditions_mesh_fuzz_scored.set_index(keys).index\n",
    "\n",
    "    tomap_conditions = conditions_mesh_fuzz_scored[\"downcase_mesh_term\"].values.tolist()\n",
    "    tomap_conditions = list(set(tomap_conditions))\n",
    "    \n",
    "    print(\"Since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_fuzz_mapped_chunked = split_list(tomap_conditions, sublist_length)\n",
    "    mesh_fuzz_mapped_curied = run_parallel_threads_nr(mesh_fuzz_mapped_chunked)\n",
    "    mesh_fuzz_mapped_curied = [element for sublist in mesh_fuzz_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique conditions mapped using fuzzy matching to MeSH terms: {}\".format(len(mesh_fuzz_mapped_curied)))\n",
    "    mesh_fuzz_mapped_curied = {key: value for dictionary in mesh_fuzz_mapped_curied for key, value in dictionary.items()}\n",
    "    fuzz_mapped_conditions = pd.DataFrame({\"condition_input\": list(mesh_fuzz_mapped_curied.keys()),\n",
    "                                           \"condition_CURIE_id\": [value[0] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                           \"condition_CURIE_name\": [value[-1] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                           \"source\": \"MeSH term fuzzy mapped, Name Resolver CURIE\"})\n",
    "    previously_mapped = ct_terms[\"mapped_conditions\"]\n",
    "    combined_mapped_conditions = pd.concat([previously_mapped, fuzz_mapped_conditions], ignore_index=True) # get dataframe of combined previously mapped conditions and additional fuzzy MeSH mapped conditions\n",
    "\n",
    "    all_conditions_list = conditions[\"downcase_name\"].values.tolist()\n",
    "    all_conditions_list = list(set(all_conditions_list))\n",
    "    unmapped_conditions = list(set(all_conditions_list)-set(list(combined_mapped_conditions.condition_input.values)))\n",
    "    print(\"Number of unique conditions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_conditions)))\n",
    "\n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "\n",
    "    interventions = df_dict[\"interventions\"] \n",
    "    interventions['downcase_name'] = interventions['name'].str.lower()\n",
    "    interventions = interventions[[\"nct_id\", \"downcase_name\"]]\n",
    "    browse_interventions = df_dict[\"browse_interventions\"] \n",
    "    all_mesh_interventions = browse_interventions.downcase_mesh_term.unique()\n",
    "    mask = np.isin(interventions['downcase_name'], all_mesh_interventions)\n",
    "    interventions['mesh_interventions_exact_mapped'] = np.where(mask, interventions['downcase_name'], np.nan)\n",
    "    interventions_unmapped = interventions[interventions['mesh_interventions_exact_mapped'].isnull()] # get the rows where mesh_term is empty bc there was no match there, we will run fuzzy scoring on these rows (I'm getting unmapped interventions along with their NCT IDs, not just the condition, bc I need this to find possible MeSH terms by study)\n",
    "    interventions_unmapped = interventions_unmapped.drop('mesh_interventions_exact_mapped', axis=1) # drop the empty column now\n",
    "\n",
    "    mesh_interventions_per_study = pd.DataFrame(browse_interventions[[\"nct_id\", \"downcase_mesh_term\", \"mesh_type\"]].groupby(\"nct_id\")[\"downcase_mesh_term\"].apply(list)) # get all MeSH terms available for each study\n",
    "\n",
    "    interventions_unmapped_all_mesh_terms = pd.merge(interventions_unmapped, \n",
    "                                                  mesh_interventions_per_study,\n",
    "                                                  how='left',\n",
    "                                                  left_on=['nct_id'],\n",
    "                                                  right_on = ['nct_id'])\n",
    "\n",
    "    # # some clinical trials are missing from browse_interventions (those nct_ids are not present in the browse_interventions text) They have NaN in the downcase_mesh_term column\n",
    "    interventions_unmapped_all_mesh_terms = interventions_unmapped_all_mesh_terms[~interventions_unmapped_all_mesh_terms['downcase_mesh_term'].isnull()] # subset or delete rows where either column is empty/Nonetype bc fuzzymatching functions will throw error if handling\n",
    "    interventions_unmapped_all_mesh_terms = interventions_unmapped_all_mesh_terms.explode('downcase_mesh_term')\n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    interventions_unmapped_all_mesh_terms[\"sort_ratio\"] = sort_ratio(interventions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, interventions_unmapped_all_mesh_terms[[\"downcase_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    interventions_unmapped_all_mesh_terms[\"sim_score\"] = sim_score(interventions_unmapped_all_mesh_terms[[\"downcase_mesh_term\"]].values, interventions_unmapped_all_mesh_terms[[\"downcase_name\"]].values)\n",
    "    interventions_mesh_fuzz_scored = interventions_unmapped_all_mesh_terms[(interventions_unmapped_all_mesh_terms['sim_score'] > 88) | (interventions_unmapped_all_mesh_terms['sort_ratio'] > 88)]\n",
    "    interventions_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(by = ['nct_id', 'downcase_name'], ascending = [True, True], na_position = 'first')\n",
    "\n",
    "    interventions_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(by = ['sim_score', 'sort_ratio'], ascending=[False,False], na_position='last').drop_duplicates(['nct_id', 'downcase_name']).sort_index() # there may be many mesh terms that passed the ratio and score filter; this causes duplicates bc I exploded the df...this line of code picks one and throws away other potential matches for one disease-nct_id pair\n",
    "    interventions_mesh_fuzz_scored = interventions_mesh_fuzz_scored.sort_values(['nct_id'], ascending=False)\n",
    "\n",
    "    keys = list(interventions_mesh_fuzz_scored[[\"nct_id\", \"downcase_name\"]].columns.values)\n",
    "    i1 = interventions_unmapped.set_index(keys).index\n",
    "    i2 = interventions_mesh_fuzz_scored.set_index(keys).index\n",
    "\n",
    "    tomap_interventions = interventions_mesh_fuzz_scored[\"downcase_mesh_term\"].values.tolist()\n",
    "    tomap_interventions = list(set(tomap_interventions))\n",
    "    \n",
    "    print(\"Since these MeSH terms don't come with identifers, we retrieve them from Name Resolver\")\n",
    "    mesh_fuzz_mapped_chunked = split_list(tomap_interventions, sublist_length)\n",
    "    mesh_fuzz_mapped_curied = run_parallel_threads_nr(mesh_fuzz_mapped_chunked)\n",
    "    mesh_fuzz_mapped_curied = [element for sublist in mesh_fuzz_mapped_curied for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique interventions mapped using fuzzy matching to MeSH terms: {}\".format(len(mesh_fuzz_mapped_curied)))\n",
    "    mesh_fuzz_mapped_curied = {key: value for dictionary in mesh_fuzz_mapped_curied for key, value in dictionary.items()}\n",
    "    fuzz_mapped_interventions = pd.DataFrame({\"intervention_input\": list(mesh_fuzz_mapped_curied.keys()),\n",
    "                                              \"intervention_CURIE_id\": [value[0] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                              \"intervention_CURIE_name\": [value[-1] for value in mesh_fuzz_mapped_curied.values()],\n",
    "                                              \"source\": \"MeSH term fuzzy mapped, Name Resolver CURIE\"})\n",
    "    previously_mapped = ct_terms[\"mapped_interventions\"]\n",
    "    combined_mapped_interventions = pd.concat([previously_mapped, fuzz_mapped_interventions], ignore_index=True) # get dataframe of combined previously mapped interventions and additional fuzzy MeSH mapped interventions\n",
    "    \n",
    "    all_interventions_list = interventions[\"downcase_name\"].values.tolist()\n",
    "    all_interventions_list = list(set(all_interventions_list))\n",
    "    unmapped_interventions = list(set(all_interventions_list)-set(list(combined_mapped_interventions.intervention_input.values)))\n",
    "    print(\"Number of unique interventions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: {}\".format(len(unmapped_interventions)))\n",
    "\n",
    "    ct_terms = {'mapped_conditions': combined_mapped_conditions,\n",
    "                'unmapped_conditions': unmapped_conditions,\n",
    "                'mapped_interventions': combined_mapped_interventions,\n",
    "                'unmapped_interventions': unmapped_interventions,\n",
    "                \"mesh_conditions_per_study\": mesh_conditions_per_study,\n",
    "                \"mesh_interventions_per_study\": mesh_interventions_per_study}\n",
    "    return ct_terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d7897b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_nr(df_dict, ct_terms):\n",
    "    \n",
    "    # -------    CONDITIONS    ------- #\n",
    "\n",
    "    unmapped_conditions = ct_terms[\"unmapped_conditions\"]\n",
    "    conditions_unmapped_chunked = split_list(unmapped_conditions, sublist_length)\n",
    "    nr_conditions = run_parallel_threads_nr(conditions_unmapped_chunked)\n",
    "    nr_conditions = [element for sublist in nr_conditions for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique conditions mapped using Name Resolver: {}\".format(len(nr_conditions)))\n",
    "    nr_conditions = {key: value for dictionary in nr_conditions for key, value in dictionary.items()}\n",
    "    nr_conditions_df = pd.DataFrame({\"condition_input\": list(nr_conditions.keys()),\n",
    "                                     \"condition_CURIE_id\": [value[0] for value in nr_conditions.values()],\n",
    "                                     \"condition_CURIE_name\": [value[-1] for value in nr_conditions.values()],\n",
    "                                     \"source\": \"Name Resolver, no further curation\"})\n",
    "    \n",
    "    previously_mapped = ct_terms[\"mapped_conditions\"]\n",
    "    combined_mapped_conditions = pd.concat([previously_mapped, nr_conditions_df], ignore_index=True) # get dataframe of combined previously mapped conditions and additional fuzzy MeSH mapped conditions\n",
    "    \n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    all_conditions_list = conditions[\"downcase_name\"].values.tolist()\n",
    "    all_conditions_list = list(set(all_conditions_list))\n",
    "    unmapped_conditions = list(set(all_conditions_list)-set(list(combined_mapped_conditions.condition_input.values)))\n",
    "    print(\"Number of unique conditions that are unmapped after using Name Resolver: {}\".format(len(unmapped_conditions)))\n",
    "    \n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    \n",
    "    unmapped_interventions = ct_terms[\"unmapped_interventions\"]\n",
    "    interventions_unmapped_chunked = split_list(unmapped_interventions, sublist_length)\n",
    "    nr_interventions = run_parallel_threads_nr(interventions_unmapped_chunked)\n",
    "    nr_interventions = [element for sublist in nr_interventions for element in sublist] # flatten the list of lists\n",
    "    print(\"Number of unique interventions mapped using Name Resolver: {}\".format(len(nr_interventions)))\n",
    "    nr_interventions = {key: value for dictionary in nr_interventions for key, value in dictionary.items()}\n",
    "    nr_interventions_df = pd.DataFrame({\"intervention_input\": list(nr_interventions.keys()),\n",
    "                                     \"intervention_CURIE_id\": [value[0] for value in nr_interventions.values()],\n",
    "                                     \"intervention_CURIE_name\": [value[-1] for value in nr_interventions.values()],\n",
    "                                     \"source\": \"Name Resolver, no further curation\"})\n",
    "    \n",
    "    previously_mapped = ct_terms[\"mapped_interventions\"]\n",
    "    combined_mapped_interventions = pd.concat([previously_mapped, nr_interventions_df], ignore_index=True) # get dataframe of combined previously mapped interventions and additional fuzzy MeSH mapped interventions\n",
    "    \n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    all_interventions_list = interventions[\"downcase_name\"].values.tolist()\n",
    "    all_interventions_list = list(set(all_interventions_list))\n",
    "    unmapped_interventions = list(set(all_interventions_list)-set(list(combined_mapped_interventions.intervention_input.values)))\n",
    "    print(\"Number of unique interventions that are unmapped after using Name Resolver: {}\".format(len(unmapped_interventions)))\n",
    "    \n",
    "    ct_terms = {'mapped_conditions': combined_mapped_conditions, 'unmapped_conditions': unmapped_conditions, 'mapped_interventions': combined_mapped_interventions, 'unmapped_interventions': unmapped_interventions}\n",
    "    return ct_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a6036",
   "metadata": {},
   "source": [
    "# USE METAMAP API TO MAP REMAINING TERMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a0caec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_ticket(serverurl, ticket_granting_ticket, serviceurl):\n",
    "    \"\"\" Obtain a Single-Use Proxy Ticket (also known as service ticket).\n",
    "    Request for a Service Ticket:\n",
    "        POST /cas/v1/tickets/{TGT id} HTTP/1.0\n",
    "    data:\n",
    "           service={form encoded parameter for the service url}\n",
    "    Sucessful Response:\n",
    "        200 OK\n",
    "        ST-1-FFDFHDSJKHSDFJKSDHFJKRUEYREWUIFSD2132\n",
    "    @param serverurl authentication server\n",
    "    @param ticketGrantingTicket a Proxy Granting Ticket.\n",
    "    @param serviceurl url of service with protected resources\n",
    "    @return authentication ticket for service. \"\"\"\n",
    "    resp = requests.post(\"{}/{}\".format(serverurl, ticket_granting_ticket),\n",
    "                         {\"service\": serviceurl})\n",
    "    if resp.status_code == 200:\n",
    "        return resp.content\n",
    "    return 'Error: status: {}'.format(resp.content)\n",
    "\n",
    "def get_ticket(cas_serverurl, apikey, serviceurl):\n",
    "    # set ticket granting ticket server url\n",
    "    tgtserverurl = cas_serverurl + \"/api-key\"\n",
    "    # set service ticket server url\n",
    "    stserverurl = cas_serverurl + \"/tickets\"\n",
    "    tgt = get_ticket_granting_ticket(tgtserverurl, apikey)\n",
    "    return get_service_ticket(stserverurl, tgt, serviceurl)\n",
    "\n",
    "def get_ticket_granting_ticket(tgtserverurl, apikey):\n",
    "    # http://serviceurl/cas/v1/tickets/{TGT id}\n",
    "    response = requests.post(tgtserverurl, {'apikey': apikey},\n",
    "                             headers={'Accept': 'test/plain'})\n",
    "    return extract_tgt_ticket(response.content)\n",
    "\n",
    "def extract_tgt_ticket(htmlcontent):\n",
    "#     print(htmlcontent)\n",
    "    \"Extract ticket granting ticket from HTML.\"    \n",
    "    soup = BeautifulSoup(htmlcontent)\n",
    "#     print(soup.find('form').get(\"action\"))\n",
    "    cas_url = soup.find(\"form\").get(\"action\")\n",
    "    \"Extract ticket granting ticket out of 'action' attribute\"\n",
    "    return cas_url.rsplit('/')[-1]\n",
    "\n",
    "def get_redirect_target(resp):\n",
    "        \"\"\"Receives a Response. Returns a redirect URI or ``None``\"\"\"\n",
    "        # Due to the nature of how requests processes redirects this method will\n",
    "        # be called at least once upon the original response and at least twice\n",
    "        # on each subsequent redirect response (if any).\n",
    "        # If a custom mixin is used to handle this logic, it may be advantageous\n",
    "        # to cache the redirect location onto the response object as a private\n",
    "        # attribute.\n",
    "        if resp.is_redirect:\n",
    "            location = resp.headers[\"location\"]\n",
    "            # Currently the underlying http module on py3 decode headers\n",
    "            # in latin1, but empirical evidence suggests that latin1 is very\n",
    "            # rarely used with non-ASCII characters in HTTP headers.\n",
    "            # It is more likely to get UTF8 header rather than latin1.\n",
    "            # This causes incorrect handling of UTF8 encoded location headers.\n",
    "            # To solve this, we re-encode the location in latin1.\n",
    "#             print(location)\n",
    "            location = location.encode(\"latin1\")\n",
    "#             print(location)\n",
    "#             print(to_native_string(location, \"utf8\"))\n",
    "            return to_native_string(location, \"utf8\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f28a4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metamap_mappings(chunk, args):\n",
    "    \n",
    "    form = {}\n",
    "    form['KSOURCE'] = ksource\n",
    "    form['COMMAND_ARGS'] = args\n",
    "    headers = {'Accept': 'application/json'}\n",
    "\n",
    "    mm_terms = {}\n",
    "    cui_pattern = r\"C\\d+(?=:)\"\n",
    "    name_pattern = r\"(?<=:)[^[]+\"\n",
    "    semtype_pattern = r\"\\[(.*?)\\]\"\n",
    "    \n",
    "    form['APIText'] = chunk\n",
    "    service_ticket = get_ticket(CAS_SERVERURL, apikey, serviceurl)\n",
    "    params = {'ticket': service_ticket}\n",
    "\n",
    "    s = requests.Session()\n",
    "    trycnt = 5  # max try count to receive response from MetaMap Interactive API\n",
    "    while trycnt > 0:\n",
    "        try:\n",
    "            response = s.post(serviceurl, form, headers=headers, params=params, allow_redirects=False)\n",
    "            if response.status_code == 302:\n",
    "                newurl = s.get_redirect_target(response)\n",
    "                response = s.post(newurl, form, headers=headers, params=params, allow_redirects=False)\n",
    "            trycnt = 0 # success, recieved response from MetaMap Interactive Server\n",
    "        except (ConnectionResetError,OSError) as ex:  # Catch ProtocolError or socket.error in requests that raises a ConnectionError as \"OSError\" ....https://stackoverflow.com/questions/74253820/cannot-catch-requests-exceptions-connectionerror-with-try-except\n",
    "            if trycnt <= 0: print(\"Failed to retrieve MetaMap response\\n\" + str(ex))  # done retrying\n",
    "            else: trycnt -= 1  # retry\n",
    "            time.sleep(5)  # wait 5 seconds, then retry\n",
    "            \n",
    "    for line in response.text.splitlines():\n",
    "        if not any(s in line for s in [\"Meta Mapping\", \"Processing\", \"/dmzfiler/\"]):\n",
    "            if \"Phrase:\" in line:\n",
    "                cuis_per_input = []\n",
    "                mm_input = line.split(\":\")[1].strip()\n",
    "                cui_match_count = 0\n",
    "            else:\n",
    "                cui_match = re.findall(cui_pattern, line)\n",
    "                if cui_match: \n",
    "                    cui_match_count +=1\n",
    "                    if cui_match_count > 1: # get only 1st CUI/CURIE per Phrase; continue to next loop iteration to skip lines with more available CUIs\n",
    "                        continue\n",
    "                    cui_info = []\n",
    "                    name_match = re.findall(name_pattern, line)\n",
    "                    semtype_match = re.findall(semtype_pattern, line)\n",
    "                    try: cui_info.append(cui_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    try: cui_info.append(name_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    try: cui_info.append(semtype_match[0].strip())\n",
    "                    except: cui_info.append(None)\n",
    "                    cuis_per_input.append(cui_info)\n",
    "                    mm_terms[mm_input] = cuis_per_input\n",
    "    return mm_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "577ef253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_threads_mm(terms_chunked, args):\n",
    "    # multithread implementation for retrieving MetaMap API responses\n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    with concurrent.futures.ThreadPoolExecutor(multiprocessing.cpu_count() - 1) as executor:\n",
    "        # Submit the get_response() function for each item in the list\n",
    "        futures = [executor.submit(get_metamap_mappings, term, args) for term in terms_chunked]\n",
    "\n",
    "        # Retrieve the results as they become available\n",
    "        output = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    mm_dict = reduce(lambda d1, d2: {**d1, **d2}, output) # merge the list of dicts of MetaMap responses in output into 1 dict\n",
    "    return mm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c75d5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_by_char_lim(lst):\n",
    "    result = []\n",
    "    current_sublist = []\n",
    "    current_length = 0\n",
    "    for item in lst:\n",
    "        item_length = len(item)\n",
    "        if current_length + item_length > 9000: # max is 10,000 char allowed by MetaMap\n",
    "            result.append(current_sublist)\n",
    "            current_sublist = []\n",
    "            current_length = 0\n",
    "        item = item + \"\\n\"  # add a \"\\n\" for term processing option by MetaMap, the terms in the input file must be separated by blank lines (https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/TermProcessing.pdf)\n",
    "        current_sublist.append(item)\n",
    "        current_length += item_length\n",
    "    result.append(current_sublist)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5317dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_list_to_mm(df_dict, ct_terms):\n",
    "    \n",
    "    # -------    CONDITIONS    ------- #\n",
    "    print(\"Using UMLS MetaMap to get more mappings for conditions. MetaMap returns mappings, CUIs, and semantic type of mapping.\")\n",
    "    unmapped_conditions = ct_terms[\"unmapped_conditions\"]\n",
    "    conditions_unmapped_chunked = split_list_by_char_lim(unmapped_conditions)\n",
    "    # see MetaMap Usage instructions: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/MM_2016_Usage.pdf\n",
    "    # removing sosy semantic type (sign or symptom) - often get MetaMap matches to the sign or symptom instead of the full disease...for example, will get back \"exercise-induced\" instead of \"immune dysfunction\" for \"exercise-induced immune dysfunction\" bc it matches the descriptive quality \"exercise-induced\" is matched on \n",
    "    condition_args = ['--sldi -I -C -J acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,sosy -z -i -f']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    mm_conditions = run_parallel_threads_mm(conditions_unmapped_chunked, condition_args)\n",
    "    flattened_mm_conditions = {key: [item for sublist in value for item in sublist] for key, value in mm_conditions.items()}\n",
    "    mm_conditions_df = pd.DataFrame({\"condition_input\": list(flattened_mm_conditions.keys()),\n",
    "                                     \"condition_CURIE_id\": [value[0] for value in flattened_mm_conditions.values()],\n",
    "                                     \"condition_CURIE_name\": [value[1] for value in flattened_mm_conditions.values()],\n",
    "                                     \"condition_semantic_type\": [value[-1] for value in flattened_mm_conditions.values()],\n",
    "                                     \"source\": \"MetaMap via UMLS, term and CURIE\"})\n",
    "    \n",
    "    mm_conditions_df[['condition_CURIE_name_1', 'condition_CURIE_name_2']] = mm_conditions_df['condition_CURIE_name'].str.extract(r'^(.*?)\\s*\\((.*?)\\)$').fillna('NA') # \n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    # many MetaMap terms are returned as \"term (term)\". For example, \"Nonessential Amino Acid (Nonessential amino acid)\". This repetition messes up the sort ratio and sim score, so we extract the substrings out of the parenthesis to conduct scoring on those\n",
    "    mm_conditions_scored = mm_conditions_df.copy()\n",
    "    mm_conditions_scored[\"sort_ratio\"] = sort_ratio(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    mm_conditions_scored[\"sim_score\"] = sim_score(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name\"]].values)\n",
    "\n",
    "    mm_conditions_scored[\"sort_ratio_1\"] = sort_ratio(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name_1\"]].values) # generate fuzzy scores based between original and MetaMap term\n",
    "    mm_conditions_scored[\"sim_score_1\"] = sim_score(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name_1\"]].values)\n",
    "\n",
    "    mm_conditions_scored[\"sort_ratio_2\"] = sort_ratio(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name_2\"]].values) # generate fuzzy scores based between original and MetaMap term\n",
    "    mm_conditions_scored[\"sim_score_2\"] = sim_score(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name_2\"]].values)\n",
    "\n",
    "    mm_conditions_scored_thresholded = mm_conditions_scored.copy() \n",
    "    \n",
    "    mm_conditions_scored[\"sort_ratio\"] = sort_ratio(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name\"]].values) # generate fuzzy scores based between original and MetaMap term\n",
    "    mm_conditions_scored[\"sim_score\"] = sim_score(mm_conditions_scored[[\"condition_input\"]].values, mm_conditions_scored[[\"condition_CURIE_name\"]].values)\n",
    "    mm_conditions_scored_thresholded = mm_conditions_scored.copy() \n",
    "    mm_conditions_scored_thresholded = mm_conditions_scored_thresholded[(mm_conditions_scored_thresholded['sim_score'] > 88) |\n",
    "                                                                        (mm_conditions_scored_thresholded['sort_ratio'] > 88) |\n",
    "                                                                        (mm_conditions_scored_thresholded['sim_score_1'] > 88) |\n",
    "                                                                        (mm_conditions_scored_thresholded['sort_ratio_1'] > 88) |\n",
    "                                                                        (mm_conditions_scored_thresholded['sim_score_2'] > 88) |\n",
    "                                                                        (mm_conditions_scored_thresholded['sort_ratio_2'] > 88)]\n",
    "    \n",
    "    print(\"Number of unique conditions that are mapped after using MetaMap and similarity and ratio score thresholds of 88: {}\".format(mm_conditions_scored_thresholded.shape[0]))\n",
    "    \n",
    "    mm_conditions_scored_thresholded = mm_conditions_scored_thresholded.drop(['condition_CURIE_name_1',\n",
    "                                                                              'condition_CURIE_name_2',\n",
    "                                                                              'sort_ratio',\n",
    "                                                                              'sim_score',\n",
    "                                                                              'sort_ratio_1',\n",
    "                                                                              'sim_score_1',\n",
    "                                                                              'sort_ratio_2',\n",
    "                                                                              'sim_score_2'], axis=1)\n",
    "    previously_mapped = ct_terms[\"mapped_conditions\"]\n",
    "    combined_mapped_conditions = pd.concat([previously_mapped, mm_conditions_scored_thresholded], ignore_index=True) # get dataframe of combined previously mapped conditions and additional MetaMapped interventions that passed threshold scoring\n",
    "\n",
    "    conditions = df_dict[\"conditions\"]\n",
    "    all_conditions_list = conditions[\"downcase_name\"].values.tolist()\n",
    "    all_conditions_list = list(set(all_conditions_list))\n",
    "    unmapped_conditions = list(set(all_conditions_list)-set(list(combined_mapped_conditions.condition_input.values)))\n",
    "    print(\"Number of unique conditions that are unmapped after using MetaMap and similarity and ratio score thresholds of 88: {}\".format(len(unmapped_conditions)))\n",
    "          \n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    print(\"Using UMLS MetaMap to get more mappings for interventions. MetaMap returns mappings, CUIs, and semantic type of mapping.\")\n",
    "    unmapped_interventions = ct_terms[\"unmapped_interventions\"]\n",
    "    interventions_unmapped_chunked = split_list_by_char_lim(unmapped_interventions)\n",
    "    # see MetaMap Usage instructions: https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/MM_2016_Usage.pdf\n",
    "    # removing sosy semantic type (sign or symptom) - often get MetaMap matches to the sign or symptom instead of the full disease...for example, will get back \"exercise-induced\" instead of \"immune dysfunction\" for \"exercise-induced immune dysfunction\" bc it matches the descriptive quality \"exercise-induced\" is matched on \n",
    "    intervention_args = ['--sldi -I -C -k acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,sosy -z -i -f']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\") (I used inverse of semantic terms picked for conditions here)\n",
    "    mm_interventions = run_parallel_threads_mm(interventions_unmapped_chunked, intervention_args)\n",
    "    flattened_mm_interventions = {key: [item for sublist in value for item in sublist] for key, value in mm_interventions.items()}\n",
    "    mm_interventions_df = pd.DataFrame({\"intervention_input\": list(flattened_mm_interventions.keys()),\n",
    "                                        \"intervention_CURIE_id\": [value[0] for value in flattened_mm_interventions.values()],\n",
    "                                        \"intervention_CURIE_name\": [value[1] for value in flattened_mm_interventions.values()],\n",
    "                                        \"intervention_semantic_type\": [value[-1] for value in flattened_mm_interventions.values()],\n",
    "                                        \"source\": \"MetaMap via UMLS, term and CURIE\"})\n",
    "\n",
    "    mm_interventions_df[['intervention_CURIE_name_1', 'intervention_CURIE_name_2']] = mm_interventions_df['intervention_CURIE_name'].str.extract(r'^(.*?)\\s*\\((.*?)\\)$').fillna('NA') # \n",
    "\n",
    "    sort_ratio = np.vectorize(get_token_sort_ratio)\n",
    "    set_ratio = np.vectorize(get_token_set_ratio)\n",
    "    sim_score = np.vectorize(get_similarity_score)\n",
    "\n",
    "    # many MetaMap terms are returned as \"term (term)\". For example, \"Nonessential Amino Acid (Nonessential amino acid)\". This repetition messes up the sort ratio and sim score, so we extract the substrings out of the parenthesis to conduct scoring on those\n",
    "    mm_interventions_scored = mm_interventions_df.copy()\n",
    "    mm_interventions_scored[\"sort_ratio\"] = sort_ratio(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name\"]].values) # generate fuzzy scores based between original and MeSH term\n",
    "    mm_interventions_scored[\"sim_score\"] = sim_score(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name\"]].values)\n",
    "\n",
    "    mm_interventions_scored[\"sort_ratio_1\"] = sort_ratio(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name_1\"]].values) # generate fuzzy scores based between original and MetaMap term\n",
    "    mm_interventions_scored[\"sim_score_1\"] = sim_score(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name_1\"]].values)\n",
    "\n",
    "    mm_interventions_scored[\"sort_ratio_2\"] = sort_ratio(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name_2\"]].values) # generate fuzzy scores based between original and MetaMap term\n",
    "    mm_interventions_scored[\"sim_score_2\"] = sim_score(mm_interventions_scored[[\"intervention_input\"]].values, mm_interventions_scored[[\"intervention_CURIE_name_2\"]].values)\n",
    "\n",
    "    mm_interventions_scored_thresholded = mm_interventions_scored.copy() \n",
    "    mm_interventions_scored_thresholded = mm_interventions_scored_thresholded[(mm_interventions_scored_thresholded['sim_score'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sort_ratio'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sim_score_1'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sort_ratio_1'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sim_score_2'] > 88) |\n",
    "                                                                              (mm_interventions_scored_thresholded['sort_ratio_2'] > 88)]\n",
    "    \n",
    "    print(\"Number of unique interventions that are mapped after using MetaMap and similarity and ratio score thresholds of 88: {}\".format(mm_interventions_scored_thresholded.shape[0]))\n",
    "\n",
    "    mm_interventions_scored_thresholded = mm_interventions_scored_thresholded.drop(['intervention_CURIE_name_1',\n",
    "                                                                                    'intervention_CURIE_name_2',\n",
    "                                                                                    'sort_ratio',\n",
    "                                                                                    'sim_score',\n",
    "                                                                                    'sort_ratio_1',\n",
    "                                                                                    'sim_score_1',\n",
    "                                                                                    'sort_ratio_2',\n",
    "                                                                                    'sim_score_2'], axis=1)\n",
    "    previously_mapped = ct_terms[\"mapped_interventions\"]\n",
    "    combined_mapped_interventions = pd.concat([previously_mapped, mm_interventions_scored_thresholded], ignore_index=True) # get dataframe of combined previously mapped interventions and additional MetaMapped interventions that passed threshold scoring\n",
    "    interventions = df_dict[\"interventions\"]\n",
    "    all_interventions_list = interventions[\"downcase_name\"].values.tolist()\n",
    "    all_interventions_list = list(set(all_interventions_list))\n",
    "    unmapped_interventions = list(set(all_interventions_list)-set(list(combined_mapped_interventions.intervention_input.values)))\n",
    "    print(\"Number of unique interventions that are unmapped after using MetaMap and similarity and ratio score thresholds of 88: {}\".format(len(unmapped_interventions)))\n",
    "    ct_terms = {'mapped_conditions': combined_mapped_conditions,\n",
    "                'unmapped_conditions': unmapped_conditions,\n",
    "                'mapped_interventions': combined_mapped_interventions,\n",
    "                'unmapped_interventions': unmapped_interventions,\n",
    "                'all_metamapped_conditions': mm_conditions_df,\n",
    "                'all_metamapped_interventions': mm_interventions_df}\n",
    "\n",
    "\n",
    "    return ct_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "305f22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output all results to TSVs\n",
    "def compile_and_output(df_dict, ct_terms, remaining_unmapped_possible):\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"#   -------- -------- -------- --------  \")\n",
    "    print(\"Final Tallies:\")\n",
    "    print(\"Total # of conditions mapped: {}\".format(ct_terms[\"mapped_conditions\"].shape[0]))\n",
    "    print(\"Total # of interventions mapped: {}\".format(ct_terms[\"mapped_interventions\"].shape[0]))\n",
    "    print(\"Total # of conditions unmapped or not mapped: {}\".format(len(ct_terms[\"unmapped_conditions\"])))\n",
    "    print(\"Total # of interventions unmapped or not mapped: {}\".format(len(ct_terms[\"unmapped_interventions\"])))\n",
    "    print(\"#   -------- -------- -------- --------  \")\n",
    "    \n",
    "    # How many Clinical Trials are there? Well, it's different depending on the Conditions or Interventions dataframes...\n",
    "    conditions_nctids = len(df_dict[\"conditions\"].nct_id.unique())\n",
    "    interventions_nctids = len(df_dict[\"interventions\"].nct_id.unique())\n",
    "    print(\"Number of Clinical Trials NCITs in Conditions table: {}\".format(conditions_nctids))      \n",
    "    print(\"Number of Clinical Trials NCITs in Interventions table: {}\".format(interventions_nctids))\n",
    "    \n",
    "    \"\"\" create tables of unused MeSH and MetaMap CURIEs that could be used for unmapped Conditions and Interventions \"\"\"\n",
    "    \n",
    "    # -------    CONDITIONS    ------- #\n",
    "    all_conditions = df_dict[\"conditions\"][[\"nct_id\", \"downcase_name\"]]\n",
    "    conditions_mesh = pd.merge(all_conditions, \n",
    "                               mesh_conditions_per_study,\n",
    "                               how='left',\n",
    "                               left_on=['nct_id'],\n",
    "                               right_on = ['nct_id'])\n",
    "    \n",
    "    metamap_possibilities = remaining_unmapped_possible[\"all_metamapped_conditions\"][[\"condition_input\", \"condition_CURIE_id\", \"condition_CURIE_name\", \"condition_semantic_type\"]]\n",
    "    conditions_mesh_metamap = pd.merge(conditions_mesh, \n",
    "                                       metamap_possibilities,\n",
    "                                       how='left',\n",
    "                                       left_on=['downcase_name'],\n",
    "                                       right_on = ['condition_input'])\n",
    "    \n",
    "    unmapped_conditions_possible_terms = conditions_mesh_metamap[conditions_mesh_metamap['downcase_name'].isin(ct_terms[\"unmapped_conditions\"])]\n",
    "    unmapped_conditions_possible_terms = unmapped_conditions_possible_terms.drop('condition_input', axis=1) # drop the empty column now\n",
    "    \n",
    "    # -------    INTERVENTIONS    ------- #\n",
    "    all_interventions = df_dict[\"interventions\"][[\"nct_id\", \"downcase_name\"]]\n",
    "    interventions_mesh = pd.merge(all_interventions, \n",
    "                               mesh_interventions_per_study,\n",
    "                               how='left',\n",
    "                               left_on=['nct_id'],\n",
    "                               right_on = ['nct_id'])\n",
    "    \n",
    "    metamap_possibilities = remaining_unmapped_possible[\"all_metamapped_interventions\"][[\"intervention_input\", \"intervention_CURIE_id\", \"intervention_CURIE_name\", \"intervention_semantic_type\"]]\n",
    "    interventions_mesh_metamap = pd.merge(interventions_mesh, \n",
    "                                       metamap_possibilities,\n",
    "                                       how='left',\n",
    "                                       left_on=['downcase_name'],\n",
    "                                       right_on = ['intervention_input'])\n",
    "    \n",
    "    unmapped_interventions_possible_terms = interventions_mesh_metamap[interventions_mesh_metamap['downcase_name'].isin(ct_terms[\"unmapped_interventions\"])]\n",
    "    unmapped_interventions_possible_terms = unmapped_interventions_possible_terms.drop('intervention_input', axis=1) # drop the empty column now\n",
    "          \n",
    "        \n",
    "    \"\"\"   Output all to TSVs   \"\"\"    \n",
    "    ct_terms[\"unmapped_conditions\"].to_csv('unmapped_conditions.tsv', sep=\"\\t\")\n",
    "    ct_terms[\"unmapped_interventions\"].to_csv('unmapped_interventions.tsv', sep=\"\\t\")\n",
    "    ct_terms[\"mapped_conditions\"].to_csv('mapped_conditions.tsv', sep=\"\\t\")\n",
    "    ct_terms[\"mapped_interventions\"].to_csv('mapped_interventions.tsv', sep=\"\\t\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caae664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e0830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87989a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f955a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9729a4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79579f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7479369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611b5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221a21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a2fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16519252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda3165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ea53e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique conditions in this Clinical Trials data dump: 1219\n",
      "Number of unique conditions that have an exact MeSH term match given in this dump: 264\n",
      "since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique conditions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: 955\n",
      "Number of unique interventions in this Clinical Trials data dump: 1702\n",
      "Number of unique interventions that have an exact MeSH term match given in this dump: 163\n",
      "since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique interventions that are unmapped after finding exact MeSH mappings and using Name Resolver to get CURIES: 1539\n",
      "Use fuzzy matching on MeSH terms from Clinical Trials dump to find more potential matches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kamileh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique conditions mapped using fuzzy matching to MeSH terms: 90\n",
      "Number of unique conditions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: 955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kamileh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the MeSH terms don't come with identifers, we retrieve them from Name Resolver\n",
      "Number of unique interventions mapped using fuzzy matching to MeSH terms: 11\n",
      "Number of unique interventions that are unmapped after finding fuzzy MeSH mappings and using Name Resolver to get CURIES: 1539\n",
      "Number of unique conditions mapped using Name Resolver: 888\n",
      "Number of unique conditions that are unmapped after using Name Resolver: 67\n",
      "Number of unique interventions mapped using Name Resolver: 627\n",
      "Number of unique interventions that are unmapped after using Name Resolver: 912\n",
      "Using UMLS MetaMap to get more mappings for conditions. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "Number of unique conditions that are mapped after using MetaMap and similarity and ratio score thresholds of 88: 9\n",
      "Number of unique conditions that are unmapped after using MetaMap and similarity and ratio score thresholds of 88: 61\n",
      "Using UMLS MetaMap to get more mappings for interventions. MetaMap returns mappings, CUIs, and semantic type of mapping.\n",
      "Number of unique interventions that are mapped after using MetaMap and similarity and ratio score thresholds of 88: 34\n",
      "Number of unique interventions that are unmapped after using MetaMap and similarity and ratio score thresholds of 88: 892\n"
     ]
    }
   ],
   "source": [
    "# flag_and_path = get_raw_ct_data() # uncomment for production\n",
    "flag_and_path = {'term_program_flag': False, 'data_extracted_path': '/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/06_27_2023_extracted'} # comment for production\n",
    "df_dict = read_raw_ct_data(flag_and_path)\n",
    "ct_terms = exact_match_mesh(df_dict)\n",
    "ct_terms = inexact_match_mesh(df_dict, ct_terms)\n",
    "\n",
    "# pull the available MeSH terms per study out of the returned ct_terms dict \n",
    "mesh_conditions_per_study = ct_terms[\"mesh_conditions_per_study\"]\n",
    "mesh_interventions_per_study = ct_terms[\"mesh_interventions_per_study\"]\n",
    "\n",
    "ct_terms = term_list_to_nr(df_dict, ct_terms)\n",
    "ct_terms = term_list_to_mm(df_dict, ct_terms)\n",
    "\n",
    "# pull the available UMLS terms per study out of the returned ct_terms dict \n",
    "all_metamapped_conditions = ct_terms[\"all_metamapped_conditions\"]\n",
    "all_metamapped_interventions = ct_terms[\"all_metamapped_interventions\"]\n",
    "\n",
    "remaining_unmapped_possible = {\"mesh_conditions_per_study\": mesh_conditions_per_study,\n",
    "                               \"mesh_interventions_per_study\": mesh_interventions_per_study,\n",
    "                               \"all_metamapped_conditions\": all_metamapped_conditions,\n",
    "                               \"all_metamapped_interventions\": all_metamapped_interventions}\n",
    "# compile_and_output(df_dict, ct_terms, remaining_unmapped_possible)\n",
    "\n",
    "# # for the remaining unmapped terms, collect UMLS and MeSH terms available for them that may not have passed threshold scoring filter\n",
    "# # collect_available_terms(remaining_unmapped_possible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd936ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_unmapped_possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
