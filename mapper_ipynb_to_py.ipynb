{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da527b7e-2d30-4660-b6ee-1a63bc9df48f",
   "metadata": {},
   "source": [
    "### THIS SCRIPT USES MetaMap to try and map the bulk of terms, and Name Resolver to pick up what's left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730291b8-7f12-4e72-bfa6-4c7ae33ae738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "# lets you preint multiple outputs per cell, not just last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b0f433-c7bb-4b98-aa7b-1d1e6ad1e3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "# import concurrent\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import sys\n",
    "import urllib\n",
    "import zipfile\n",
    "import csv\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "# %pip install ratelimit\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "# 40 calls per minute\n",
    "CALLS = 40\n",
    "RATE_LIMIT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d4d831-06e5-4439-920e-b6495d81adc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token_sort_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_seconds_to_hms(seconds):\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'start']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'start']\n",
    "\n",
    "    # Start servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'stop']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'stop']\n",
    "    \n",
    "    # Stop servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(2)  \n",
    "    \n",
    "def add_manually_selected_terms_to_cache():\n",
    "    # -----     ------     GENERATE MANUALLY SELECTED CACHE     -----     ------  #\n",
    "    try:\n",
    "        #  --- --- --   CONDITIONS     --- --- --   #\n",
    "        files = glob.glob(\"*.xlsx\")\n",
    "        conditions_manselected_files = [i for i in files if \"conditions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "        conditions_manselected = pd.read_excel(conditions_manselected_files)\n",
    "        conditions_manselected.name.ffill(inplace=True)\n",
    "        conditions_manselected.orig_con.ffill(inplace=True)\n",
    "        conditions_manselected = conditions_manselected[~conditions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "        conditions_manselected.drop([\"curie_info\"], axis = 1, inplace = True)\n",
    "        conditions_manselected.rename(columns = {'name':'original_clin_trial_term', 'orig_con':'modified_clin_trial_term'}, inplace = True)\n",
    "\n",
    "        with open('conditions_manually_selected_cache.tsv', 'a') as output:\n",
    "            conditions_manselected.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "        \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "        cache = pd.read_csv(\"conditions_manually_selected_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        cache = cache.drop_duplicates()\n",
    "        cache.to_csv('conditions_manually_selected_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "\n",
    "        #  --- --- --   INTERVENTIONS and Alternate INTERVENTIONS   --- --- --   #\n",
    "        files = glob.glob(\"*.xlsx\")\n",
    "        interventions_manselected_files = [i for i in files if \"interventions_manual_review\" in i if not i.startswith(\"~\")][0]  \n",
    "        interventions_manselected = pd.read_excel(interventions_manselected_files)\n",
    "        interventions_manselected.name.ffill(inplace=True)\n",
    "        interventions_manselected.orig_int.ffill(inplace=True)\n",
    "        interventions_manselected = interventions_manselected[~interventions_manselected['manually_selected_CURIE'].isnull()] # check if the conditions got mapped to any CURIEs\n",
    "        interventions_manselected.drop([\"curie_info\", \"description\"], axis = 1, inplace = True)\n",
    "        interventions_manselected.rename(columns = {'name':'original_clin_trial_term', 'orig_int':'modified_clin_trial_term'}, inplace = True)\n",
    "\n",
    "        with open('interventions_manually_selected_cache.tsv', 'a') as output:\n",
    "            interventions_manselected.to_csv(output, mode='a',sep=\"\\t\", index=False, header=output.tell()==0)\n",
    "        \"\"\" Remove duplicate rows from cache \"\"\"\n",
    "        cache = pd.read_csv(\"interventions_manually_selected_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        cache = cache.drop_duplicates()\n",
    "        cache.to_csv('interventions_manually_selected_cache.tsv', sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "    except:\n",
    "        print(\"No terms in manual select column; either column is empty or bug. Proceeding without them\")\n",
    "        \n",
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir} \n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS, period=RATE_LIMIT)\n",
    "def check_limit():\n",
    "    ''' Empty function just to check for calls to Name Resolver API '''\n",
    "    return\n",
    "\n",
    "def wrap(x): # use this to convert string objects to dicts \n",
    "    try:\n",
    "        a = ast.literal_eval(x)\n",
    "        return(a)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4f5d88-4ac8-4e12-b160-b576db13726d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def timeout(seconds=20, default=None):\n",
    "#     \"\"\" https://stackoverflow.com/questions/75928586/how-to-stop-the-execution-of-a-function-in-python-after-a-certain-time \"\"\"\n",
    "#     def decorator(func):\n",
    "#         @functools.wraps(func)\n",
    "#         def wrapper(*args, **kwargs):\n",
    "#             def handle_timeout(signum, frame):\n",
    "#                 raise TimeoutError()\n",
    "#             signal.signal(signal.SIGALRM, handle_timeout)\n",
    "#             signal.alarm(seconds)\n",
    "#             result = func(*args, **kwargs)\n",
    "#             signal.alarm(0)\n",
    "#             return result\n",
    "#         return wrapper\n",
    "#     return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6746b051-2524-4d93-ad30-69e7fcd225e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    try:\n",
    "        # get all the links and associated dates of upload into a dict called date_link\n",
    "        url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "        response = requests.get(url_all)\n",
    "        soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "        body = soup.find_all('option') #Find all\n",
    "        date_link = {}\n",
    "        for el in body:\n",
    "            tags = el.find('a')\n",
    "            try:\n",
    "                zip_name = tags.contents[0].split()[0]\n",
    "                date = zip_name.split(\"_\")[0]\n",
    "                date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "                date_link[date] = tags.get('href')\n",
    "            except:\n",
    "                pass\n",
    "        latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "        url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "        date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "        data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "        data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "        data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    except:\n",
    "        print(\"continue\")\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Attempting download of Clinical Trial data as of {}\\n\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                # list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    # latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    pattern = os.path.join(data_dir, \"*.zip\")\n",
    "                    zip_file = glob.glob(pattern) # look for file in directory that ends in \".zip\"\n",
    "                    zip_file = zip_file[0]\n",
    "                    print(\"File found at: \")\n",
    "                    print(zip_file)\n",
    "                    # print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    if not os.path.exists(data_extracted):   # if folder of unzipped data does not exist, unzip\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_file, 'r') as download:\n",
    "                                print(\"Unzipping data into\")\n",
    "                                cttime = os.path.getctime(zip_file)\n",
    "                                date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                                data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                                print(data_extracted)\n",
    "                                download.extractall(data_extracted)\n",
    "                        except:\n",
    "                            pattern = os.path.join(data_dir, \"*_extracted\")\n",
    "                            extracted_file = glob.glob(pattern) # look for file in directory that ends in \"_extracted\"\n",
    "                            data_extracted = extracted_file[0]\n",
    "                            extracted_name = os.path.basename(os.path.normpath(extracted_file[0]))\n",
    "                            date_string = extracted_name.replace('_extracted', '')\n",
    "                            print(\"Assuming data is already unzipped\")\n",
    "                        \n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4850c49-b00e-422a-805b-137fccb7cd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            interventions_alts_df = interventions_alts_df.sample(n=subset_size)\n",
    "    \n",
    "    df_dict = {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd612efd-e483-4754-8edd-3dc45fe95e88",
   "metadata": {},
   "source": [
    "# Check against cache, retrieve terms not already mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eddc0c83-10b4-4065-b2bb-1bad9cb0e63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_against_cache(df_dict):\n",
    "    conditions_list = df_dict['conditions'].name.unique().tolist()\n",
    "    conditions_list = [str(i) for i in conditions_list]\n",
    "    conditions_list = list(set([i.lower() for i in conditions_list]))\n",
    "    \n",
    "    interventions_list = df_dict['interventions'].name.unique().tolist()\n",
    "    interventions_list = [str(i) for i in interventions_list]\n",
    "    interventions_list = list(set([i.lower() for i in interventions_list]))\n",
    "    \n",
    "    interventions_alts_list = df_dict['interventions_alts'].name.unique().tolist()\n",
    "    interventions_alts_list = [str(i) for i in interventions_alts_list]\n",
    "    interventions_alts_list = list(set([i.lower() for i in interventions_alts_list]))\n",
    "    \n",
    "    try:        \n",
    "        cache_df = pd.read_csv(\"mapping_cache.tsv\", sep =\"\\t\", index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        \n",
    "        conditions_cache = cache_df[cache_df[\"term_type\"] == \"condition\"]\n",
    "        conditions_cache = conditions_cache['clintrial_term'].unique().tolist()\n",
    "        conditions_cache = list(set([i.lower() for i in conditions_cache]))\n",
    "        \n",
    "        conditions_new = [x for x in conditions_list if x not in conditions_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "        conditions_new = list(filter(None, conditions_new))\n",
    "        conditions_new = [str(i) for i in conditions_new]\n",
    "        \n",
    "        interventions_cache = cache_df[cache_df[\"term_type\"] == \"intervention\"]\n",
    "        interventions_cache = interventions_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_cache = list(set([i.lower() for i in interventions_cache]))\n",
    "        \n",
    "        interventions_new = [x for x in interventions_list if x not in interventions_cache] # find interventions not in the cache (i.g. new interventions to map)\n",
    "        interventions_new = list(filter(None, interventions_new))\n",
    "        interventions_new = [str(i) for i in interventions_new]\n",
    "        \n",
    "        interventions_alts_cache = cache_df[cache_df[\"term_type\"] == \"intervention_alternate\"]\n",
    "        interventions_alts_cache = interventions_alts_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_alts_cache = list(set([i.lower() for i in interventions_alts_cache]))\n",
    "        \n",
    "        interventions_alts_new = [x for x in interventions_alts_list if x not in interventions_alts_cache] # find interventions_alts not in the cache (i.g. new interventions_alts to map)\n",
    "        interventions_alts_new = list(filter(None, interventions_alts_new))\n",
    "        interventions_alts_new = [str(i) for i in interventions_alts_new]\n",
    "        \n",
    "    except:\n",
    "        print(\"No cache of terms found. Proceeding to map entire KG from scratch\")\n",
    "        conditions_new = conditions_list\n",
    "        interventions_new = interventions_list\n",
    "        interventions_alts_new = interventions_alts_list\n",
    "        \n",
    "    dict_new_terms = {\"conditions\": conditions_new, \"interventions\": interventions_new, \"interventions_alts\": interventions_alts_new}\n",
    "\n",
    "    return dict_new_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a77e9-7ead-4ee6-a4df-d2aa636cc893",
   "metadata": {},
   "source": [
    "# Map new terms using Mapper function (MetaMap + Name Resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1c310c-1087-4d72-bef2-53f535f92e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nr_response(orig_term):\n",
    "    def create_session():\n",
    "        s = requests.Session()\n",
    "        return s\n",
    " \n",
    "    sess = create_session()\n",
    " \n",
    "    \"\"\"   Runs Name Resolver   \"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    max_retries = 3 \n",
    "    \n",
    "    input_term = orig_term # in MetaMap, we have to potentially deascii the term and lower case it...for Name Resolver, we don't need to do that. To keep columns consist with MetaMap output, we just keep it and say the original term and the input term are the same. For MetaMap, they might be different\n",
    "    retries = 0\n",
    "    params = {'string':orig_term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give (deprecated)\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            r = sess.post(nr_url, params=params)\n",
    "            check_limit() # counts how many requests have been sent to NR. If limit of 40 have been sent, sleeps for 1 min\n",
    "            if r.status_code == 200:\n",
    "                mapping_tool_response = r.json()  # process Name Resolver response\n",
    "                return mapping_tool_response\n",
    "            else:\n",
    "                return None\n",
    "        except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "            print(f\"\\nName Resolver request failed for term: {term}. Error: {ex}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "                time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "            else:\n",
    "                print(f\"Max retries (Name Resolver) reached for term: {term}.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b60d026-6fdb-44bc-8cf2-6ae28d2edcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I'm only getting 1 concept from Name Resolver. \n",
    "# Both MetaMap and Name Resolver return several, \n",
    "# but I only take 1 from Name Resolver bc they have a preferred concept.\n",
    "# MetaMap's 2nd or 3rd result is often the best one, so I collect all of them and try to score\"\n",
    "\n",
    "def process_metamap_concept(concept):\n",
    "    concept = concept._asdict()\n",
    "    concept_dict  = {\"mapped_name\": concept.get(\"preferred_name\"),\n",
    "                     \"mapped_curie\": concept.get(\"cui\"),\n",
    "                     \"mapped_score\": concept.get(\"score\"),\n",
    "                     \"mapped_semtypes\": concept.get(\"semtypes\")}\n",
    "    if not concept.get(\"preferred_name\"): # if condition triggered if the concept dict looks like following, where AA is for Abbreviation.... {'index': 'USER', 'aa': 'AA', 'short_form': 'copd', 'long_form': 'chronic obstructive pulmonary disease', 'num_tokens_short_form': '1', 'num_chars_short_form': '4', 'num_tokens_long_form': '7', 'num_chars_long_form': '37', 'pos_info': '43:4'}\n",
    "        concept_dict = None\n",
    "    return concept_dict\n",
    "\n",
    "\n",
    "# def process_metamap_concept(concept):\n",
    "#     concept = concept._asdict()\n",
    "#     if concept.get(\"semtypes\"):\n",
    "#         print(concept.get(\"semtypes\"))\n",
    "#         \"\"\" Give long form name for semantic type instead of short form abbreviations \"\"\"\n",
    "#         sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "#         metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names, on_bad_lines = 'warn')\n",
    "#         sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict from MetaMap semtypes\n",
    "\n",
    "#         semtypes = concept.get(\"semtypes\")  # get semtypes\n",
    "#         semtypes = semtypes.strip('][').split(', ') # convert string representation of concepts to list \n",
    "        \n",
    "#         semtypes = [sem_type_dict.get(abbv) for abbv in semtypes]  # convert the abbreviations to long form\n",
    "        \n",
    "#         print(semtypes)\n",
    "#     concept_dict  = {\"mapped_name\": concept.get(\"preferred_name\"),\n",
    "#                      \"mapped_curie\": concept.get(\"cui\"),\n",
    "#                      \"mapped_score\": concept.get(\"score\"),\n",
    "#                      \"mapped_semtypes\": concept.get(\"semtypes\")}\n",
    "#     if not concept.get(\"preferred_name\"): # if condition triggered if the concept dict looks like following, where AA is for Abbreviation.... {'index': 'USER', 'aa': 'AA', 'short_form': 'copd', 'long_form': 'chronic obstructive pulmonary disease', 'num_tokens_short_form': '1', 'num_chars_short_form': '4', 'num_tokens_long_form': '7', 'num_chars_long_form': '37', 'pos_info': '43:4'}\n",
    "#         concept_dict = None\n",
    "#     return concept_dict\n",
    "\n",
    "def process_nameresolver_response(nr_response):              \n",
    "    nr_curie = nr_response[0][\"curie\"]\n",
    "    nr_name = nr_response[0][\"label\"]\n",
    "    nr_type = nr_response[0][\"types\"][0]\n",
    "    nr_score = nr_response[0][\"score\"]\n",
    "    concept_dict = {\"mapped_name\": nr_name,\n",
    "                    \"mapped_curie\": nr_curie,\n",
    "                    \"mapped_score\": nr_score,\n",
    "                    \"mapped_semtypes\": nr_type}\n",
    "    return concept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1292c371-6c17-4102-8f17-0f45846c23b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_mappers(term_pair, params, mm, term_type, csv_writer):\n",
    "\n",
    "    orig_term = term_pair[0]\n",
    "    input_term = term_pair[1]\n",
    "    from_mapper = []\n",
    "    \n",
    "    # Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "\n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],)\n",
    "                                                    \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term)\n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # Add None for score column, empty bc not scored yet\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "            \n",
    "    else:   # Else block triggered if mapping Interventions\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],) \n",
    "                                                   \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term) \n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict])\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "\n",
    "    for result in from_mapper:\n",
    "        # print(result)\n",
    "        result.append(\"unscored\")\n",
    "        # print(result)\n",
    "        csv_writer.writerow(result)\n",
    "    # return from_metamap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63330b9a-2d52-431f-b00e-9609192bc21c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def run_mappers(term_pair, params, mm, term_type, csv_writer):\n",
    "\n",
    "#     orig_term = term_pair[0]\n",
    "#     input_term = term_pair[1]\n",
    "#     from_mapper = []\n",
    "    \n",
    "#     # Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "\n",
    "#     if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "#         try:\n",
    "#             concepts,error = mm.extract_concepts([input_term],\n",
    "#                                                  restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "#                                                  term_processing = params[\"term_processing\"],\n",
    "#                                                  ignore_word_order = params[\"ignore_word_order\"],\n",
    "#                                                  strict_model = params[\"strict_model\"],)\n",
    "                                                    \n",
    "#             if concepts:   # if MetaMap gives response, process response\n",
    "#                 mapping_tool = \"metamap\"\n",
    "#                 for concept in concepts:\n",
    "#                     concept_info = []\n",
    "#                     new_concept_dict = process_metamap_concept(concept)\n",
    "#                     concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "#                     from_mapper.append(concept_info)\n",
    "#             else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "#                 nr_response = get_nr_response(orig_term)\n",
    "#                 if nr_response: # if Name Resolver gives response, process repsonse\n",
    "#                     input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "#                     mapping_tool = \"nameresolver\"\n",
    "#                     concept_info = []\n",
    "#                     new_concept_dict = process_nameresolver_response(nr_response)\n",
    "#                     concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # Add None for score column, empty bc not scored yet\n",
    "#                     from_mapper.append(concept_info)\n",
    "#                 else:\n",
    "#                     concept_info = []\n",
    "#                     # print(\"Nothing returned from NR or Metamap\")\n",
    "#                     concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "#                     from_mapper.append(concept_info)\n",
    "#         except:\n",
    "#             concept_info = []\n",
    "#             # print(\"Nothing returned from NR or Metamap\")\n",
    "#             concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "#             from_mapper.append(concept_info)\n",
    "            \n",
    "#     else:   # Else block triggered if mapping Interventions\n",
    "#         try:\n",
    "#             concepts,error = mm.extract_concepts([input_term],\n",
    "#                                                  exclude_sts = params[\"exclude_sts\"],\n",
    "#                                                  term_processing = params[\"term_processing\"],\n",
    "#                                                  ignore_word_order = params[\"ignore_word_order\"],\n",
    "#                                                  strict_model = params[\"strict_model\"],) \n",
    "                                                   \n",
    "#             if concepts:   # if MetaMap gives response, process response\n",
    "#                 mapping_tool = \"metamap\"\n",
    "#                 for concept in concepts:\n",
    "#                     concept_info = []\n",
    "#                     new_concept_dict = process_metamap_concept(concept)\n",
    "#                     concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "#                     from_mapper.append(concept_info)\n",
    "#             else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "#                 nr_response = get_nr_response(orig_term) \n",
    "#                 if nr_response: # if Name Resolver gives response, process repsonse\n",
    "#                     input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "#                     mapping_tool = \"nameresolver\"\n",
    "#                     concept_info = []\n",
    "#                     new_concept_dict = process_nameresolver_response(nr_response)\n",
    "#                     concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict])\n",
    "#                     from_mapper.append(concept_info)\n",
    "#                 else:\n",
    "#                     concept_info = []\n",
    "#                     # print(\"Nothing returned from NR or Metamap\")\n",
    "#                     concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "#                     from_mapper.append(concept_info)\n",
    "#         except:\n",
    "#             concept_info = []\n",
    "#             # print(\"Nothing returned from NR or Metamap\")\n",
    "#             concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "#             from_mapper.append(concept_info)\n",
    "\n",
    "#     for result in from_mapper:\n",
    "#         # print(result)\n",
    "#         result.append(\"unscored\")\n",
    "#         # print(result)\n",
    "#         csv_writer.writerow(result)\n",
    "#     # return from_metamap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485fd6bb-2e4a-40f5-bbc9-e92af2c16a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallelize_mappers(term_pair_list, params, term_type, csv_writer):\n",
    "\n",
    "    LENGTH = len(term_pair_list)  # Number of iterations required to fill progress bar (pbar)\n",
    "    pbar = tqdm(total=LENGTH, desc=\"% {}s mapped\".format(term_type), position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "\n",
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "    with concurrent.futures.ThreadPoolExecutor((multiprocessing.cpu_count()*2) - 1) as executor:\n",
    "        futures = [executor.submit(run_mappers, term_pair, params, mm, term_type, csv_writer) for term_pair in term_pair_list]\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            pbar.update(n=1)  # Increments counter\n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f79ca09-cac5-4f30-a262-ff86690265e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def term_list_to_mappers(dict_new_terms):   \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "    \n",
    "    # open mapping cache to add mapped terms\n",
    "    mapping_filename = \"mapping_cache.tsv\"\n",
    "    if os.path.exists(mapping_filename):\n",
    "        output = open(mapping_filename, 'a', newline='', encoding=\"utf-8\") \n",
    "        csv_writer = csv.writer(output, delimiter='\\t')\n",
    "    else:\n",
    "        output = open(mapping_filename, 'w+', newline='')\n",
    "        col_names = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "        # col_names = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response']\n",
    "        csv_writer = csv.writer(output, delimiter='\\t')\n",
    "        csv_writer.writerow(col_names)\n",
    "\n",
    "    #  - Conditions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    conditions = dict_new_terms.get(\"conditions\")\n",
    "    condition_params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # conditon_term_type = \"condition\"\n",
    "\n",
    "    #  - Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions = dict_new_terms.get(\"interventions\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # intervention_term_type = \"intervention\"\n",
    "\n",
    "    #  - Alternate Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions_alts = dict_new_terms.get(\"interventions_alts\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # intervention_alternate_term_type = \"intervention_alternate\"\n",
    "    \n",
    "    if metamap_version[0] >= 20:\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        parallelize_mappers(list(zip(conditions, conditions)), condition_params, \"condition\", csv_writer)\n",
    "        parallelize_mappers(list(zip(interventions, interventions)), intervention_params, \"intervention\", csv_writer)\n",
    "        parallelize_mappers(list(zip(interventions_alts, interventions_alts)), intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        deascii_cons = deasciier(conditions)\n",
    "        deascii_ints = deasciier(interventions)\n",
    "        deascii_int_alts = deasciier(interventions_alts)\n",
    "        parallelize_mappers(list(zip(conditions, deascii_cons)), condition_params, \"condition\", csv_writer)\n",
    "        parallelize_mappers(list(zip(interventions, deascii_ints)), intervention_params, \"intervention\", csv_writer)\n",
    "        parallelize_mappers(list(zip(interventions_alts, deascii_int_alts)), intervention_params, \"intervention_alternate\", csv_writer)\n",
    "\n",
    "    output.close()\n",
    "    \n",
    "    # \"\"\" Remove duplicate rows \"\"\"\n",
    "    cache = pd.read_csv(mapping_filename, sep='\\t', index_col=False, header=0)\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv(mapping_filename, sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30b40eb7-50af-40e8-8fc8-8cc4e506285c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_mappings():\n",
    "\n",
    "    def get_max_score(str1, str2):\n",
    "        try:\n",
    "            sortratio_score = get_token_sort_ratio(str1, str2)\n",
    "            similarity_score = get_similarity_score(str1, str2)\n",
    "            max_score = max(sortratio_score, similarity_score)\n",
    "            return max_score\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def wrap(x): # use this to convert string objects to dicts \n",
    "        try:\n",
    "            a = ast.literal_eval(x)\n",
    "            return(a)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', usecols=lambda c: not c.startswith('Unnamed:'), chunksize=1000) as reader:\n",
    "        write_header = True\n",
    "        for chunk in reader:\n",
    "            chunk[\"mapping_tool_response\"] = chunk[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "            mapping_info = chunk[\"mapping_tool_response\"].apply(pd.Series)\n",
    "            chunk[\"mapped_name\"] = mapping_info[\"mapped_name\"]\n",
    "            chunk[\"score\"] = chunk.apply(lambda x: int(get_max_score(x['input_term'], x['mapped_name'])) if x[\"mapping_tool\"] != \"mapping_tools_failed\" else 0, axis=1) # get score for score rows that are empty/not scored yet\n",
    "            # chunk[\"score\"] = chunk.apply(lambda x:get_max_score(x['input_term'], x['mapped_name']) if x[\"score\"] == \"unscored\" else x, axis=1) # DOES NOT WORK get score for score rows that are empty/not scored yet\n",
    "            chunk.drop([\"mapped_name\"], axis = 1, inplace = True)\n",
    "            chunk.to_csv(f'mapping_cache_scored_temp.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a', encoding=\"utf-8\") # output to TSV\n",
    "            write_header = False\n",
    "\n",
    "    os.rename('mapping_cache.tsv','mapping_cache_backup.tsv')       \n",
    "    os.rename('mapping_cache_scored_temp.tsv','mapping_cache.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee8fa952-8f98-4f6f-ad84-1668c137bb59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def output_terms_files():\n",
    "\n",
    "    \"\"\"   Get high scorers   \"\"\"\n",
    "    cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "    highscorers = cache[cache['score'] >= 80] \n",
    "    # test = highscorers.groupby('clintrial_term')\n",
    "    idx = highscorers.groupby('clintrial_term')['score'].idxmax()  # group by the clinical trial term and get the highest scoring\n",
    "    auto_selected = highscorers.loc[idx]\n",
    "    auto_selected.to_csv(f'autoselected_terms.tsv', sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    \"\"\"   Get low scorers, aggregate for manual selections  \"\"\"\n",
    "    low_scorers = cache[cache['score'] < 80]\n",
    "    manual_review = low_scorers[~low_scorers.clintrial_term.isin(highscorers['clintrial_term'].unique().tolist())] # there are terms autoselected that have mappings that didn't pass threshold too, but we want to consider that term mapped. So get rid of these rows too\n",
    "    mapping_tool_response = manual_review['mapping_tool_response'].apply(lambda x: wrap(x))\n",
    "    manual_review = manual_review.copy()\n",
    "    mapping_tool_response = mapping_tool_response.apply(pd.Series)\n",
    "    manual_review.loc[:, 'mapping_tool_response_lists'] = mapping_tool_response.values.tolist()\n",
    "    manual_review.drop('mapping_tool_response', axis=1, inplace=True)\n",
    "    manual_review = manual_review[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"mapping_tool_response_lists\", \"input_term\", \"score\"]]\n",
    "    # manual_review['mapping_tool_response_lists'] = manual_review['mapping_tool_response_lists'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "    manual_review['mapping_tool_response'] = [' | '.join(map(str, l)) for l in manual_review['mapping_tool_response_lists']]\n",
    "    manual_review.drop('mapping_tool_response_lists', axis=1, inplace=True)\n",
    "\n",
    "    manual_review.set_index([\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], inplace=True)   # create index\n",
    "    # manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "    manual_review.to_excel('manual_review.xlsx', engine='xlsxwriter', index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# manual_review.loc[:, \"curie_info\"]\n",
    "\n",
    "\n",
    "# mapping_tool_response\n",
    "\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):  # more options can be specified also\n",
    "#     display(manual_review)\n",
    "\n",
    "\n",
    "\n",
    "# manual_review = manual_review.groupby(['mapping_tool', 'term_type', 'clintrial_term', 'input_term'])['mapping_tool_response'].agg(list).reset_index()\n",
    "# manual_review.set_index(['mapping_tool', 'term_type', 'clintrial_term'], inplace=True)   # create index\n",
    "# manual_review\n",
    "# manual_review['manually_selected_CURIE'] = None # make a column \n",
    "\n",
    "# manual_review.to_excel('manual_review.xlsx', engine='xlsxwriter', index=True)\n",
    "\n",
    "\n",
    "\n",
    "# manual_review.loc[:, \"curie_info\"] = manual_review.curie_info.apply(lambda x: convert_to_list(x)) # in order to multi-index, we have to group-by the original input term. To do this, first convert the curie_info column to list of lists\n",
    "#     manual_review = manual_review.explode('curie_info')  # explode that column so every sublist is on a separate row\n",
    "#     manual_review['curie_info'] = manual_review['curie_info'].apply(lambda x: x[:3] if isinstance(x, list) else None)   # remove the scores (sort_ratio and similarity score) from the list, don't need them and they compromise readability of manual outputs \n",
    "#     manual_review['curie_info'] = manual_review['curie_info'].apply(lambda x: '|--|'.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "\n",
    "#     manual_review['temp'] = \"temp\"   # create a temp column to facilitate multi-indexing\n",
    "#     manual_review.set_index([\"name\", \"orig_con\", \"curie_info\"], inplace=True)   # create index\n",
    "#     manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "#     manual_review['manually_selected_CURIE'] = None # make a column \n",
    "\n",
    "#     manual_review.to_excel('{}_conditions_manual_review.xlsx'.format(relevant_date), engine='xlsxwriter', index=True)\n",
    "    \n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):  # more options can be specified also\n",
    "#     display(test)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54099966-dc1d-40bb-aad1-c335d70de479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7340b4-b5fe-459c-913d-7f90eefe84c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ac648-8c24-4609-8f58-ba8a19445340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e77a6-e7f5-4add-8e5f-a45604155a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e880a9-2cd3-4f5d-87e2-ee39e6f9ae39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85690bf-7346-46a2-91f3-e59f9b9fd471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6c76a-6a0b-4458-9fea-ff8f506c5547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52134fe8-bf71-40e9-8447-48afccf4debf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f1196-7bc4-4522-92e2-d561202172a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "# metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names, on_bad_lines = 'warn')\n",
    "# sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "\n",
    "# cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "# cache[\"mapping_tool_response\"] = cache[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "\n",
    "# mapping_info = cache[\"mapping_tool_response\"].apply(pd.Series)\n",
    "# # mapping_info\n",
    "# cache[\"mapped_semtypes\"] = mapping_info[\"mapped_semtypes\"]\n",
    "# cache\n",
    "# # cache['test'] = cache['mapping_tool_response'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan) # Handle NaN (None) values in metamap_semantic_type column\n",
    "# # cache['metamap_semantic_type'] = cache['test'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan) # Handle NaN (None) values in metamap_semantic_type column\n",
    "# cache['metamap_semantic_type'] = cache['mapped_semtypes'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x) # map semantic type abbreviations to the full name of the semantic type\n",
    "\n",
    "# cache\n",
    "# # metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan) # Handle NaN (None) values in metamap_semantic_type column\n",
    "# # metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x) # map semantic type abbreviations to the full name of the semantic type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20468b35-416f-4359-9c23-d6b85c5d3ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "manual_review.groupby(['mapping_tool', 'term_type', 'clintrial_term', 'input_term'])['mapping_tool_response'].agg(list).reset_index().apply(display)\n",
    "# max_scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326ca1e-8bc0-447c-9b34-10d55b9c8a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49385d1f-1482-4870-84b4-1fd43fc3727d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120a580-67c1-4302-907a-b3dae2584805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf7d8b-891b-4512-b196-ee9481b42a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ec785-fd9c-494b-9eac-6f47f1163a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb7a8e-8666-476f-a735-24e8dffb0d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986668f-6776-4b5f-9f2c-6649d5852719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a2c38-6e03-4c42-bbd5-d93e3246e470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def convert_semantic_types(mm_semtype):\n",
    "#     print(mm_semtype)\n",
    "#     metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", on_bad_lines = 'warn') # get the full names of the semantic types so we know what we're looking at\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# convert_semantic_types(['inspo'])    \n",
    "sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names, on_bad_lines = 'warn')\n",
    "sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "mapping_info = cache[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "mapping_info = mapping_info.apply(pd.Series)\n",
    "cache[\"mapped_semantic_type\"] = mapping_info[\"mapped_semtypes\"]\n",
    "cache[\"mapped_semantic_type\"] = cache['mapped_semantic_type'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "# cache\n",
    "\n",
    "cache['mapped_semantic_type'] = cache['mapped_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan) # Handle NaN (None) values in metamap_semantic_type column\n",
    "cache['mapped_semantic_type'] = cache['mapped_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x) # map semantic type abbreviations to the full name of the semantic type\n",
    "\n",
    "\n",
    "cache\n",
    "\n",
    "# test = mapping_info.apply(lambda x: x['mapped_semtypes'])\n",
    "# test\n",
    "\n",
    "\n",
    "# cache['mapping_tool_response'].apply(lambda x: x['mapped_semtypes'])\n",
    "# cache\n",
    "# mapping_info = cache[\"mapping_tool_response\"].apply(pd.Series)\n",
    "# mapping_info        \n",
    "# cache[\"mapped_semantic_type\"] = mapping_info \n",
    "# cache             \n",
    "            # chunk[\"mapping_tool_response\"] = chunk[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "            # mapping_info = chunk[\"mapping_tool_response\"].apply(pd.Series)\n",
    "            # chunk[\"mapped_name\"] = mapping_info[\"mapped_name\"]\n",
    "\n",
    "\n",
    "# metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].str.replace(r'\\[|\\]', '', regex=True)\n",
    "# sem_type_col_names = [\"abbv\", \"group\", \"semantic_type_full\"]\n",
    "# metamap_semantic_types = pd.read_csv(\"MetaMap_SemanticTypes_2018AB.txt\", sep=\"|\", index_col=False, header=None, names=sem_type_col_names, on_bad_lines = 'warn')\n",
    "# sem_type_dict = dict(zip(metamap_semantic_types['abbv'], metamap_semantic_types['semantic_type_full'])) # make a dict of semantic type abbv and full name\n",
    "\n",
    "# metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: x.split(',') if isinstance(x, str) else np.nan) # Handle NaN (None) values in metamap_semantic_type column\n",
    "# metamapped['metamap_semantic_type'] = metamapped['metamap_semantic_type'].apply(lambda x: '|'.join([sem_type_dict[term] if term in sem_type_dict else term for term in x]) if isinstance(x, list) else x) # map semantic type abbreviations to the full name of the semantic type\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a06999-b360-4d9d-92f2-9ba87a033c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         manual_review.loc[:, \"curie_info\"] = manual_review.curie_info.apply(lambda x: convert_to_list(x)) # in order to multi-index, we have to group-by the original input term. To do this, first convert the curie_info column to list of lists\n",
    "\n",
    "#         sort_ratio = chunk[\"score\"].apply(lambda x: x.fillna('NA') if x.dtype == 'O' else x.fillna(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # def score_metamap_mappings():\n",
    "\n",
    "\n",
    "# header = True\n",
    "# with pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', chunksize=1000) as reader:\n",
    "#     for chunk in reader:\n",
    "#         chunk[\"scored\"] = np.where(~chunk[\"score\"].isnull(), chunk[\"score\"],\n",
    "#                                    np.where((chunk.score.isnull())&(chunk.mapping_tool == \"metamap\"),\n",
    "#         df['d'] = np.where(df.a.isnull(),\n",
    "#          np.nan,\n",
    "#          np.where((df.b == \"N\")&(~df.c.isnull()),\n",
    "#                   df.a*df.c,\n",
    "#                   df.a))\n",
    "        \n",
    "        \n",
    "#         for i, row in chunk.iterrows():\n",
    "#             print(type(row[\"score\"]))\n",
    "#             if not row[\"score\"].isnull():\n",
    "#                 print(i)\n",
    "# #                 break\n",
    "#             elif pd.isnull(row[\"score\"]) and row[\"mapping_tool\"] == \"metamap\":\n",
    "#                 mm_dict = ast.literal_eval(row[\"mapping_tool_response\"])\n",
    "#                 mapped_term = mm_dict['metamap_preferred_name']\n",
    "#                 sort_ratio_score = get_token_sort_ratio(row[\"clintrial_term\"], mapped_term)\n",
    "#                 similarity_score = get_similarity_score(row[\"clintrial_term\"], mapped_term)\n",
    "#                 max_score = max(sort_ratio_score, similarity_score)\n",
    "#                 chunk.at[i, \"score\"] = max_score\n",
    "#             elif pd.isnull(row[\"score\"]) and row[\"mapping_tool\"] == \"nameresolver\":\n",
    "#                 break\n",
    "            \n",
    "#             chunk.to_csv(\"mapping_cache_scored.tsv\", header=header, sep=\"\\t\", index=False, mode='a+')\n",
    "#             header = False\n",
    "\n",
    "#       header = True\n",
    "# for chunk in chunks:\n",
    "\n",
    "#     chunk.to_csv(os.path.join(folder, new_folder, \"new_file_\" + filename),\n",
    "#         header=header, cols=[['TIME','STUFF']], mode='a')\n",
    "\n",
    "#     header = False          \n",
    "                \n",
    "            \n",
    "# original_clintrial_term = row[\"clintrial_term\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ceddf5-0ebe-41d9-9b89-85fc647f4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download of Clinical Trial data as of 02_23_2024\n",
      "\n",
      "Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\n",
      "Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Done when done:  Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at: \n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/8vstm2enpo0ocbo2z7oypqurhgmz.zip\n",
      "Please make sure this the correct zip file from AACT\n",
      "Unzipping data into\n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/02_20_2024_extracted\n"
     ]
    }
   ],
   "source": [
    "flag_and_path = get_raw_ct_data() # download raw data\n",
    "global metamap_dirs\n",
    "metamap_dirs = check_os()\n",
    "subset_size = 20\n",
    "df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "dict_new_terms = check_against_cache(df_dict) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "term_list_to_mappers(dict_new_terms)\n",
    "score_mappings()\n",
    "output_terms_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9775bd6-feaf-4536-a7d9-0b981f00df6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a2122-0904-4b7c-938c-90ccf6027170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9297205-dc21-4f39-8e91-ad3f004ece85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c8425-0bc6-4908-b979-c388c0b2be2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7955a-24a6-4bc1-82b2-5d4c8d81b59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ddb07-a201-4573-8841-cb55a6ca0399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588dde2b-142d-4d88-9b6b-eb49c6fb5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flag_and_path = get_raw_ct_data() # download raw data\n",
    "\n",
    "global metamap_dirs\n",
    "metamap_dirs = check_os()\n",
    "df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "dict_new_terms = check_against_cache(df_dict, flag_and_path) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "\n",
    "term_list_to_mm(dict_new_terms, flag_and_path) # map new terms using MetaMap\n",
    "\n",
    "map_to_trial(flag_and_path) # map MetaMap terms back to trial \n",
    "score_mappings(flag_and_path) # score the mappings\n",
    "auto_select_curies(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "\n",
    "# compile_curies_for_trials(flag_and_path) # select CURIEs automatically that pass score threshold\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
