{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da527b7e-2d30-4660-b6ee-1a63bc9df48f",
   "metadata": {},
   "source": [
    "### THIS SCRIPT USES MetaMap to try and map the bulk of terms, and Name Resolver to pick up what's left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730291b8-7f12-4e72-bfa6-4c7ae33ae738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display cells to maximum width \n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "# lets you preint multiple outputs per cell, not just last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b0f433-c7bb-4b98-aa7b-1d1e6ad1e3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import time\n",
    "from time import sleep\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pathlib\n",
    "import configparser\n",
    "import sys\n",
    "import urllib\n",
    "import zipfile\n",
    "import csv\n",
    "sys.path.insert(0, '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/pymetamap-master')\n",
    "from pymetamap import MetaMap  # https://github.com/AnthonyMRios/pymetamap/blob/master/pymetamap/SubprocessBackend.py\n",
    "from pandas import ExcelWriter\n",
    "import ast\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import shlex\n",
    "from collections import Counter\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "# %pip install thefuzz\n",
    "# %pip install levenshtein\n",
    "# %pip install xlsxwriter\n",
    "# %pip install ratelimit\n",
    "\n",
    "from thefuzz import fuzz # fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python\n",
    "\n",
    "# 40 calls per minute\n",
    "CALLS = 40\n",
    "RATE_LIMIT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d4d831-06e5-4439-920e-b6495d81adc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token_sort_ratio(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.token_sort_ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_similarity_score(str1, str2):\n",
    "    \"\"\" fuzzy matching explained: https://www.datacamp.com/tutorial/fuzzy-string-python \"\"\"\n",
    "    try:\n",
    "        return fuzz.ratio(str1, str2)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def convert_seconds_to_hms(seconds):\n",
    "    \"\"\" converts the elapsed time or runtime to hours, min, sec \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return hours, minutes, seconds\n",
    "\n",
    "def de_ascii_er(text):\n",
    "    non_ascii = \"[^\\x00-\\x7F]\"\n",
    "    pattern = re.compile(r\"[^\\x00-\\x7F]\")\n",
    "    non_ascii_text = re.sub(pattern, ' ', text)\n",
    "    return non_ascii_text\n",
    "\n",
    "def start_metamap_servers(metamap_dirs):\n",
    "    global metamap_pos_server_dir\n",
    "    global metamap_wsd_server_dir\n",
    "    metamap_pos_server_dir = 'bin/skrmedpostctl' # Part of speech tagger\n",
    "    metamap_wsd_server_dir = 'bin/wsdserverctl' # Word sense disambiguation \n",
    "    \n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'start']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'start']\n",
    "\n",
    "    # Start servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(5)\n",
    "\n",
    "def stop_metamap_servers(metamap_dirs):\n",
    "    metamap_executable_path_pos = os.path.join(metamap_dirs['metamap_base_dir'], metamap_pos_server_dir)\n",
    "    metamap_executable_path_wsd = os.path.join(metamap_dirs['metamap_base_dir'], metamap_wsd_server_dir)\n",
    "    command_pos = [metamap_executable_path_pos, 'stop']\n",
    "    command_wsd = [metamap_executable_path_wsd, 'stop']\n",
    "    \n",
    "    # Stop servers, with open portion redirects output of metamap server printing output to NULL\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        result_post = subprocess.call(command_pos, stdout = fnull, stderr = fnull)\n",
    "        result_wsd = subprocess.call(command_wsd, stdout = fnull, stderr = fnull)\n",
    "    sleep(2)  \n",
    "    \n",
    "def check_os():\n",
    "    if \"linux\" in sys.platform:\n",
    "        print(\"Linux platform detected\")\n",
    "        metamap_base_dir = \"{}/metamap/\".format(pathlib.Path.cwd().parents[0])\n",
    "        metamap_bin_dir = 'bin/metamap20'\n",
    "    else:\n",
    "        metamap_base_dir = '/Volumes/TOSHIBA_EXT/ISB/clinical_trials/public_mm/' # for running on local\n",
    "        metamap_bin_dir = 'bin/metamap18'\n",
    "        \n",
    "    return {\"metamap_base_dir\":metamap_base_dir, \"metamap_bin_dir\":metamap_bin_dir} \n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=CALLS, period=RATE_LIMIT)\n",
    "def check_limit():\n",
    "    ''' Empty function just to check for calls to Name Resolver API '''\n",
    "    return\n",
    "\n",
    "def wrap(x): # use this to convert string objects to dicts \n",
    "    try:\n",
    "        a = ast.literal_eval(x)\n",
    "        return(a)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6746b051-2524-4d93-ad30-69e7fcd225e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_ct_data():\n",
    "    term_program_flag = True\n",
    "    global data_dir\n",
    "    global data_extracted\n",
    "    \n",
    "    try:\n",
    "        # get all the links and associated dates of upload into a dict called date_link\n",
    "        url_all = \"https://aact.ctti-clinicaltrials.org/download\"\n",
    "        response = requests.get(url_all)\n",
    "        soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "        body = soup.find_all('option') #Find all\n",
    "        date_link = {}\n",
    "        for el in body:\n",
    "            tags = el.find('a')\n",
    "            try:\n",
    "                zip_name = tags.contents[0].split()[0]\n",
    "                date = zip_name.split(\"_\")[0]\n",
    "                date = dt.datetime.strptime(date, '%Y%m%d').date()\n",
    "                date_link[date] = tags.get('href')\n",
    "            except:\n",
    "                pass\n",
    "        latest_file_date = max(date_link.keys())   # get the date of the latest upload\n",
    "        url = date_link[latest_file_date]   # get the corresponding download link of the latest upload so we can download the raw data\n",
    "        date_string = latest_file_date.strftime(\"%m_%d_%Y\")\n",
    "        data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "        data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "        data_path = \"{}/{}_pipe-delimited-export.zip\".format(data_dir, date_string)\n",
    "    except:\n",
    "        print(\"continue\")\n",
    "\n",
    "    if not os.path.exists(data_path):   # if folder containing most recent data doesn't exist, download and extract it into data folder\n",
    "\n",
    "        term_program_flag = False   # flag below for terminating program if latest download exists (KG is assumed up to date)\n",
    "        print(\"Attempting download of Clinical Trial data as of {}\\n\".format(date_string))\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                with open(data_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(\"Finished download of zip\")\n",
    "                with zipfile.ZipFile(data_path, 'r') as download:\n",
    "                    print(\"Unzipping data\")\n",
    "                    download.extractall(data_extracted)\n",
    "        except:\n",
    "            print(\"Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\")\n",
    "            print(\"Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\")\n",
    "            done = input(\"Type Done when done: \")\n",
    "            if done == \"Done\":\n",
    "                data_dir = \"{}/data\".format(pathlib.Path.cwd())\n",
    "                # list_of_files = glob.glob(data_dir + \"/*\") # get all files in directory\n",
    "                try:\n",
    "                    # latest_file = max(list_of_files, key=os.path.getctime) # get the most recent file in the directory\n",
    "                    pattern = os.path.join(data_dir, \"*.zip\")\n",
    "                    zip_file = glob.glob(pattern) # look for file in directory that ends in \".zip\"\n",
    "                    zip_file = zip_file[0]\n",
    "                    print(\"File found at: \")\n",
    "                    print(zip_file)\n",
    "                    # print(latest_file)\n",
    "                    print(\"Please make sure this the correct zip file from AACT\")\n",
    "                    if not os.path.exists(data_extracted):   # if folder of unzipped data does not exist, unzip\n",
    "                        try:\n",
    "                            with zipfile.ZipFile(zip_file, 'r') as download:\n",
    "                                print(\"Unzipping data into\")\n",
    "                                cttime = os.path.getctime(zip_file)\n",
    "                                date_string = dt.datetime.fromtimestamp(cttime).strftime('%m_%d_%Y')\n",
    "                                data_extracted = data_dir + \"/{}_extracted\".format(date_string)\n",
    "                                print(data_extracted)\n",
    "                                download.extractall(data_extracted)\n",
    "                        except:\n",
    "                            pattern = os.path.join(data_dir, \"*_extracted\")\n",
    "                            extracted_file = glob.glob(pattern) # look for file in directory that ends in \"_extracted\"\n",
    "                            data_extracted = extracted_file[0]\n",
    "                            extracted_name = os.path.basename(os.path.normpath(extracted_file[0]))\n",
    "                            date_string = extracted_name.replace('_extracted', '')\n",
    "                            print(\"Assuming data is already unzipped\")\n",
    "                        \n",
    "                except:\n",
    "                    print(\"Unable to download and extract Clincal Trial data.\")\n",
    "                    print(\"Cannot find pipe-delimited zip in /data folder.\")\n",
    "    else:\n",
    "        print(\"KG is already up to date.\")\n",
    "\n",
    "    return {\"term_program_flag\": term_program_flag, \"data_extracted_path\": data_extracted, \"date_string\": date_string}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4850c49-b00e-422a-805b-137fccb7cd94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_raw_ct_data(flag_and_path, subset_size):\n",
    "    if flag_and_path[\"term_program_flag\"]:\n",
    "        print(\"Exiting program. Assuming KG has already been constructed from most recent data dump from AACT.\")\n",
    "        exit()\n",
    "    else:\n",
    "        data_extracted = flag_and_path[\"data_extracted_path\"]\n",
    "        # read in pipe-delimited files \n",
    "        conditions_df = pd.read_csv(data_extracted + '/conditions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_df = pd.read_csv(data_extracted + '/interventions.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        interventions_alts_df = pd.read_csv(data_extracted + '/intervention_other_names.txt', sep='|', index_col=False, header=0, on_bad_lines = 'warn')\n",
    "\n",
    "        if subset_size:   # if a subset size is given, we are running this script on a small subset of the dataset\n",
    "            conditions_df = conditions_df.sample(n=subset_size)\n",
    "            interventions_df = interventions_df.sample(n=subset_size)\n",
    "            interventions_alts_df = interventions_alts_df.sample(n=subset_size)\n",
    "    \n",
    "    df_dict = {\"conditions\": conditions_df, \"interventions\": interventions_df, \"interventions_alts\": interventions_alts_df}\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd612efd-e483-4754-8edd-3dc45fe95e88",
   "metadata": {},
   "source": [
    "# Check against cache, retrieve terms not already mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddc0c83-10b4-4065-b2bb-1bad9cb0e63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_against_cache(df_dict):\n",
    "    \n",
    "    conditions_list = df_dict['conditions'].name.unique().tolist()\n",
    "    conditions_list = [str(i) for i in conditions_list]\n",
    "    conditions_list = list(set([i.lower() for i in conditions_list]))\n",
    "    \n",
    "    interventions_list = df_dict['interventions'].name.unique().tolist()\n",
    "    interventions_list = [str(i) for i in interventions_list]\n",
    "    interventions_list = list(set([i.lower() for i in interventions_list]))\n",
    "    \n",
    "    interventions_alts_list = df_dict['interventions_alts'].name.unique().tolist()\n",
    "    interventions_alts_list = [str(i) for i in interventions_alts_list]\n",
    "    interventions_alts_list = list(set([i.lower() for i in interventions_alts_list]))\n",
    "    \n",
    "    try:\n",
    "        cache_manually_selected_terms()\n",
    "    except:\n",
    "        print(\"No manually selected terms file found\")\n",
    "    \n",
    "    try:        \n",
    "        cache_df = pd.read_csv(\"mapping_cache.tsv\", sep =\"\\t\", index_col=False, header=0, on_bad_lines = 'warn')\n",
    "        \n",
    "        conditions_cache = cache_df[cache_df[\"term_type\"] == \"condition\"]\n",
    "        conditions_cache = conditions_cache['clintrial_term'].unique().tolist()\n",
    "        conditions_cache = list(set([i.lower() for i in conditions_cache]))\n",
    "        \n",
    "        conditions_new = [x for x in conditions_list if x not in conditions_cache] # find conditions not in the cache (i.g. new conditions to map)\n",
    "        conditions_new = list(filter(None, conditions_new))\n",
    "        conditions_new = [str(i) for i in conditions_new]\n",
    "        \n",
    "        interventions_cache = cache_df[cache_df[\"term_type\"] == \"intervention\"]\n",
    "        interventions_cache = interventions_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_cache = list(set([i.lower() for i in interventions_cache]))\n",
    "        \n",
    "        interventions_new = [x for x in interventions_list if x not in interventions_cache] # find interventions not in the cache (i.g. new interventions to map)\n",
    "        interventions_new = list(filter(None, interventions_new))\n",
    "        interventions_new = [str(i) for i in interventions_new]\n",
    "        \n",
    "        interventions_alts_cache = cache_df[cache_df[\"term_type\"] == \"intervention_alternate\"]\n",
    "        interventions_alts_cache = interventions_alts_cache['clintrial_term'].unique().tolist()\n",
    "        interventions_alts_cache = list(set([i.lower() for i in interventions_alts_cache]))\n",
    "        \n",
    "        interventions_alts_new = [x for x in interventions_alts_list if x not in interventions_alts_cache] # find interventions_alts not in the cache (i.g. new interventions_alts to map)\n",
    "        interventions_alts_new = list(filter(None, interventions_alts_new))\n",
    "        interventions_alts_new = [str(i) for i in interventions_alts_new]\n",
    "        \n",
    "    except:\n",
    "        print(\"No cache of terms found. Proceeding to map entire KG from scratch\")\n",
    "        conditions_new = conditions_list\n",
    "        interventions_new = interventions_list\n",
    "        interventions_alts_new = interventions_alts_list\n",
    "        \n",
    "    dict_new_terms = {\"conditions\": conditions_new, \"interventions\": interventions_new, \"interventions_alts\": interventions_alts_new}\n",
    "\n",
    "    return dict_new_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2745f5-583f-41be-9073-1b29760604b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cache_manually_selected_terms():    \n",
    "\n",
    "    def return_curie_dict(curie_info_delimited):\n",
    "        keys = [\"mapped_name\", \"mapped_curie\", \"mapped_score\", \"mapped_semtypes\"]\n",
    "        curie_list = curie_info_delimited.split(\" | \")\n",
    "        curie_dict = dict(zip(keys, curie_list))\n",
    "        return curie_dict\n",
    "\n",
    "    files = glob.glob(\"*.xlsx\")\n",
    "    manually_selected_file = [i for i in files if \"manual_review\" in i if not i.startswith(\"~\")][0] # find the file of manual selections\n",
    "    manually_selected = pd.read_excel(manually_selected_file)\n",
    "    cols_to_fill = [\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"]\n",
    "    manually_selected.loc[:,cols_to_fill] = manually_selected.loc[:,cols_to_fill].ffill()\n",
    "\n",
    "    manually_selected = manually_selected[~manually_selected['manually_selected_CURIE'].isnull()] # get rows where terms were manually chosen\n",
    "    manually_selected.drop([\"mapping_tool_response\"], axis = 1, inplace = True)\n",
    "\n",
    "    manually_selected[\"manually_selected_CURIE\"] = manually_selected[\"manually_selected_CURIE\"].apply(lambda x: return_curie_dict(x)) # convert | delimited strings to CURIE dict\n",
    "    manually_selected[\"score\"] = 1000  # human curated score = 1000\n",
    "    manually_selected.rename(columns = {'manually_selected_CURIE':'mapping_tool_response'}, inplace = True)\n",
    "    manually_selected = manually_selected[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\", \"mapping_tool_response\", \"score\"]] # reorder columns to be same as the cache files we're appending to \n",
    "    manually_selected.to_csv(\"mapping_cache.tsv\", mode='a', header=False, sep =\"\\t\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a77e9-7ead-4ee6-a4df-d2aa636cc893",
   "metadata": {},
   "source": [
    "# Map new terms using Mapper function (MetaMap + Name Resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1c310c-1087-4d72-bef2-53f535f92e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nr_response(orig_term):\n",
    "    def create_session():\n",
    "        s = requests.Session()\n",
    "        return s\n",
    " \n",
    "    sess = create_session()\n",
    " \n",
    "    \"\"\"   Runs Name Resolver   \"\"\"\n",
    "    nr_url = 'https://name-resolution-sri.renci.org/lookup'\n",
    "    max_retries = 3 \n",
    "    \n",
    "    input_term = orig_term # in MetaMap, we have to potentially deascii the term and lower case it...for Name Resolver, we don't need to do that. To keep columns consist with MetaMap output, we just keep it and say the original term and the input term are the same. For MetaMap, they might be different\n",
    "    retries = 0\n",
    "    params = {'string':orig_term, 'limit':1} # limit -1 makes this return all available equivalent CURIEs name resolver can give (deprecated)\n",
    "    while retries <= max_retries:\n",
    "        try:\n",
    "            r = sess.post(nr_url, params=params)\n",
    "            check_limit() # counts how many requests have been sent to NR. If limit of 40 have been sent, sleeps for 1 min\n",
    "            if r.status_code == 200:\n",
    "                mapping_tool_response = r.json()  # process Name Resolver response\n",
    "                return mapping_tool_response\n",
    "            else:\n",
    "                return None\n",
    "        except (requests.RequestException, ConnectionResetError, OSError) as ex:\n",
    "            print(f\"\\nName Resolver request failed for term: {term}. Error: {ex}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying ({retries}/{max_retries}) after a delay.\")\n",
    "                time.sleep(2 ** retries)  # Increase the delay between retries exponentially\n",
    "            else:\n",
    "                print(f\"Max retries (Name Resolver) reached for term: {term}.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b60d026-6fdb-44bc-8cf2-6ae28d2edcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I'm only getting 1 concept from Name Resolver. \n",
    "# Both MetaMap and Name Resolver return several, \n",
    "# but I only take 1 from Name Resolver bc they have a preferred concept.\n",
    "# MetaMap's 2nd or 3rd result is often the best one, so I collect all of them and try to score\"\n",
    "\n",
    "def process_metamap_concept(concept):\n",
    "    concept = concept._asdict()\n",
    "    concept_dict  = {\"mapped_name\": concept.get(\"preferred_name\"),\n",
    "                     \"mapped_curie\": concept.get(\"cui\"),\n",
    "                     \"mapped_score\": concept.get(\"score\"),\n",
    "                     \"mapped_semtypes\": concept.get(\"semtypes\")}\n",
    "    if not concept.get(\"preferred_name\"): # if condition triggered if the concept dict looks like following, where AA is for Abbreviation.... {'index': 'USER', 'aa': 'AA', 'short_form': 'copd', 'long_form': 'chronic obstructive pulmonary disease', 'num_tokens_short_form': '1', 'num_chars_short_form': '4', 'num_tokens_long_form': '7', 'num_chars_long_form': '37', 'pos_info': '43:4'}\n",
    "        concept_dict = None\n",
    "    return concept_dict\n",
    "\n",
    "def process_nameresolver_response(nr_response):              \n",
    "    nr_curie = nr_response[0][\"curie\"]\n",
    "    nr_name = nr_response[0][\"label\"]\n",
    "    nr_type = nr_response[0][\"types\"][0]\n",
    "    nr_score = nr_response[0][\"score\"]\n",
    "    concept_dict = {\"mapped_name\": nr_name,\n",
    "                    \"mapped_curie\": nr_curie,\n",
    "                    \"mapped_score\": nr_score,\n",
    "                    \"mapped_semtypes\": nr_type}\n",
    "    return concept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1292c371-6c17-4102-8f17-0f45846c23b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_mappers(term_pair, params, mm, term_type, csv_writer):\n",
    "\n",
    "    orig_term = term_pair[0]\n",
    "    input_term = term_pair[1]\n",
    "    from_mapper = []\n",
    "    \n",
    "    # Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "\n",
    "    if params.get(\"exclude_sts\") is None: # exclude_sts is used for Interventions. restrict_to_sts is used for Conditions. So, the logic is, if we're mapping Conditions, execute \"if\" part of code. If we're mapping Interventions, execute \"else\" part of code\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 restrict_to_sts = params[\"restrict_to_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],)\n",
    "                                                    \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term)\n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # Add None for score column, empty bc not scored yet\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "            \n",
    "    else:   # Else block triggered if mapping Interventions\n",
    "        try:\n",
    "            concepts,error = mm.extract_concepts([input_term],\n",
    "                                                 exclude_sts = params[\"exclude_sts\"],\n",
    "                                                 term_processing = params[\"term_processing\"],\n",
    "                                                 ignore_word_order = params[\"ignore_word_order\"],\n",
    "                                                 strict_model = params[\"strict_model\"],) \n",
    "                                                   \n",
    "            if concepts:   # if MetaMap gives response, process response\n",
    "                mapping_tool = \"metamap\"\n",
    "                for concept in concepts:\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_metamap_concept(concept)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict]) # score column is empty, Format of output TSV: header = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "                    from_mapper.append(concept_info)\n",
    "            else:   # if MetaMap fails, try using Name Resolver and process response\n",
    "                nr_response = get_nr_response(orig_term) \n",
    "                if nr_response: # if Name Resolver gives response, process repsonse\n",
    "                    input_term = orig_term # no preprocessing (lowercasing or deascii-ing) necessary to submit terms to Name Resolver (unlike MetaMap)\n",
    "                    mapping_tool = \"nameresolver\"\n",
    "                    concept_info = []\n",
    "                    new_concept_dict = process_nameresolver_response(nr_response)\n",
    "                    concept_info.extend([mapping_tool, term_type, orig_term, input_term, new_concept_dict])\n",
    "                    from_mapper.append(concept_info)\n",
    "                else:\n",
    "                    concept_info = []\n",
    "                    # print(\"Nothing returned from NR or Metamap\")\n",
    "                    concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "                    from_mapper.append(concept_info)\n",
    "        except:\n",
    "            concept_info = []\n",
    "            # print(\"Nothing returned from NR or Metamap\")\n",
    "            concept_info.extend([\"mapping_tools_failed\", term_type, orig_term, input_term, \"mapping_tools_failed\"])\n",
    "            from_mapper.append(concept_info)\n",
    "\n",
    "    for result in from_mapper:\n",
    "        # print(result)\n",
    "        if result[0] == \"mapping_tools_failed\":\n",
    "            result.append(-1)\n",
    "        else:\n",
    "            result.append(\"unscored\")\n",
    "        # print(result)\n",
    "        csv_writer.writerow(result)\n",
    "    # return from_metamap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485fd6bb-2e4a-40f5-bbc9-e92af2c16a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallelize_mappers(term_pair_list, params, term_type, csv_writer):\n",
    "\n",
    "    LENGTH = len(term_pair_list)  # Number of iterations required to fill progress bar (pbar)\n",
    "    pbar = tqdm(total=LENGTH, desc=\"% {}s mapped\".format(term_type), position=0, leave=True, mininterval = LENGTH/20, bar_format='{l_bar}{bar:20}{r_bar}{bar:-10b}')  # Init progress bar\n",
    "\n",
    "    start_metamap_servers(metamap_dirs) # start the MetaMap servers\n",
    "    mm = MetaMap.get_instance(metamap_dirs[\"metamap_base_dir\"] + metamap_dirs[\"metamap_bin_dir\"])\n",
    "    with concurrent.futures.ThreadPoolExecutor((multiprocessing.cpu_count()*2) - 1) as executor:\n",
    "        futures = [executor.submit(run_mappers, term_pair, params, mm, term_type, csv_writer) for term_pair in term_pair_list]\n",
    "        for _ in concurrent.futures.as_completed(futures):\n",
    "            pbar.update(n=1)  # Increments counter\n",
    "    stop_metamap_servers(metamap_dirs) # stop the MetaMap servers\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f79ca09-cac5-4f30-a262-ff86690265e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def term_list_to_mappers(dict_new_terms):   \n",
    "    metamap_version = [int(s) for s in re.findall(r'\\d+', metamap_dirs.get('metamap_bin_dir'))] # get MetaMap version being run \n",
    "    deasciier = np.vectorize(de_ascii_er) # vectorize function\n",
    "    \n",
    "    # open mapping cache to add mapped terms\n",
    "    mapping_filename = \"mapping_cache.tsv\"\n",
    "    if os.path.exists(mapping_filename):\n",
    "        output = open(mapping_filename, 'a', newline='', encoding=\"utf-8\") \n",
    "        csv_writer = csv.writer(output, delimiter='\\t')\n",
    "    else:\n",
    "        output = open(mapping_filename, 'w+', newline='', encoding='utf-8')\n",
    "        col_names = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response', 'score']\n",
    "        # col_names = ['mapping_tool', 'term_type', 'clintrial_term', 'input_term', 'mapping_tool_response']\n",
    "        csv_writer = csv.writer(output, delimiter='\\t')\n",
    "        csv_writer.writerow(col_names)\n",
    "\n",
    "    #  - Conditions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    conditions = dict_new_terms.get(\"conditions\")\n",
    "    condition_params = {\"restrict_to_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # conditon_term_type = \"condition\"\n",
    "\n",
    "    #  - Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions = dict_new_terms.get(\"interventions\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # intervention_term_type = \"intervention\"\n",
    "\n",
    "    #  - Alternate Interventions\n",
    "    condition_semantic_type_restriction = ['acab,anab,cgab,comd,dsyn,inpo,mobd,neop,patf,clna,fndg']  # see https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/Docs/SemanticTypes_2018AB.txt for semantic types (\"acab,anab,etc.\")\n",
    "    interventions_alts = dict_new_terms.get(\"interventions_alts\")\n",
    "    intervention_params = {\"exclude_sts\":condition_semantic_type_restriction, \"term_processing\":True, \"ignore_word_order\":True, \"strict_model\":False} # strict_model and relaxed_model are presumably opposites? relaxed_model = True is what I want, but that option appears to be broken in Pymetamap (returns no results when used). Using strict_model = False instead...\n",
    "    # intervention_alternate_term_type = \"intervention_alternate\"\n",
    "    \n",
    "    if metamap_version[0] >= 20:\n",
    "        print(\"MetaMap version >= 2020, conduct mapping on original terms\")\n",
    "        parallelize_mappers(list(zip(conditions, conditions)), condition_params, \"condition\", csv_writer)\n",
    "        parallelize_mappers(list(zip(interventions, interventions)), intervention_params, \"intervention\", csv_writer)\n",
    "        parallelize_mappers(list(zip(interventions_alts, interventions_alts)), intervention_alts_params, \"alternate_intervention\", csv_writer)\n",
    "    else:\n",
    "        print(\"MetaMap version < 2020, conduct mapping on terms after removing ascii characters\")\n",
    "        deascii_cons = deasciier(conditions)\n",
    "        deascii_ints = deasciier(interventions)\n",
    "        deascii_int_alts = deasciier(interventions_alts)\n",
    "        parallelize_mappers(list(zip(conditions, deascii_cons)), condition_params, \"condition\", csv_writer)\n",
    "        parallelize_mappers(list(zip(interventions, deascii_ints)), intervention_params, \"intervention\", csv_writer)\n",
    "        parallelize_mappers(list(zip(interventions_alts, deascii_int_alts)), intervention_params, \"intervention_alternate\", csv_writer)\n",
    "\n",
    "    output.close()\n",
    "    \n",
    "    # \"\"\" Remove duplicate rows \"\"\"\n",
    "    # cache = pd.read_csv(mapping_filename, sep='\\t', index_col=False, header=0, encoding = \"utf-8\")\n",
    "    cache = pd.read_csv(mapping_filename, sep='\\t', index_col=False, header=0, encoding_errors='ignore')\n",
    "    cache = cache.drop_duplicates()\n",
    "    cache.to_csv(mapping_filename, sep=\"\\t\", index=False, header=True) # output deduplicated cache terms to TSV\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30b40eb7-50af-40e8-8fc8-8cc4e506285c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score_mappings():\n",
    "\n",
    "    def get_max_score(str1, str2, old_score):\n",
    "        try:\n",
    "            if old_score == 1000:\n",
    "                return 1000\n",
    "            elif old_score == -1:\n",
    "                return -1\n",
    "            else:\n",
    "                sortratio_score = get_token_sort_ratio(str1, str2)\n",
    "                similarity_score = get_similarity_score(str1, str2)\n",
    "                max_score = max(sortratio_score, similarity_score)\n",
    "                return max_score\n",
    "        except:\n",
    "            return old_score\n",
    "\n",
    "    def wrap(x): # use this to convert string objects to dicts \n",
    "        try:\n",
    "            a = ast.literal_eval(x)\n",
    "            return(a)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    with pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0, on_bad_lines = 'warn', usecols=lambda c: not c.startswith('Unnamed:'), chunksize=1000) as reader:\n",
    "        write_header = True\n",
    "        for chunk in reader:\n",
    "            chunk[\"mapping_tool_response\"] = chunk[\"mapping_tool_response\"].apply(lambda x: wrap(x))\n",
    "            mapping_info = chunk[\"mapping_tool_response\"].apply(pd.Series)\n",
    "            chunk[\"mapped_name\"] = mapping_info[\"mapped_name\"]\n",
    "            chunk[\"score\"] = chunk.apply(lambda x: int(get_max_score(x['input_term'], x['mapped_name'], x['score'])), axis=1) # get score for score rows that are empty/not scored yet\n",
    "\n",
    "            # chunk[\"score\"] = chunk.apply(lambda x: int(get_max_score(x['input_term'], x['mapped_name'])) if (x[\"mapping_tool\"] != \"mapping_tools_failed\" or x[\"score\"] in [\"unscored\", 1000]) else 0, axis=1) # get score for score rows that are empty/not scored yet\n",
    "\n",
    "            # chunk[\"score\"] = chunk.apply(lambda x: int(get_max_score(x['input_term'], x['mapped_name'])) if (x[\"mapping_tool\"] != \"mapping_tools_failed\" or x[\"score\"] != \"unscored\") else 0, axis=1) # get score for score rows that are empty/not scored yet\n",
    "            # chunk[\"score\"] = chunk.apply(lambda x:get_max_score(x['input_term'], x['mapped_name']) if x[\"score\"] == \"unscored\" else x, axis=1) # DOES NOT WORK get score for score rows that are empty/not scored yet\n",
    "            chunk.drop([\"mapped_name\"], axis = 1, inplace = True)\n",
    "            chunk.to_csv(f'mapping_cache_scored_temp.tsv', sep=\"\\t\", index=False, header=write_header, mode = 'a', encoding=\"utf-8\") # output to TSV\n",
    "            write_header = False\n",
    "\n",
    "    os.rename('mapping_cache.tsv','mapping_cache_backup.tsv')       \n",
    "    os.rename('mapping_cache_scored_temp.tsv','mapping_cache.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e12b00-b8a1-4720-91cb-8654406fcb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"   Get high scorers   \"\"\"\n",
    "# cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "# cache[\"score\"] = cache[\"score\"].astype()\n",
    "\n",
    "\n",
    "# highscorers = cache[cache['score'] >= 80] \n",
    "# # # test = highscorers.groupby('clintrial_term')\n",
    "# # idx = highscorers.groupby('clintrial_term')['score'].idxmax()  # group by the clinical trial term and get the highest scoring\n",
    "# # auto_selected = highscorers.loc[idx]\n",
    "# # auto_selected.to_csv(f'autoselected_terms.tsv', sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "# # \"\"\"   Get low scorers, aggregate for manual selections  \"\"\"\n",
    "# # low_scorers = cache[cache['score'] < 80]\n",
    "# # manual_review = low_scorers[~low_scorers.clintrial_term.isin(highscorers['clintrial_term'].unique().tolist())] # there are terms autoselected that have mappings that didn't pass threshold too, but we want to consider that term mapped. So get rid of these rows too\n",
    "# # mapping_tool_response = manual_review['mapping_tool_response'].apply(lambda x: wrap(x))\n",
    "# # manual_review = manual_review.copy()\n",
    "# # mapping_tool_response = mapping_tool_response.apply(pd.Series)\n",
    "# # manual_review.loc[:, 'mapping_tool_response_lists'] = mapping_tool_response.values.tolist()\n",
    "# # manual_review.drop('mapping_tool_response', axis=1, inplace=True)\n",
    "# # manual_review = manual_review[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"mapping_tool_response_lists\", \"input_term\", \"score\"]]\n",
    "# # # manual_review['mapping_tool_response_lists'] = manual_review['mapping_tool_response_lists'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "# # manual_review['mapping_tool_response'] = [' | '.join(map(str, l)) for l in manual_review['mapping_tool_response_lists']]\n",
    "# # manual_review.drop('mapping_tool_response_lists', axis=1, inplace=True)\n",
    "\n",
    "# # manual_review.set_index([\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], inplace=True)   # create index\n",
    "# # # manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "# # manual_review['manually_selected_CURIE'] = None # make a column \n",
    "# # manual_review.to_excel('manual_review.xlsx', engine='xlsxwriter', index=True)\n",
    "\n",
    "\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):  # more options can be specified also\n",
    "#     display(cache[:2000])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892972ef-3a2d-4a1c-83d0-fc33f2a995d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def output_terms_files():\n",
    "\n",
    "    \"\"\"   Get high scorers   \"\"\"\n",
    "    cache = pd.read_csv(\"mapping_cache.tsv\", sep='\\t', index_col=False, header=0)\n",
    "    highscorers = cache[cache['score'] >= 80] \n",
    "    # test = highscorers.groupby('clintrial_term')\n",
    "    idx = highscorers.groupby('clintrial_term')['score'].idxmax()  # group by the clinical trial term and get the highest scoring\n",
    "    auto_selected = highscorers.loc[idx]\n",
    "    auto_selected.to_csv(f'autoselected_terms.tsv', sep=\"\\t\", index=False, header=True) # output to TSV\n",
    "\n",
    "    \"\"\"   Get low scorers, aggregate for manual selections  \"\"\"\n",
    "    low_scorers = cache[cache['score'] < 80]\n",
    "    manual_review = low_scorers[~low_scorers.clintrial_term.isin(highscorers['clintrial_term'].unique().tolist())] # there are terms autoselected that have mappings that didn't pass threshold too, but we want to consider that term mapped. So get rid of these rows too\n",
    "    mapping_tool_response = manual_review['mapping_tool_response'].apply(lambda x: wrap(x))\n",
    "    manual_review = manual_review.copy()\n",
    "    mapping_tool_response = mapping_tool_response.apply(pd.Series)\n",
    "    manual_review.loc[:, 'mapping_tool_response_lists'] = mapping_tool_response.values.tolist()\n",
    "    manual_review.drop('mapping_tool_response', axis=1, inplace=True)\n",
    "    manual_review = manual_review[[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"mapping_tool_response_lists\", \"input_term\", \"score\"]]\n",
    "    # manual_review['mapping_tool_response_lists'] = manual_review['mapping_tool_response_lists'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else None)  # Multindexing does not work on lists, so remove the CURIE information out of the list to enable this\n",
    "    manual_review['mapping_tool_response'] = [' | '.join(map(str, l)) for l in manual_review['mapping_tool_response_lists']]\n",
    "    manual_review.drop('mapping_tool_response_lists', axis=1, inplace=True)\n",
    "    manual_review = manual_review.sort_values(by=[\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], ascending=False)\n",
    "    manual_review.set_index([\"mapping_tool\", \"term_type\", \"clintrial_term\", \"input_term\"], inplace=True)   # create index\n",
    "    # manual_review.drop([\"temp\"], axis = 1, inplace = True)   # drop the temp column\n",
    "    manual_review['manually_selected_CURIE'] = None # make a column \n",
    "    manual_review.to_excel('manual_review.xlsx', engine='xlsxwriter', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a271356-978d-454f-ade0-baf110ad8036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07e6dd-3a54-4f4e-9a7b-d38af0e684dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b718c-2949-4003-b956-549d8e5b9e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046274d4-f7fe-4daf-8dd6-84e1ea90fd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd659f7-df88-47fd-9965-567b640b61ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96458f45-d889-4c08-b9d0-b2213d2c343a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d719ee3-148e-4948-8dfe-72a6bd0d730c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ed414-58ba-4746-ad22-bd8872c8886f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfddebe-14c3-4289-9df7-f68d2657b2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d00fe8-c542-4d2a-b379-27e3719b00fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1413de-fdda-4bde-ad88-b9d69f8748e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98ceddf5-0ebe-41d9-9b89-85fc647f4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download of Clinical Trial data as of 02_25_2024\n",
      "\n",
      "Failed to scrape AACT for download. Please navigate to https://aact.ctti-clinicaltrials.org/download and manually download zip file.\n",
      "Please store the downloaded zip in the /data directory. This should be the only item besides the cache file, condition manual review file, and intervention manual review file, in the directory at this time.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type Done when done:  Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found at: \n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/8vstm2enpo0ocbo2z7oypqurhgmz.zip\n",
      "Please make sure this the correct zip file from AACT\n",
      "Unzipping data into\n",
      "/Users/Kamileh/Work/ISB/NCATS_BiomedicalTranslator/Projects/ClinicalTrials/ETL_Python/data/02_20_2024_extracted\n",
      "MetaMap version < 2020, conduct mapping on terms after removing ascii characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "% conditions mapped: 100%|████████████████████| 49/49 [00:57<00:00,  1\n",
      "% interventions mapped: 100%|████████████████████| 47/47 [00:43<00:00,\n",
      "% intervention_alternates mapped: 100%|████████████████████| 50/50 [01\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xba in position 176773: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m df_dict \u001b[38;5;241m=\u001b[39m read_raw_ct_data(flag_and_path, subset_size) \u001b[38;5;66;03m# read the clinical trial data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dict_new_terms \u001b[38;5;241m=\u001b[39m check_against_cache(df_dict) \u001b[38;5;66;03m# use the existing cache of MetaMapped terms so that only new terms are mapped\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m term_list_to_mappers(dict_new_terms)\n\u001b[1;32m      8\u001b[0m score_mappings()\n\u001b[1;32m      9\u001b[0m output_terms_files()\n",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m, in \u001b[0;36mterm_list_to_mappers\u001b[0;34m(dict_new_terms)\u001b[0m\n\u001b[1;32m     49\u001b[0m output\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# \"\"\" Remove duplicate rows \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m cache \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(mapping_filename, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     53\u001b[0m cache \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[1;32m     54\u001b[0m cache\u001b[38;5;241m.\u001b[39mto_csv(mapping_filename, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:550\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:639\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2021\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xba in position 176773: invalid start byte"
     ]
    }
   ],
   "source": [
    "flag_and_path = get_raw_ct_data() # download raw data\n",
    "global metamap_dirs\n",
    "metamap_dirs = check_os()\n",
    "subset_size = 50\n",
    "df_dict = read_raw_ct_data(flag_and_path, subset_size) # read the clinical trial data\n",
    "dict_new_terms = check_against_cache(df_dict) # use the existing cache of MetaMapped terms so that only new terms are mapped\n",
    "term_list_to_mappers(dict_new_terms)\n",
    "score_mappings()\n",
    "output_terms_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9775bd6-feaf-4536-a7d9-0b981f00df6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a2122-0904-4b7c-938c-90ccf6027170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9297205-dc21-4f39-8e91-ad3f004ece85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c8425-0bc6-4908-b979-c388c0b2be2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7955a-24a6-4bc1-82b2-5d4c8d81b59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ddb07-a201-4573-8841-cb55a6ca0399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
